{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPZ8he0uLW9EW9Bd5T3dUgZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weedge/doraemon-nb/blob/main/gemma_inference_with_CPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "参考资料：\n",
        "- [google-deepmind/gemma](https://github.com/google-deepmind/gemma)\n",
        "- [Gemma 开放模型](https://ai.google.dev/gemma?hl=zh-cn)\n",
        "- [Gemma: Open Models Based on Gemini Research and Technology](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf)\n",
        "- [gemma-open-models](https://blog.google/technology/developers/gemma-open-models/)\n",
        "- [github google/gemma_pytorch](https://github.com/google/gemma_pytorch/tree/main)\n",
        "- [github google-deepmind/gemma](https://github.com/google-deepmind/gemma)\n",
        "- [Grouped Query Attention论文阅读](https://mltalks.com/posts/1372481864/)\n",
        "- [SwiGLU论文阅读](https://mltalks.com/posts/3606043780/)\n",
        "\n"
      ],
      "metadata": {
        "id": "al1WKRehRV5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!lscpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ1y9bUym5Zw",
        "outputId": "222c5701-061d-4785-8171-5b95d9d735e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:            x86_64\n",
            "  CPU op-mode(s):        32-bit, 64-bit\n",
            "  Address sizes:         46 bits physical, 48 bits virtual\n",
            "  Byte Order:            Little Endian\n",
            "CPU(s):                  2\n",
            "  On-line CPU(s) list:   0,1\n",
            "Vendor ID:               GenuineIntel\n",
            "  Model name:            Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "    CPU family:          6\n",
            "    Model:               79\n",
            "    Thread(s) per core:  2\n",
            "    Core(s) per socket:  1\n",
            "    Socket(s):           1\n",
            "    Stepping:            0\n",
            "    BogoMIPS:            4399.99\n",
            "    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clf\n",
            "                         lush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_\n",
            "                         good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fm\n",
            "                         a cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hyp\n",
            "                         ervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsb\n",
            "                         ase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsa\n",
            "                         veopt arat md_clear arch_capabilities\n",
            "Virtualization features: \n",
            "  Hypervisor vendor:     KVM\n",
            "  Virtualization type:   full\n",
            "Caches (sum of all):     \n",
            "  L1d:                   32 KiB (1 instance)\n",
            "  L1i:                   32 KiB (1 instance)\n",
            "  L2:                    256 KiB (1 instance)\n",
            "  L3:                    55 MiB (1 instance)\n",
            "NUMA:                    \n",
            "  NUMA node(s):          1\n",
            "  NUMA node0 CPU(s):     0,1\n",
            "Vulnerabilities:         \n",
            "  Gather data sampling:  Not affected\n",
            "  Itlb multihit:         Not affected\n",
            "  L1tf:                  Mitigation; PTE Inversion\n",
            "  Mds:                   Vulnerable; SMT Host state unknown\n",
            "  Meltdown:              Vulnerable\n",
            "  Mmio stale data:       Vulnerable\n",
            "  Retbleed:              Vulnerable\n",
            "  Spec rstack overflow:  Not affected\n",
            "  Spec store bypass:     Vulnerable\n",
            "  Spectre v1:            Vulnerable: __user pointer sanitization and usercopy barriers only; no swap\n",
            "                         gs barriers\n",
            "  Spectre v2:            Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Not affected\n",
            "  Srbds:                 Not affected\n",
            "  Tsx async abort:       Vulnerable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# gemma.cpp"
      ],
      "metadata": {
        "id": "6nh2F0wde2ix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "开发者笔记\n",
        "\n",
        "## 动机：为研究和实验设计的极简C++语言模型运行时\n",
        "\n",
        "过去，神经网络推理类似于一个简单、不透明的无状态函数，具有单一的输入和输出。相比之下，基础模型运行时更像是具有多种形式的状态、子系统和异构输入输出的系统。它们通常与其他具有自己资源的系统（例如RAG和工具）集成，并可能与外部环境交互。它们已经成为计算引擎，用于在广泛、通用的世界模型中嵌入接近任务和目标。\n",
        "\n",
        "考虑到这一点，我们相信开发一个灵活且易于接近的实验性运行时将使我们能够探索高级模型关注点与低级运行时计算之间的共同设计空间。\n",
        "\n",
        "## 设计优先级\n",
        "\n",
        "鉴于这些动机，我们提出以下优先级，用于决定代码库的方向和设计。\n",
        "\n",
        "**在狭窄范围内最大化利用。** 我们专注于直接实现像Gemma这样的基础模型。这使我们能够集中精力解决特定模型的瓶颈。我们愿意牺牲通用性，以保持实现代码在所有层次上相对简单和可读，实现良好的性能，并保持小团队的速度。\n",
        "\n",
        "**面向数据的设计。** 在可能的情况下遵循面向数据的设计原则，以最小化不必要的性能悲观。最好是在初始设计时或重构子组件时应用这些优化。第一步是考虑批次或普通旧数据（POD）类型的元组：分离的数组，而不是结构数组。第二步是降低控制流（if语句、虚函数和类层次结构）。第三步是了解数据的内在属性，并将其融入布局和算法中。\n",
        "\n",
        "**优先考虑小批量延迟。** 由于生产服务解决方案已经可以大规模服务，由加速器优化吞吐量，该项目专注于基础模型的本地、交互式使用的可能性。虽然吞吐量仍然重要，但在其他条件相同的情况下，优先考虑低延迟和小批量大小。\n",
        "\n",
        "**保持可移植的基线。** 我们的起点是一个可移植的CPU SIMD（通过[highway](https://github.com/google/highway)）。我们预计未来会添加加速器和混合CPU/GPU支持，但该项目应继续允许使用这个可移植基线进行构建。这确保了面向研究和实验的运行时和硬件平台即使没有专门的生产就绪部署路径，也能运行Gemma。\n",
        "\n",
        "## 代码组织\n",
        "\n",
        "实现代码大致分为从高到低的4层：\n",
        "\n",
        "1. 前端（`run.cc`） - 交互式接口或自动化编排，与模型推理和生成（2）的调用进行交互。前端代码以用例目标的形式实现，通过调用模型推理和生成。将gemma.cpp作为库的项目被认为是`run.cc`的替代前端。我们将在未来添加额外前端的示例。\n",
        "\n",
        "2. 模型（`gemma.cc`，`gemma.h`，`configs.h`） - 实现模型的计算图，包括使用层（3）提供的变换操作加载和压缩权重的支撑函数。\n",
        "\n",
        "3. 操作（`ops.h`） - 使用计算后端（4）实现的最小变换和支撑数学操作集。此代码应对模型实现的计算图的具体细节（2）保持不知情。\n",
        "\n",
        "4. 后端（`highway`） - 低级硬件接口（在highway的情况下是SIMD），支持（3）中的实现。\n",
        "\n",
        "除了这些层次，支撑实用程序包括：\n",
        "\n",
        "- `compression/` - 模型压缩操作。8位切换浮点模型转换在这里。\n",
        "- `util/` - 命令行参数处理和任何其他实用程序。\n",
        "\n",
        "## 风格和格式化\n",
        "\n",
        "我们提供了一个`.clang-format`配置文件，其中包含我们的默认设置，请在提交PR之前运行源文件通过`clang-format`（或产生等效行为的格式化器）。\n",
        "\n",
        "## 编译时标志（高级）\n",
        "\n",
        "有几个编译时标志需要注意（请注意，这些可能或可能不会暴露给构建系统）：\n",
        "\n",
        "- `GEMMA_WEIGHT_T`：设置权重的压缩级别（在CMakeLists.txt中作为WEIGHT_TYPE显示）。目前，如果没有指定标志，应将其设置为`SfpStream`（默认值，用于8位SFP），或`hwy::bfloat16_t`以启用更高保真度（但速度较慢）的bfloat16支持。这在`gemma.h`中定义。\n",
        "- `GEMMA_MAX_SEQ_LEN`：设置KV缓存的预分配最大序列长度。默认值为4096个标记，但可以覆盖。这尚未通过`CMakeLists.txt`暴露。\n",
        "\n",
        "从中期来看，这两个都可能被废弃，以支持在单个构建中处理多个权重压缩方案，并根据需要动态调整KV缓存的大小。\n",
        "\n",
        "## 将gemma.cpp作为库使用（高级）\n",
        "\n",
        "除非你正在进行更低级别的实现或研究，从应用的角度来看，你可以将gemma.h和gemma.cc视为库的“核心”。\n",
        "\n",
        "你可以将`run.cc`视为你的应用程序替代的一个示例应用程序，所以在`run.cc`中看到的对gemma.h和gemma.cc的调用可能是你将要调用的函数。你可以在`run.cc`中找到对tokenizer方法和`GenerateGemma`的调用示例。\n",
        "\n",
        "请记住，gemma.cpp面向的是更实验性/原型/研究应用程序。如果你的目标是生产，那么通过jax / pytorch / keras有更标准的NN部署路径。\n",
        "\n",
        "### Gemma结构包含推理引擎的所有状态 - 分词器、权重和激活\n",
        "\n",
        "`Gemma(...)` - 构造函数，创建一个gemma模型对象，它是分词器对象、权重、激活和KV缓存的包装。\n",
        "\n",
        "在标准的LLM聊天应用程序中，你可能会直接使用Gemma对象，在更奇特的数据处理或研究应用程序中，你可能会直接分解处理权重、KV缓存和激活（例如，对于一组权重，你可能有多个KV缓存和激活），而不是仅使用Gemma对象。\n",
        "\n",
        "## 使用Gemma对象中的分词器（或直接与分词器对象交互）\n",
        "\n",
        "你几乎只与分词器做一些事情，调用`Encode()`将字符串提示转换为标记ID向量，或调用`Decode()`将模型的标记ID向量输出转换回字符串。\n",
        "\n",
        "## 生成的主要入口点是`GenerateGemma()`\n",
        "\n",
        "通过调用`GenerateGemma`并传递一个标记化的提示，将1) 改变`model`中的激活值 2) 调用StreamFunc - 一个lambda回调，用于每个生成的标记。\n",
        "\n",
        "你的应用程序定义自己的StreamFunc作为lambda回调，每次从引擎流式传输一个标记字符串时（例如打印到屏幕、将数据写入磁盘、将字符串发送到服务器等）。你可以在`run.cc`中看到StreamFunc lambda负责在每个标记到达时将其打印到屏幕上。\n",
        "\n",
        "可选地，你可以定义accept_token作为另一个lambda - 这主要用于受限解码类型的用例，你希望强制生成适应语法。如果你不做这个，你可以发送一个空lambda作为无操作，这就是`run.cc`所做的。\n",
        "\n",
        "## 如果你想直接调用神经网络前向函数，调用`Transformer()`函数\n",
        "\n",
        "对于高级应用程序，你可能只调用`GenerateGemma()`，从不直接与神经网络交互，但如果你正在做一些更定制的事情，你可以调用transformer，它对单个标记执行单个推理操作，并通过神经网络计算改变激活和KV缓存。\n",
        "\n",
        "## 对于低级操作，定义新架构，直接调用`ops.h`函数\n",
        "\n",
        "如果你正在编写其他NN架构或修改Gemma模型的推理路径，你会使用`ops.h`。\n",
        "\n",
        "## Discord\n",
        "\n",
        "我们也在尝试使用discord服务器进行讨论 - https://discord.gg/H5jCBAWxAe"
      ],
      "metadata": {
        "id": "qwshi1w-bbPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 从kaggle下载预训练好的 模型权重 和 tokenizer词表文件\n",
        "\n",
        "https://www.kaggle.com/models/google/gemma/frameworks/gemmaCpp\n"
      ],
      "metadata": {
        "id": "eQoURmyZ3g79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget \"https://storage.googleapis.com/kagglesdsdata/models/8358/11366/tokenizer.spm?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240302%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240302T133349Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=173aa155f7c6321a75b36092f87d7a778548cafb0e61b5d95ea446d6bbadd97d67b738b5396c81c10ef80ac9570abb596cdec625bb8dfcde819232fa32f0a292243cac311ac2e1e4ea44065d612ce84537001a111a1a5429a20fc63bccfb1d2f90291aef9a0a657802eddf10d4ab5497465759f5e06689eddb57504175c69267028253776e948970801b2f52a816851fceac2a0f327b6345796a9c93895f1699c0961859dabd45477f1090af2d158531994df6b0fdb72254a315322797dbdc295a809425275ee7b8b7cc7be9bfd767787f0f59ffb96aa8145e8ee0239a12f885b1c0d1e9babf4ebcd0d02a0ff2b19baea6baa025f15382fa0ec46ed2bc5d820c\" \\\n",
        "!wget \"https://storage.googleapis.com/kagglesdsdata/models/8385/11363/tokenizer.spm?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240302%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240302T140024Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=30b7f62a8319d8583b708e9819f69b9575a1c51d46354cd4ec27b8bc401dc4354f257f9b12bf9c983de02a9ca8fe79cd382d323726cf3d0b7adaaddf254ecdf205e35ff84113e80100d647a19f2484674f3a770e736a1a3831c95c48e9fd62ceedaf18f7537f4d3039e5e0a1109c106d2847e44341c8e84b7a45d2b313457dedf56881d46807cb940f389e850ae5639a16b594f7a7270fe4a99688ff694fe14fc35181ce855e82cdbcdad406a39fbc90d9c40abc60220699db34bf9c296367651f2f280ad97893e89591bad931f5d2759134de7228e8bffde55d592b26df9283ad731bf656100aea337ba5950ce26d8594e6c0b2b0975a983fc80b00fb7af513\" \\\n",
        " -O tokenizer.spm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtVbm5PwXrWh",
        "outputId": "f087b633-8898-49e3-de1c-355b07daf7e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-02 14:01:33--  https://storage.googleapis.com/kagglesdsdata/models/8385/11363/tokenizer.spm?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240302%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240302T140024Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=30b7f62a8319d8583b708e9819f69b9575a1c51d46354cd4ec27b8bc401dc4354f257f9b12bf9c983de02a9ca8fe79cd382d323726cf3d0b7adaaddf254ecdf205e35ff84113e80100d647a19f2484674f3a770e736a1a3831c95c48e9fd62ceedaf18f7537f4d3039e5e0a1109c106d2847e44341c8e84b7a45d2b313457dedf56881d46807cb940f389e850ae5639a16b594f7a7270fe4a99688ff694fe14fc35181ce855e82cdbcdad406a39fbc90d9c40abc60220699db34bf9c296367651f2f280ad97893e89591bad931f5d2759134de7228e8bffde55d592b26df9283ad731bf656100aea337ba5950ce26d8594e6c0b2b0975a983fc80b00fb7af513\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.119.207, 108.177.111.207, 142.250.1.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.119.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4241003 (4.0M) [application/octet-stream]\n",
            "Saving to: ‘tokenizer.spm’\n",
            "\n",
            "tokenizer.spm       100%[===================>]   4.04M  15.7MB/s    in 0.3s    \n",
            "\n",
            "2024-03-02 14:01:34 (15.7 MB/s) - ‘tokenizer.spm’ saved [4241003/4241003]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gemma 模型权重类型\n",
        "2B instruction-tuned (`it`) and pre-trained (`pt`) models:\n",
        "\n",
        "| Model name  | Description |\n",
        "| ----------- | ----------- |\n",
        "| `2b-it`     | 2 billion parameter instruction-tuned model, bfloat16 |\n",
        "| `2b-it-sfp` | 2 billion parameter instruction-tuned model, 8-bit switched floating point |\n",
        "| `2b-pt`     | 2 billion parameter pre-trained model, bfloat16 |\n",
        "| `2b-pt-sfp` | 2 billion parameter pre-trained model, 8-bit switched floating point |\n",
        "\n",
        "7B instruction-tuned (`it`) and pre-trained (`pt`) models:\n",
        "\n",
        "| Model name  | Description |\n",
        "| ----------- | ----------- |\n",
        "| `7b-it`     | 7 billion parameter instruction-tuned model, bfloat16 |\n",
        "| `7b-it-sfp` | 7 billion parameter instruction-tuned model, 8-bit switched floating point |\n",
        "| `7b-pt`     | 7 billion parameter pre-trained model, bfloat16 |\n",
        "| `7b-pt-sfp` | 7 billion parameter pre-trained model, 8-bit switched floating point |\n",
        "\n",
        "**Tips**:\n",
        "\n",
        "8位切换浮点（8-bit Switched Floating Point，简称8-bit SFP）是一种用于表示浮点数的压缩格式，它旨在减少存储和计算资源的使用，同时尽量保持数据的精度。在深度学习和机器学习领域，尤其是在模型部署和推理阶段，这种格式可以帮助减少模型的大小，提高内存效率，从而在资源受限的设备上实现更快的推理速度。\n",
        "\n",
        "8-bit SFP通常涉及以下几个关键特性：\n",
        "\n",
        "1. **精度**：与标准的32位浮点数（单精度）相比，8-bit SFP提供了较低的精度。这意味着在表示非常大或非常小的数值时，可能会有较大的舍入误差。\n",
        "\n",
        "2. **范围**：由于只有8位，这种格式的数值范围比32位浮点数要小。这可能限制了它在某些需要宽数值范围的应用中的使用。\n",
        "\n",
        "3. **压缩**：8-bit SFP可以显著减少模型的存储空间需求，因为它只需要8位来表示一个浮点数，而不是标准的32位。\n",
        "\n",
        "4. **兼容性**：在某些硬件和软件平台上，8-bit SFP可能需要特定的支持才能正确处理。这可能涉及到在编译时设置特定的标志，或者在运行时进行特定的配置。\n",
        "\n",
        "5. **性能**：在支持8-bit SFP的硬件上，使用这种格式可以提高计算性能，因为处理8位数据通常比处理32位数据更快。\n",
        "\n",
        "在实际应用中，开发者需要权衡使用8-bit SFP带来的内存和性能优势与其可能引入的精度损失。在某些情况下，这种权衡是可接受的，尤其是在对模型大小和推理速度有严格要求的嵌入式系统或移动设备上。然而，在对精度要求较高的应用中，可能需要考虑其他压缩技术，或者在模型设计阶段就考虑到精度和资源的平衡。"
      ],
      "metadata": {
        "id": "sTjkrLUL4lqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### gemma-2b-it-sfp"
      ],
      "metadata": {
        "id": "lfiL_fGxATq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://storage.googleapis.com/kagglesdsdata/models/8385/11363/2b-it-sfp.sbs?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240302%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240302T135831Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=ac2a0bc761ddb609201e333c65a0ff98a3b1a0a87dde1af6c397cef6c64498bedda374657483a1b679cf78e777c3b5c3d2295aabc91ea374b3019f67de38c1d6aa323a71001ea91afa857725853e9502e7acc522f524449ca45c895f55c2befca963bc76b5d9bbafb691d930d93d2d1d9f537bb25d2b9531ea3fdf77eebd3fd06f01cf598eb41e33677567531804e74e1a931ab46239876fef630ade4b7b5987e3a57e5b729bead24cda908b23eb69511be09ad8cc60b2bdd38f5fcb95b122ce62e2f054930a65cc10571cea068bbdbf624388e22a697c43431deb0afe65a77d44240ad23641a35c49d391eaa1a2973a6138d3df7fcd24f9f334cb36c1124682\" \\\n",
        "  -O 2b-it-sfp.sbs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6x7r09Dkdhf0",
        "outputId": "91338ee3-adf6-44a0-ff55-ad02619d2abe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-02 13:59:33--  https://storage.googleapis.com/kagglesdsdata/models/8385/11363/2b-it-sfp.sbs?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240302%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240302T135831Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=ac2a0bc761ddb609201e333c65a0ff98a3b1a0a87dde1af6c397cef6c64498bedda374657483a1b679cf78e777c3b5c3d2295aabc91ea374b3019f67de38c1d6aa323a71001ea91afa857725853e9502e7acc522f524449ca45c895f55c2befca963bc76b5d9bbafb691d930d93d2d1d9f537bb25d2b9531ea3fdf77eebd3fd06f01cf598eb41e33677567531804e74e1a931ab46239876fef630ade4b7b5987e3a57e5b729bead24cda908b23eb69511be09ad8cc60b2bdd38f5fcb95b122ce62e2f054930a65cc10571cea068bbdbf624388e22a697c43431deb0afe65a77d44240ad23641a35c49d391eaa1a2973a6138d3df7fcd24f9f334cb36c1124682\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.126.207, 74.125.132.207, 74.125.201.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.126.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3163184640 (2.9G) [application/octet-stream]\n",
            "Saving to: ‘2b-it-sfp.sbs’\n",
            "\n",
            "2b-it-sfp.sbs       100%[===================>]   2.95G  70.4MB/s    in 41s     \n",
            "\n",
            "2024-03-02 14:00:14 (73.2 MB/s) - ‘2b-it-sfp.sbs’ saved [3163184640/3163184640]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### gemma-7b-it-sfp"
      ],
      "metadata": {
        "id": "VhVDfoekAaCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://storage.googleapis.com/kagglesdsdata/models/8445/11370/7b-it-sfp.sbs?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240302%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240302T163205Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=824f075e8de538d45977f02b0cfe6488b926933ad914086ca5416c918d87cfbf3ff48f3936a3bbd852dc3a773ae75e55c39be08a9893b092b1ece4a3acee94b732c2869e29452e7d4624932707f27d9564ace31e1e3fc9d0ef687e482e22b901451356d6508ced70f807cd5544d0675c506e9833c033f0036153d81d8337d8c9b6e7073dd8c5b305a128433e29106e4bdbb2bb0356f5bec75434d67a8549bc7030526496fce0fc7f379649519c1aa579d985cffc01a419f7d0aaf8459fa20651f306026236bee4f8b1e301c64386a6c11750fccdb0f0df504351088f7053e634fdc97bff17324c2f0f087f3a9c3e1fb5bff8ffe0fe0e9e0ff3d40bc83cc9cce2\" \\\n",
        "  -O 7b-it-sfp.sbs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVFFcbmCApA8",
        "outputId": "02fed96a-9461-497f-c8c4-abc65b958c2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-02 16:34:06--  https://storage.googleapis.com/kagglesdsdata/models/8445/11370/7b-it-sfp.sbs?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240302%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240302T163205Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=824f075e8de538d45977f02b0cfe6488b926933ad914086ca5416c918d87cfbf3ff48f3936a3bbd852dc3a773ae75e55c39be08a9893b092b1ece4a3acee94b732c2869e29452e7d4624932707f27d9564ace31e1e3fc9d0ef687e482e22b901451356d6508ced70f807cd5544d0675c506e9833c033f0036153d81d8337d8c9b6e7073dd8c5b305a128433e29106e4bdbb2bb0356f5bec75434d67a8549bc7030526496fce0fc7f379649519c1aa579d985cffc01a419f7d0aaf8459fa20651f306026236bee4f8b1e301c64386a6c11750fccdb0f0df504351088f7053e634fdc97bff17324c2f0f087f3a9c3e1fb5bff8ffe0fe0e9e0ff3d40bc83cc9cce2\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.120.207, 142.251.171.207, 142.250.159.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.120.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9325080064 (8.7G) [application/octet-stream]\n",
            "Saving to: ‘7b-it-sfp.sbs’\n",
            "\n",
            "7b-it-sfp.sbs       100%[===================>]   8.68G  80.9MB/s    in 1m 44s  \n",
            "\n",
            "2024-03-02 16:35:50 (85.2 MB/s) - ‘7b-it-sfp.sbs’ saved [9325080064/9325080064]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 推理(inference)"
      ],
      "metadata": {
        "id": "KZVyjfHx3yrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/google/gemma.cpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLfaxnl7X80t",
        "outputId": "4515f90e-51d0-42f6-baed-e462c0fb8632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gemma.cpp'...\n",
            "remote: Enumerating objects: 261, done.\u001b[K\n",
            "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
            "remote: Compressing objects: 100% (80/80), done.\u001b[K\n",
            "remote: Total 261 (delta 75), reused 59 (delta 43), pack-reused 138\u001b[K\n",
            "Receiving objects: 100% (261/261), 161.06 KiB | 2.24 MiB/s, done.\n",
            "Resolving deltas: 100% (129/129), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $(nproc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T40xNiUgZhb6",
        "outputId": "fc4aa1d1-4eaf-4551-ab30-042df49135c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defualt SfpStream use 8-bit switched floating point\n",
        "!cd gemma.cpp && rm -rf build/* && cmake -B build -S . && make -C build -j $(nproc) gemma\n",
        "\n",
        "#!cd gemma.cpp && rm -rf build/* && cmake -B build -S . -DPROFILER_ENABLED=1 && make -C build -j $(nproc) gemma\n",
        "#!cd gemma.cpp && rm -rf build/* && cmake -B build -S . -DWEIGHT_TYPE=hwy::bfloat16_t && make -C build -j $(nproc) gemma\n",
        "#!cd gemma.cpp && rm -rf build/* && cmake -B build -S . -DPROFILER_ENABLED=1 -DWEIGHT_TYPE=hwy::bfloat16_t && make -C build -j $(nproc) gemma\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbmU11KDYH1y",
        "outputId": "4c8c139e-d23e-49b6-c3f2-21c661c864cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[0mCMake Deprecation Warning at build/_deps/highway-src/CMakeLists.txt:25 (cmake_policy):\n",
            "  The OLD behavior for policy CMP0111 will be removed from a future version\n",
            "  of CMake.\n",
            "\n",
            "  The cmake-policies(7) manual explains that the OLD behaviors of all\n",
            "  policies are deprecated and that a policy should be set to OLD only under\n",
            "  specific short-term circumstances.  Projects should be ported to the NEW\n",
            "  behavior and not rely on setting a policy to OLD.\n",
            "\n",
            "\u001b[0m\n",
            "-- Performing Test ATOMICS_LOCK_FREE_INSTRUCTIONS\n",
            "-- Performing Test ATOMICS_LOCK_FREE_INSTRUCTIONS - Success\n",
            "-- Performing Test HWY_EMSCRIPTEN\n",
            "-- Performing Test HWY_EMSCRIPTEN - Failed\n",
            "-- Performing Test HWY_RISCV\n",
            "-- Performing Test HWY_RISCV - Failed\n",
            "-- Looking for sys/auxv.h\n",
            "-- Looking for sys/auxv.h - found\n",
            "-- Looking for asm/hwcap.h\n",
            "-- Looking for asm/hwcap.h - not found\n",
            "CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "  CMake that the project does not need compatibility with older versions.\n",
            "\n",
            "\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/gemma.cpp/build/_deps/highway-build/googletest-download\n",
            "[ 11%] Creating directories for 'googletest'\n",
            "[ 22%] Performing download step (git clone) for 'googletest'\n",
            "Cloning into 'googletest-src'...\n",
            "HEAD is now at 43efa0a4 Merge pull request #3617 from Bagira80:fix_3616\n",
            "[ 33%] Performing update step for 'googletest'\n",
            "[ 44%] No patch step for 'googletest'\n",
            "[ 55%] No configure step for 'googletest'\n",
            "[ 66%] No build step for 'googletest'\n",
            "[ 77%] No install step for 'googletest'\n",
            "[ 88%] No test step for 'googletest'\n",
            "[100%] Completed 'googletest'\n",
            "[100%] Built target googletest\n",
            "-- Found Python: /usr/local/bin/python (found version \"3.10.12\") found components: Interpreter \n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE  \n",
            "\u001b[0mCMake Deprecation Warning at build/_deps/sentencepiece-src/CMakeLists.txt:15 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "  CMake that the project does not need compatibility with older versions.\n",
            "\n",
            "\u001b[0m\n",
            "-- VERSION: 0.2.0\n",
            "-- Not Found TCMalloc: TCMALLOC_LIB-NOTFOUND\n",
            "-- Configuring done (16.5s)\n",
            "-- Generating done (0.3s)\n",
            "-- Build files have been written to: /content/gemma.cpp/build\n",
            "make: Entering directory '/content/gemma.cpp/build'\n",
            "make[1]: Entering directory '/content/gemma.cpp/build'\n",
            "make[2]: Entering directory '/content/gemma.cpp/build'\n",
            "make[3]: Entering directory '/content/gemma.cpp/build'\n",
            "make[3]: Entering directory '/content/gemma.cpp/build'\n",
            "make[3]: Leaving directory '/content/gemma.cpp/build'\n",
            "make[3]: Leaving directory '/content/gemma.cpp/build'\n",
            "make[3]: Entering directory '/content/gemma.cpp/build'\n",
            "make[3]: Entering directory '/content/gemma.cpp/build'\n",
            "[  0%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy.dir/hwy/aligned_allocator.cc.o\u001b[0m\n",
            "[  0%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/arena.cc.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy.dir/hwy/nanobenchmark.cc.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/arenastring.cc.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/bytestream.cc.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy.dir/hwy/per_target.cc.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/coded_stream.cc.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/common.cc.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/extension_set.cc.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy.dir/hwy/print.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy.dir/hwy/targets.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy.dir/hwy/timer.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32m\u001b[1mLinking CXX static library libhwy.a\u001b[0m\n",
            "make[3]: Leaving directory '/content/gemma.cpp/build'\n",
            "[ 15%] Built target hwy\n",
            "make[3]: Entering directory '/content/gemma.cpp/build'\n",
            "make[3]: Leaving directory '/content/gemma.cpp/build'\n",
            "make[3]: Entering directory '/content/gemma.cpp/build'\n",
            "[ 15%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_128a.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/generated_enum_util.cc.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/generated_message_table_driven_lite.cc.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/generated_message_util.cc.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/implicit_weak_message.cc.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/int128.cc.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/io_win32.cc.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/message_lite.cc.o\u001b[0m\n",
            "In file included from \u001b[01m\u001b[K/usr/include/string.h:535\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/gemma.cpp/build/_deps/sentencepiece-src/src/../third_party/protobuf-lite/google/protobuf/stubs/port.h:39\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/gemma.cpp/build/_deps/sentencepiece-src/src/../third_party/protobuf-lite/google/protobuf/stubs/macros.h:34\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/gemma.cpp/build/_deps/sentencepiece-src/src/../third_party/protobuf-lite/google/protobuf/stubs/common.h:46\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/gemma.cpp/build/_deps/sentencepiece-src/src/../third_party/protobuf-lite/google/protobuf/message_lite.h:45\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/gemma.cpp/build/_deps/sentencepiece-src/third_party/protobuf-lite/message_lite.cc:36\u001b[m\u001b[K:\n",
            "In function ‘\u001b[01m\u001b[Kvoid* memcpy(void*, const void*, size_t)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kgoogle::protobuf::uint8* google::protobuf::io::EpsCopyOutputStream::WriteRaw(const void*, int, google::protobuf::uint8*)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/gemma.cpp/build/_deps/sentencepiece-src/src/../third_party/protobuf-lite/google/protobuf/io/coded_stream.h:699:16\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvirtual google::protobuf::uint8* google::protobuf::internal::ImplicitWeakMessage::_InternalSerialize(google::protobuf::uint8*, google::protobuf::io::EpsCopyOutputStream*) const\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/gemma.cpp/build/_deps/sentencepiece-src/src/../third_party/protobuf-lite/google/protobuf/implicit_weak_message.h:85:28\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kbool google::protobuf::MessageLite::SerializePartialToZeroCopyStream(google::protobuf::io::ZeroCopyOutputStream*) const\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/gemma.cpp/build/_deps/sentencepiece-src/third_party/protobuf-lite/message_lite.cc:419:30\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/string_fortified.h:29:33:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid* __builtin___memcpy_chk(void*, const void*, long unsigned int, long unsigned int)\u001b[m\u001b[K’ specified size between 18446744071562067968 and 18446744073709551615 exceeds maximum object size 9223372036854775807 [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wstringop-overflow=\u0007-Wstringop-overflow=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   29 |   return \u001b[01;35m\u001b[K__builtin___memcpy_chk (__dest, __src, __len,\u001b[m\u001b[K\n",
            "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "   30 | \u001b[01;35m\u001b[K                                 __glibc_objsize0 (__dest))\u001b[m\u001b[K;\n",
            "      |                                  \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 26%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/parse_context.cc.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/repeated_field.cc.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/status.cc.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/statusor.cc.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/stringpiece.cc.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/stringprintf.cc.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_128d.cc.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/structurally_valid.cc.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/strutil.cc.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/time.cc.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/wire_format_lite.cc.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/zero_copy_stream.cc.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/zero_copy_stream_impl.cc.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/zero_copy_stream_impl_lite.cc.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/builtin_pb/sentencepiece.pb.cc.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/builtin_pb/sentencepiece_model.pb.cc.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/bpe_model.cc.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/char_model.cc.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/error.cc.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/filesystem.cc.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_f16a.cc.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/model_factory.cc.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/model_interface.cc.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/normalizer.cc.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_f16d.cc.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/sentencepiece_processor.cc.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_f32a.cc.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/unigram_model.cc.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/util.cc.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/word_model.cc.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/absl/flags/flag.cc.o\u001b[0m\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX shared library libsentencepiece.so\u001b[0m\n",
            "make[3]: Leaving directory '/content/gemma.cpp/build'\n",
            "[ 69%] Built target sentencepiece\n",
            "[ 69%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_f32d.cc.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_f64a.cc.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_f64d.cc.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_i16a.cc.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_i16d.cc.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_i32a.cc.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_i32d.cc.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_i64a.cc.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_i64d.cc.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_kv128a.cc.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_kv128d.cc.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_kv64a.cc.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_kv64d.cc.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_u16a.cc.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_u16d.cc.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_u32a.cc.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_u32d.cc.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_u64a.cc.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_u64d.cc.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/image/image.cc.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort.cc.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX static library libhwy_contrib.a\u001b[0m\n",
            "make[3]: Leaving directory '/content/gemma.cpp/build'\n",
            "[ 96%] Built target hwy_contrib\n",
            "make[3]: Entering directory '/content/gemma.cpp/build'\n",
            "make[3]: Leaving directory '/content/gemma.cpp/build'\n",
            "make[3]: Entering directory '/content/gemma.cpp/build'\n",
            "[100%] \u001b[32mBuilding CXX object CMakeFiles/gemma.dir/run.cc.o\u001b[0m\n",
            "[100%] \u001b[32mBuilding CXX object CMakeFiles/gemma.dir/gemma.cc.o\u001b[0m\n",
            "[100%] \u001b[32mBuilding CXX object CMakeFiles/gemma.dir/compression/blob_store.cc.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable gemma\u001b[0m\n",
            "make[3]: Leaving directory '/content/gemma.cpp/build'\n",
            "[100%] Built target gemma\n",
            "make[2]: Leaving directory '/content/gemma.cpp/build'\n",
            "make[1]: Leaving directory '/content/gemma.cpp/build'\n",
            "make: Leaving directory '/content/gemma.cpp/build'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./gemma.cpp/build/gemma -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7iRXK88YQTl",
        "outputId": "6d2fcf79-7727-40d2-e5d7-455146969818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  __ _  ___ _ __ ___  _ __ ___   __ _   ___ _ __  _ __\n",
            " / _` |/ _ \\ '_ ` _ \\| '_ ` _ \\ / _` | / __| '_ \\| '_ \\\n",
            "| (_| |  __/ | | | | | | | | | | (_| || (__| |_) | |_) |\n",
            " \\__, |\\___|_| |_| |_|_| |_| |_|\\__,_(_)___| .__/| .__/\n",
            "  __/ |                                    | |   | |\n",
            " |___/                                     |_|   |_|\n",
            "\n",
            "gemma.cpp : a lightweight, standalone C++ inference engine\n",
            "==========================================================\n",
            "\n",
            "To run gemma.cpp, you need to specify 3 required model loading arguments:\n",
            "    --tokenizer\n",
            "    --compressed_weights\n",
            "    --model.\n",
            "\n",
            "*Example Usage*\n",
            "\n",
            "./gemma --tokenizer tokenizer.spm --compressed_weights 2b-it-sfp.sbs --model 2b-it\n",
            "\n",
            "*Model Loading Arguments*\n",
            "\n",
            "  --tokenizer : Path name of tokenizer model file.\n",
            "    Required argument.\n",
            "  --compressed_weights : Path name of compressed weights file, regenerated from `--weights` file if the compressed weights file does not exist.\n",
            "    Required argument.\n",
            "  --model : Model type\n",
            "    2b-it (2B parameters, instruction-tuned)\n",
            "    2b-pt (2B parameters, pretrained)\n",
            "    7b-it (7B parameters instruction-tuned)\n",
            "    7b-pt (7B parameters, pretrained)\n",
            "    Required argument.\n",
            "  --weights : Path name of model weights (.sbs) file. Only required if compressed_weights file is not present and needs to be regenerated. This parameter is only required for compressingnew model weight exports, otherwise it is not needed.\n",
            "\n",
            "*Inference Arguments*\n",
            "\n",
            "  --max_tokens : Maximum number of tokens in prompt + generation.\n",
            "  --max_generated_tokens : Maximum number of tokens to generate.\n",
            "  --temperature : Temperature for top-K\n",
            "  --deterministic : Make top-k sampling deterministic\n",
            "  --multiturn : Multiturn mode (if 0, this clears the KV cache after every interaction without quitting)\n",
            "    Default : 0 (conversation resets every turn)\n",
            "\n",
            "*Application Arguments*\n",
            "\n",
            "  --verbosity : Show verbose developer information\n",
            "   0 = only print generation output\n",
            "   1 = standard user-facing terminal ui\n",
            "   2 = show developer/debug info).\n",
            "   Default = 1.\n",
            "  --num_threads : Number of threads to use.\n",
            "    Default = Estimate of the number of suupported concurrent threads.\n",
            "  --eot_line : End of turn line. When you specify this, the prompt will be all lines before the line where only the given string appears.\n",
            "    Default = When a newline is encountered, that signals the end of the turn.\n",
            "\n",
            "Abort at /content/gemma.cpp/run.cc:296: \n",
            "Invalid args: Model type must be 2b-pt, 7b-pt, 2b-it, or 7b-it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./gemma.cpp/build/gemma --tokenizer ./tokenizer.spm --compressed_weights ./2b-it-sfp.sbs --model 2b-it"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qALmx9iQcfI4",
        "outputId": "220c103c-d727-496e-fe5e-72115177c0ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2J\u001b[1;1H  __ _  ___ _ __ ___  _ __ ___   __ _   ___ _ __  _ __\n",
            " / _` |/ _ \\ '_ ` _ \\| '_ ` _ \\ / _` | / __| '_ \\| '_ \\\n",
            "| (_| |  __/ | | | | | | | | | | (_| || (__| |_) | |_) |\n",
            " \\__, |\\___|_| |_| |_|_| |_| |_|\\__,_(_)___| .__/| .__/\n",
            "  __/ |                                    | |   | |\n",
            " |___/                                     |_|   |_|\n",
            "\n",
            "tokenizer                     : ./tokenizer.spm\n",
            "compressed_weights            : ./2b-it-sfp.sbs\n",
            "model                         : 2b-it\n",
            "weights                       : [no path specified]\n",
            "max_tokens                    : 3072\n",
            "max_generated_tokens          : 2048\n",
            "multiturn                     : 0\n",
            "\n",
            "*Usage*\n",
            "  Enter an instruction and press enter (%C resets conversation, %Q quits).\n",
            "  Since multiturn is set to 0, conversation will automatically reset every turn.\n",
            "\n",
            "*Examples*\n",
            "  - Write an email to grandma thanking her for the cookies.\n",
            "  - What are some historical attractions to visit around Massachusetts?\n",
            "  - Compute the nth fibonacci number in javascript.\n",
            "  - Write a standup comedy bit about GPU programming.\n",
            "\n",
            "> Write an email to grandma thanking her for the cookies.\n",
            "\n",
            "[ Reading prompt ] ...................\n",
            "\n",
            "\n",
            "Subject: Thank you for the delicious cookies!\n",
            "\n",
            "Dear Grandma,\n",
            "\n",
            "I just wanted to take a moment to express my sincere gratitude for the wonderful cookies you baked for me. They were absolutely delicious!\n",
            "\n",
            "I especially loved the [mention a few specific cookies or flavors]. They were so fresh and flavorful, and the perfect treat to brighten my day.\n",
            "\n",
            "Your generosity and thoughtfulness mean the world to me. It's always so sweet to receive a homemade treat, and your cookies were a perfect reminder of all the love and care you put into them.\n",
            "\n",
            "I can't wait to enjoy another batch soon! Please let me know when you're planning to bake some more.\n",
            "\n",
            "Thank you again for everything, Grandma. You're the best!\n",
            "\n",
            "Love,\n",
            "\n",
            "[Your name]\n",
            "\n",
            "> What are some historical attractions to visit around Massachusetts?\n",
            "\n",
            "[ Reading prompt ] ..................\n",
            "\n",
            "\n",
            "**Historical Cities:**\n",
            "\n",
            "* **Boston:** Freedom Trail, Faneuil Hall, Old State House, Quincy Market, Paul Revere House, USS Constitution.\n",
            "* **Salem:** Salem Witch Museum, Witch House, Old Burying Point, Salem Village.\n",
            "* **Plymouth:** Pilgrim Monument, First Parish Church, Mayflower II.\n",
            "* **Worcester:** The Old State House, The Friendly Sons of St. Patrick's Church, The Worcester Historical Museum.\n",
            "* **Springfield:** Springfield Armory, Lyman Hall, Old State House.\n",
            "\n",
            "**Historic Towns:**\n",
            "\n",
            "* **Northampton:** The Witch House, First Baptist Church, Smith College campus.\n",
            "* **Provincetown:** Province Lands, Pilgrim Monument, Chatham Bars.\n",
            "* **Mystic:** Mystic Seaport Museum, Mystic Aquarium, Mystic Walking History.\n",
            "* **Marblehead:** Marblehead Lighthouse, Marblehead Harbor, First Parish Church.\n",
            "* **Nantucket:** The Breakers mansion, The Old Burying Point, The Nantucket Cottage.\n",
            "\n",
            "**Historic Sites:**\n",
            "\n",
            "* **Boston Harbor Islands National and State Park:** Islands include Spectacle Island, Boston Harbor Islands National Monument, and Hull House.\n",
            "* **Cape Ann National Seashore:** Stunning beaches, lighthouses, and maritime history.\n",
            "* **Mount Greylock State Reservation:** Highest peak in Massachusetts, offering panoramic views.\n",
            "* **Mount Tombs State Reservation:** Historic cemetery and burial site of American soldiers.\n",
            "* **The Old State House in Boston:** One of the oldest buildings in the United States, used by the Massachusetts Bay Colony.\n",
            "\n",
            "**Other Attractions:**\n",
            "\n",
            "* **The Liberty Tree in Boston:** A symbol of American independence.\n",
            "* **The Old State House in Salem:** Built in 1692, it served as the seat of colonial government.\n",
            "* **The Pilgrim Monument in Plymouth:** A pilgrimage site for Pilgrims.\n",
            "* **The USS Constitution in Boston Harbor:** The world's oldest commissioned warship.\n",
            "* **The Witch Museum in Salem:** A chilling glimpse into the Salem Witch Trials.\n",
            "\n",
            "**Seasonal Events:**\n",
            "\n",
            "* **Christmas in Boston:** The city transforms into a festive wonderland with Christmas lights, decorations, and events.\n",
            "* **Halloween in Salem:** The town becomes a spooky and eerie place with ghost tours, haunted houses, and a witches' ball.\n",
            "* **Summer in Provincetown:** Enjoy the beaches, whale watching, and the vibrant arts scene.\n",
            "\n",
            "> ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./gemma.cpp/build/gemma --tokenizer ./tokenizer.spm --compressed_weights ./7b-it-sfp.sbs --model 7b-it"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRG9OVerB4Jl",
        "outputId": "29b825a9-4ff8-4a33-812d-9b0c4b3aac5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2J\u001b[1;1H  __ _  ___ _ __ ___  _ __ ___   __ _   ___ _ __  _ __\n",
            " / _` |/ _ \\ '_ ` _ \\| '_ ` _ \\ / _` | / __| '_ \\| '_ \\\n",
            "| (_| |  __/ | | | | | | | | | | (_| || (__| |_) | |_) |\n",
            " \\__, |\\___|_| |_| |_|_| |_| |_|\\__,_(_)___| .__/| .__/\n",
            "  __/ |                                    | |   | |\n",
            " |___/                                     |_|   |_|\n",
            "\n",
            "tokenizer                     : ./tokenizer.spm\n",
            "compressed_weights            : ./7b-it-sfp.sbs\n",
            "model                         : 7b-it\n",
            "weights                       : [no path specified]\n",
            "max_tokens                    : 3072\n",
            "max_generated_tokens          : 2048\n",
            "multiturn                     : 0\n",
            "\n",
            "*Usage*\n",
            "  Enter an instruction and press enter (%C resets conversation, %Q quits).\n",
            "  Since multiturn is set to 0, conversation will automatically reset every turn.\n",
            "\n",
            "*Examples*\n",
            "  - Write an email to grandma thanking her for the cookies.\n",
            "  - What are some historical attractions to visit around Massachusetts?\n",
            "  - Compute the nth fibonacci number in javascript.\n",
            "  - Write a standup comedy bit about GPU programming.\n",
            "\n",
            "> Write an email to grandma thanking her for the cookies.\n",
            "\n",
            "[ Reading prompt ] ...................\n",
            "\n",
            "\n",
            "Subject: Thank you for the delicious cookies!\n",
            "\n",
            "Dear Grandma,\n",
            "\n",
            "I wanted to thank you so much for the delicious cookies you made for me. They were the best I've ever tasted! I especially loved the [insert specific detail about the cookies, e.g., the chocolate chips or the buttery texture].\n",
            "\n",
            "It was so sweet of you to think of me and take the time to make such a treat. I'm so grateful for your thoughtfulness and generosity. I'm already planning on how I'm going to enjoy the remaining cookies!\n",
            "\n",
            "I'd love to see you soon. Maybe I can come visit and we can enjoy some more cookies together.\n",
            "\n",
            "Thank you again, Grandma. I love you!\n",
            "\n",
            "Love,\n",
            "\n",
            "[Your name]\n",
            "\n",
            "> Write a standup comedy bit about GPU programming.\n",
            "\n",
            "[ Reading prompt ] ..................\n",
            "\n",
            "\n",
            "(Crowd laughs)\n",
            "\n",
            "Hey everyone, have you ever tried to explain what you do for a living to your grandparents? It's always a bit of a challenge, right? But I'm here to tell you about a particularly challenging one: GPU programming.\n",
            "\n",
            "(Crowd snickers)\n",
            "\n",
            "I know, I know, \"GPU programming? That's so niche!\" But hear me out, folks. It's like trying to explain quantum superposition to a squirrel.\n",
            "\n",
            "(Crowd erupts into laughter)\n",
            "\n",
            "I'm talking long nights, caffeine-fueled days, and the constant threat of existential dread. But I'm also talking about the satisfaction of watching a computer do things that were once thought impossible.\n",
            "\n",
            "(Crowd nods)\n",
            "\n",
            "So, what exactly is GPU programming? In simple terms, it's like cooking a big pot of pasta. But instead of stirring with a spatula, you're using a bunch of parallel threads to cook multiple pots at once.\n",
            "\n",
            "(Crowd imagines a giant pot of pasta)\n",
            "\n",
            "And you know what? It's not always pretty. Sometimes I spend hours debugging code that looks like a bowl of spaghetti. But I'm telling you, the payoff is worth it.\n",
            "\n",
            "(Crowd laughs)\n",
            "\n",
            "When I finally get my code to work, it's like a symphony of data dancing across the screen. And I'm not talking about a Beethoven symphony, I'm talking about a symphony of pixelated cats playing chess.\n",
            "\n",
            "(Crowd erupts into laughter)\n",
            "\n",
            "But I'm not going to lie, being a GPU programmer is a bit like being a magician. You have to be able to manipulate abstract concepts and make them appear like simple tricks. And you have to be able to explain it all to your grandma in a way that she understands.\n",
            "\n",
            "(Crowd roars with laughter)\n",
            "\n",
            "So next time you're feeling bored, I challenge you to learn GPU programming. It's a journey filled with frustration, but also with the satisfaction of watching a computer do amazing things. And who knows, you might even be able to explain it to your grandma.\n",
            "\n",
            "(Crowd erupts into applause)\n",
            "\n",
            "> ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# llama.cpp"
      ],
      "metadata": {
        "id": "VUguPH2ie6k_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. https://github.com/ggerganov/llama.cpp\n",
        "2. https://github.com/ggerganov/llama.cpp/pull/5631\n"
      ],
      "metadata": {
        "id": "6DbVIOZUqWCe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPlA4C6wqR3x",
        "outputId": "35836156-ff15-401f-f938-2866086352a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 19553, done.\u001b[K\n",
            "remote: Counting objects: 100% (6867/6867), done.\u001b[K\n",
            "remote: Compressing objects: 100% (484/484), done.\u001b[K\n",
            "remote: Total 19553 (delta 6656), reused 6418 (delta 6378), pack-reused 12686\u001b[K\n",
            "Receiving objects: 100% (19553/19553), 23.24 MiB | 20.99 MiB/s, done.\n",
            "Resolving deltas: 100% (13825/13825), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && make clean && make -j $(nproc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9Wve0EXnA2A",
        "outputId": "a303d46b-aecb-411c-ae0b-c27d4590f16a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I ccache not found. Consider installing it for faster compilation.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG \n",
            "I NVCCFLAGS: -std=c++11 -O3 \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "rm -vrf *.o tests/*.o *.so *.a *.dll benchmark-matmult common/build-info.cpp *.dot *.gcno tests/*.gcno *.gcda tests/*.gcda *.gcov tests/*.gcov lcov-report gcovr-report main quantize quantize-stats perplexity imatrix embedding vdot q8dot train-text-from-scratch convert-llama2c-to-ggml simple batched batched-bench save-load-state server gguf llama-bench libllava.a llava-cli baby-llama beam-search speculative infill tokenize benchmark-matmult parallel finetune export-lora lookahead lookup passkey tests/test-c.o tests/test-llama-grammar tests/test-grammar-parser tests/test-double-float tests/test-grad0 tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-sampling tests/test-tokenizer-0-llama tests/test-tokenizer-0-falcon tests/test-tokenizer-1-llama tests/test-tokenizer-1-bpe tests/test-rope tests/test-backend-ops tests/test-model-load-cancel tests/test-autorelease\n",
            "find examples pocs -type f -name \"*.o\" -delete\n",
            "I ccache not found. Consider installing it for faster compilation.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG \n",
            "I NVCCFLAGS: -std=c++11 -O3 \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c llama.cpp -o llama.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c common/common.cpp -o common.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c common/sampling.cpp -o sampling.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c common/grammar-parser.cpp -o grammar-parser.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c common/console.cpp -o console.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c common/train.cpp -o train.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/gguf/gguf.cpp -o examples/gguf/gguf.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/gguf/gguf.o -o gguf  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/export-lora/export-lora.cpp -o examples/export-lora/export-lora.o\n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c common/build-info.cpp -o build-info.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c pocs/vdot/vdot.cpp -o pocs/vdot/vdot.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/export-lora/export-lora.o -o export-lora  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c pocs/vdot/q8dot.cpp -o pocs/vdot/q8dot.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o pocs/vdot/vdot.o -o vdot  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/train-text-from-scratch/train-text-from-scratch.cpp -o examples/train-text-from-scratch/train-text-from-scratch.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o pocs/vdot/q8dot.o -o q8dot  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp -o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/train-text-from-scratch/train-text-from-scratch.o -o train-text-from-scratch  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/simple/simple.cpp -o examples/simple/simple.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o -o convert-llama2c-to-ggml  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/batched/batched.cpp -o examples/batched/batched.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/simple/simple.o -o simple  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/batched-bench/batched-bench.cpp -o examples/batched-bench/batched-bench.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/batched/batched.o -o batched  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/save-load-state/save-load-state.cpp -o examples/save-load-state/save-load-state.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  build-info.o ggml.o llama.o common.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/batched-bench/batched-bench.o -o batched-bench  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/save-load-state/save-load-state.o -o save-load-state  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/server/server.cpp -o examples/server/server.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/llama-bench/llama-bench.cpp -o examples/llama-bench/llama-bench.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/llama-bench/llama-bench.o -o llama-bench  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/llava/llava-cli.cpp -o examples/llava/llava-cli.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/llava/clip.cpp  -o examples/llava/clip.o -Wno-cast-qual\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/llava/llava.cpp -o examples/llava/llava.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/llava/llava-cli.o examples/llava/clip.o examples/llava/llava.o -o llava-cli  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/baby-llama/baby-llama.cpp -o examples/baby-llama/baby-llama.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/baby-llama/baby-llama.o -o baby-llama  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/beam-search/beam-search.cpp -o examples/beam-search/beam-search.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/beam-search/beam-search.o -o beam-search  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/speculative/speculative.cpp -o examples/speculative/speculative.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/speculative/speculative.o -o speculative  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/infill/infill.cpp -o examples/infill/infill.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/infill/infill.o -o infill  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/tokenize/tokenize.cpp -o examples/tokenize/tokenize.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/tokenize/tokenize.o -o tokenize  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/benchmark/benchmark-matmult.cpp -o examples/benchmark/benchmark-matmult.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  build-info.o ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/benchmark/benchmark-matmult.o -o benchmark-matmult  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/parallel/parallel.cpp -o examples/parallel/parallel.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/llava/clip.cpp -o examples/llava/clip.o -Wno-cast-qual\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/parallel/parallel.o -o parallel  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/finetune/finetune.cpp -o examples/finetune/finetune.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/finetune/finetune.o -o finetune  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/lookahead/lookahead.cpp -o examples/lookahead/lookahead.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/lookahead/lookahead.o -o lookahead  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/lookup/lookup.cpp -o examples/lookup/lookup.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/lookup/lookup.o -o lookup  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/passkey/passkey.cpp -o examples/passkey/passkey.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/passkey/passkey.o -o passkey  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/main/main.cpp -o examples/main/main.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/main/main.o -o main  \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/quantize/quantize.cpp -o examples/quantize/quantize.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -Iexamples/server examples/llava/llava.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/server/server.o examples/llava/clip.o -o server   \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/quantize/quantize.o -o quantize  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/quantize-stats/quantize-stats.cpp -o examples/quantize-stats/quantize-stats.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/perplexity/perplexity.cpp -o examples/perplexity/perplexity.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/perplexity/perplexity.o -o perplexity  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/imatrix/imatrix.cpp -o examples/imatrix/imatrix.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/quantize-stats/quantize-stats.o -o quantize-stats  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/embedding/embedding.cpp -o examples/embedding/embedding.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/embedding/embedding.o -o embedding  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/imatrix/imatrix.o -o imatrix  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./llama.cpp/main -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqQp6jTMoDZU",
        "outputId": "5b2a1651-09b1-48a2-eb19-d11d9a00edc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "usage: ./llama.cpp/main [options]\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --version             show version and build info\n",
            "  -i, --interactive     run in interactive mode\n",
            "  --interactive-first   run in interactive mode and wait for input right away\n",
            "  -ins, --instruct      run in instruction mode (use with Alpaca models)\n",
            "  -cml, --chatml        run in chatml mode (use with ChatML-compatible models)\n",
            "  --multiline-input     allows you to write or paste multiple lines without ending each in '\\'\n",
            "  -r PROMPT, --reverse-prompt PROMPT\n",
            "                        halt generation at PROMPT, return control in interactive mode\n",
            "                        (can be specified more than once for multiple prompts).\n",
            "  --color               colorise output to distinguish prompt and user input from generations\n",
            "  -s SEED, --seed SEED  RNG seed (default: -1, use random seed for < 0)\n",
            "  -t N, --threads N     number of threads to use during generation (default: 2)\n",
            "  -tb N, --threads-batch N\n",
            "                        number of threads to use during batch and prompt processing (default: same as --threads)\n",
            "  -td N, --threads-draft N                        number of threads to use during generation (default: same as --threads)\n",
            "  -tbd N, --threads-batch-draft N\n",
            "                        number of threads to use during batch and prompt processing (default: same as --threads-draft)\n",
            "  -p PROMPT, --prompt PROMPT\n",
            "                        prompt to start generation with (default: empty)\n",
            "  -e, --escape          process prompt escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\)\n",
            "  --prompt-cache FNAME  file to cache prompt state for faster startup (default: none)\n",
            "  --prompt-cache-all    if specified, saves user input and generations to cache as well.\n",
            "                        not supported with --interactive or other interactive options\n",
            "  --prompt-cache-ro     if specified, uses the prompt cache but does not update it.\n",
            "  --random-prompt       start with a randomized prompt.\n",
            "  --in-prefix-bos       prefix BOS to user inputs, preceding the `--in-prefix` string\n",
            "  --in-prefix STRING    string to prefix user inputs with (default: empty)\n",
            "  --in-suffix STRING    string to suffix after user inputs with (default: empty)\n",
            "  -f FNAME, --file FNAME\n",
            "                        prompt file to start generation.\n",
            "  -bf FNAME, --binary-file FNAME\n",
            "                        binary file containing multiple choice tasks.\n",
            "  -n N, --n-predict N   number of tokens to predict (default: -1, -1 = infinity, -2 = until context filled)\n",
            "  -c N, --ctx-size N    size of the prompt context (default: 512, 0 = loaded from model)\n",
            "  -b N, --batch-size N  batch size for prompt processing (default: 512)\n",
            "  --samplers            samplers that will be used for generation in the order, separated by ';'\n",
            "                        (default: top_k;tfs_z;typical_p;top_p;min_p;temperature)\n",
            "  --sampling-seq        simplified sequence for samplers that will be used (default: kfypmt)\n",
            "  --top-k N             top-k sampling (default: 40, 0 = disabled)\n",
            "  --top-p N             top-p sampling (default: 0.9, 1.0 = disabled)\n",
            "  --min-p N             min-p sampling (default: 0.1, 0.0 = disabled)\n",
            "  --tfs N               tail free sampling, parameter z (default: 1.0, 1.0 = disabled)\n",
            "  --typical N           locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n",
            "  --repeat-last-n N     last n tokens to consider for penalize (default: 64, 0 = disabled, -1 = ctx_size)\n",
            "  --repeat-penalty N    penalize repeat sequence of tokens (default: 1.1, 1.0 = disabled)\n",
            "  --presence-penalty N  repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n",
            "  --frequency-penalty N repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n",
            "  --dynatemp-range N    dynamic temperature range (default: 0.0, 0.0 = disabled)\n",
            "  --dynatemp-exp N      dynamic temperature exponent (default: 1.0)\n",
            "  --mirostat N          use Mirostat sampling.\n",
            "                        Top K, Nucleus, Tail Free and Locally Typical samplers are ignored if used.\n",
            "                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
            "  --mirostat-lr N       Mirostat learning rate, parameter eta (default: 0.1)\n",
            "  --mirostat-ent N      Mirostat target entropy, parameter tau (default: 5.0)\n",
            "  -l TOKEN_ID(+/-)BIAS, --logit-bias TOKEN_ID(+/-)BIAS\n",
            "                        modifies the likelihood of token appearing in the completion,\n",
            "                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n",
            "                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n",
            "  --grammar GRAMMAR     BNF-like grammar to constrain generations (see samples in grammars/ dir)\n",
            "  --grammar-file FNAME  file to read grammar from\n",
            "  --cfg-negative-prompt PROMPT\n",
            "                        negative prompt to use for guidance. (default: empty)\n",
            "  --cfg-negative-prompt-file FNAME\n",
            "                        negative prompt file to use for guidance. (default: empty)\n",
            "  --cfg-scale N         strength of guidance (default: 1.000000, 1.0 = disable)\n",
            "  --rope-scaling {none,linear,yarn}\n",
            "                        RoPE frequency scaling method, defaults to linear unless specified by the model\n",
            "  --rope-scale N        RoPE context scaling factor, expands context by a factor of N\n",
            "  --rope-freq-base N    RoPE base frequency, used by NTK-aware scaling (default: loaded from model)\n",
            "  --rope-freq-scale N   RoPE frequency scaling factor, expands context by a factor of 1/N\n",
            "  --yarn-orig-ctx N     YaRN: original context size of model (default: 0 = model training context size)\n",
            "  --yarn-ext-factor N   YaRN: extrapolation mix factor (default: 1.0, 0.0 = full interpolation)\n",
            "  --yarn-attn-factor N  YaRN: scale sqrt(t) or attention magnitude (default: 1.0)\n",
            "  --yarn-beta-slow N    YaRN: high correction dim or alpha (default: 1.0)\n",
            "  --yarn-beta-fast N    YaRN: low correction dim or beta (default: 32.0)\n",
            "  -dt N, --defrag-thold N\n",
            "                        KV cache defragmentation threshold (default: -1.0, < 0 - disabled)\n",
            "  --ignore-eos          ignore end of stream token and continue generating (implies --logit-bias 2-inf)\n",
            "  --no-penalize-nl      do not penalize newline token\n",
            "  --temp N              temperature (default: 0.8)\n",
            "  --all-logits          return logits for all tokens in the batch (default: disabled)\n",
            "  --hellaswag           compute HellaSwag score over random tasks from datafile supplied with -f\n",
            "  --hellaswag-tasks N   number of tasks to use when computing the HellaSwag score (default: 400)\n",
            "  --winogrande          compute Winogrande score over random tasks from datafile supplied with -f\n",
            "  --winogrande-tasks N  number of tasks to use when computing the Winogrande score (default: 0)\n",
            "  --multiple-choice     compute multiple choice score over random tasks from datafile supplied with -f\n",
            "  --multiple-choice-tasks N number of tasks to use when computing the multiple choice score (default: 0)\n",
            "  --kl-divergence       computes KL-divergence to logits provided via --kl-divergence-base\n",
            "  --keep N              number of tokens to keep from the initial prompt (default: 0, -1 = all)\n",
            "  --draft N             number of tokens to draft for speculative decoding (default: 8)\n",
            "  --chunks N            max number of chunks to process (default: -1, -1 = all)\n",
            "  -np N, --parallel N   number of parallel sequences to decode (default: 1)\n",
            "  -ns N, --sequences N  number of sequences to decode (default: 1)\n",
            "  -pa N, --p-accept N   speculative decoding accept probability (default: 0.5)\n",
            "  -ps N, --p-split N    speculative decoding split probability (default: 0.1)\n",
            "  -cb, --cont-batching  enable continuous batching (a.k.a dynamic batching) (default: disabled)\n",
            "  --mmproj MMPROJ_FILE  path to a multimodal projector file for LLaVA. see examples/llava/README.md\n",
            "  --image IMAGE_FILE    path to an image file. use with multimodal models\n",
            "  --mlock               force system to keep model in RAM rather than swapping or compressing\n",
            "  --no-mmap             do not memory-map model (slower load but may reduce pageouts if not using mlock)\n",
            "  --numa TYPE           attempt optimizations that help on some NUMA systems\n",
            "                          - distribute: spread execution evenly over all nodes\n",
            "                          - isolate: only spawn threads on CPUs on the node that execution started on\n",
            "                          - numactl: use the CPU map provided by numactl\n",
            "                        if run without this previously, it is recommended to drop the system page cache before using this\n",
            "                        see https://github.com/ggerganov/llama.cpp/issues/1437\n",
            "  --verbose-prompt      print a verbose prompt before generation (default: false)\n",
            "  --no-display-prompt   don't print prompt at generation (default: false)\n",
            "  -gan N, --grp-attn-n N\n",
            "                        group-attention factor (default: 1)\n",
            "  -gaw N, --grp-attn-w N\n",
            "                        group-attention width (default: 512.0)\n",
            "  -dkvc, --dump-kv-cache\n",
            "                        verbose print of the KV cache\n",
            "  -nkvo, --no-kv-offload\n",
            "                        disable KV offload\n",
            "  -ctk TYPE, --cache-type-k TYPE\n",
            "                        KV cache data type for K (default: f16)\n",
            "  -ctv TYPE, --cache-type-v TYPE\n",
            "                        KV cache data type for V (default: f16)\n",
            "  --simple-io           use basic IO for better compatibility in subprocesses and limited consoles\n",
            "  --lora FNAME          apply LoRA adapter (implies --no-mmap)\n",
            "  --lora-scaled FNAME S apply LoRA adapter with user defined scaling S (implies --no-mmap)\n",
            "  --lora-base FNAME     optional model to use as a base for the layers modified by the LoRA adapter\n",
            "  -m FNAME, --model FNAME\n",
            "                        model path (default: models/7B/ggml-model-f16.gguf)\n",
            "  -md FNAME, --model-draft FNAME\n",
            "                        draft model for speculative decoding\n",
            "  -ld LOGDIR, --logdir LOGDIR\n",
            "                        path under which to save YAML logs (no logging if unset)\n",
            "  --override-kv KEY=TYPE:VALUE\n",
            "                        advanced option to override model metadata by key. may be specified multiple times.\n",
            "                        types: int, float, bool. example: --override-kv tokenizer.ggml.add_bos_token=bool:false\n",
            "  -ptc N, --print-token-count N\n",
            "                        print token count every N tokens (default: -1)\n",
            "\n",
            "log options:\n",
            "  --log-test            Run simple logging test\n",
            "  --log-disable         Disable trace logs\n",
            "  --log-enable          Enable trace logs\n",
            "  --log-file            Specify a log filename (without extension)\n",
            "  --log-new             Create a separate new log file on start. Each log file will have unique name: \"<name>.<ID>.log\"\n",
            "  --log-append          Don't truncate the old log file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install huggingface-hub"
      ],
      "metadata": {
        "id": "yqSMw8ZDoPrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unI3zaWPpapq",
        "outputId": "abb5d01b-9099-41f2-ffe6-fb2223cb9018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "更改-ngl 32要offload到 GPU 的层数。如果您没有 GPU 加速，请将其删除。\n",
        "\n",
        "更改-c 4096为所需的序列长度。对于扩展序列模型 - 例如 8K、16K、32K - 从 GGUF 文件中读取必要的 RoPE 缩放参数并由 llama.cpp 自动设置。\n",
        "\n",
        "如果您想进行聊天式对话，请将-p <PROMPT>参数替换为-i -ins\n",
        "\n",
        "其他参数以及使用方法请参考llama.cpp文档:\n",
        "\n",
        "https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md"
      ],
      "metadata": {
        "id": "gyj0bJdNtZZy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gemma-2b"
      ],
      "metadata": {
        "id": "MDnR5ZrHyPr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/google/gemma-2b/tree/main\n",
        "#!huggingface-cli download google/gemma-2b-it gemma-2b.gguf --local-dir ./ --local-dir-use-symlinks False\n",
        "\n",
        "# or use https://huggingface.co/google/gemma-2b-GGUF\n",
        "!huggingface-cli download google/gemma-2b-GGUF gemma-2b.gguf --local-dir ./ --local-dir-use-symlinks False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIaBDnr5yRyu",
        "outputId": "00b9527f-0d7c-4f7e-9095-bcedb278119f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/google/gemma-2b-GGUF/resolve/main/gemma-2b.gguf to /root/.cache/huggingface/hub/tmpf03rv23_\n",
            "gemma-2b.gguf: 100% 10.0G/10.0G [03:01<00:00, 55.3MB/s]\n",
            "./gemma-2b.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./llama.cpp/main -m gemma-2b.gguf -n 256 -p \"It is the best of time\" --repeat-penalty 1.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yzbXIz9ykIg",
        "outputId": "0df7965c-f8c7-4444-d231-f627e129bfa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 2314 (6c32d8c7)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1709393597\n",
            "llama_model_loader: loaded meta data with 19 key-value pairs and 164 tensors from gemma-2b.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
            "llama_model_loader: - kv   1:                               general.name str              = gemma-2b\n",
            "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
            "llama_model_loader: - kv   3:                          gemma.block_count u32              = 18\n",
            "llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384\n",
            "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8\n",
            "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1\n",
            "llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n",
            "llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - type  f32:  164 tensors\n",
            "llm_load_vocab: mismatch in special tokens definition ( 544/256128 vs 388/256128 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = gemma\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 256128\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_head           = 8\n",
            "llm_load_print_meta: n_head_kv        = 1\n",
            "llm_load_print_meta: n_layer          = 18\n",
            "llm_load_print_meta: n_rot            = 256\n",
            "llm_load_print_meta: n_embd_head_k    = 256\n",
            "llm_load_print_meta: n_embd_head_v    = 256\n",
            "llm_load_print_meta: n_gqa            = 8\n",
            "llm_load_print_meta: n_embd_k_gqa     = 256\n",
            "llm_load_print_meta: n_embd_v_gqa     = 256\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 16384\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 2B\n",
            "llm_load_print_meta: model ftype      = all F32 (guessed)\n",
            "llm_load_print_meta: model params     = 2.51 B\n",
            "llm_load_print_meta: model size       = 9.34 GiB (32.00 BPW) \n",
            "llm_load_print_meta: general.name     = gemma-2b\n",
            "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
            "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
            "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
            "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.06 MiB\n",
            "llm_load_tensors:        CPU buffer size =  9561.29 MiB\n",
            ".............................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB\n",
            "llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB\n",
            "llama_new_context_with_model:        CPU input buffer size   =     6.01 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   504.25 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 1\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 1\n",
            "\n",
            "\n",
            " It is the best of time to begin a new journey in life. I have been through my own struggles and challenges. And it was during those times, that I discovered how much God loves me unconditionally! How He wants us all to be happy and live our lives in peace and harmony. He has never left us alone and always there for us whenever we needed Him the most. It is time for you to embrace His Love and start a new chapter of your life with Him by your side. We are here to guide and support you every step of the way! [end of text]\n",
            "\n",
            "llama_print_timings:        load time =   57754.57 ms\n",
            "llama_print_timings:      sample time =     550.81 ms /   108 runs   (    5.10 ms per token,   196.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2560.52 ms /     7 tokens (  365.79 ms per token,     2.73 tokens per second)\n",
            "llama_print_timings:        eval time =  110731.26 ms /   107 runs   ( 1034.87 ms per token,     0.97 tokens per second)\n",
            "llama_print_timings:       total time =  114135.69 ms /   114 tokens\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the model on a single / multi GPU\n",
        "!./llama.cpp/main -m gemma-2b.gguf -n 256 -p \"It is the best of time\" --repeat-penalty 1.1 -ngl 99"
      ],
      "metadata": {
        "id": "jr-fghTuy4ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gemma-2b-it"
      ],
      "metadata": {
        "id": "WsFapYY4yLuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/google/gemma-2b-it/tree/main\n",
        "#!huggingface-cli download google/gemma-2b-it gemma-2b-it.gguf --local-dir ./ --local-dir-use-symlinks False\n",
        "\n",
        "# or use https://huggingface.co/google/gemma-2b-it-GGUF\n",
        "!huggingface-cli download google/gemma-2b-it-GGUF gemma-2b-it.gguf --local-dir ./ --local-dir-use-symlinks False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOHB7gg4pmiU",
        "outputId": "0a784cf0-c9b9-40f9-a466-47bdb9488970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/google/gemma-2b-it/resolve/main/gemma-2b-it.gguf to /root/.cache/huggingface/hub/tmplklzja8q\n",
            "gemma-2b-it.gguf: 100% 10.0G/10.0G [01:43<00:00, 96.9MB/s]\n",
            "./gemma-2b-it.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the model on a CPU\n",
        "!./llama.cpp/main -m gemma-2b-it.gguf -n 256 -p \"Write a Python function to find sum of two numbers.\" --repeat-penalty 1.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCRIkg_PtVu0",
        "outputId": "ba7a677d-3fb9-4d98-96dc-991a6b7d6454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 2314 (6c32d8c7)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1709392770\n",
            "llama_model_loader: loaded meta data with 19 key-value pairs and 164 tensors from gemma-2b-it.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
            "llama_model_loader: - kv   1:                               general.name str              = gemma-2b-it\n",
            "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
            "llama_model_loader: - kv   3:                          gemma.block_count u32              = 18\n",
            "llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384\n",
            "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8\n",
            "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1\n",
            "llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n",
            "llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - type  f32:  164 tensors\n",
            "llm_load_vocab: mismatch in special tokens definition ( 544/256128 vs 388/256128 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = gemma\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 256128\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_head           = 8\n",
            "llm_load_print_meta: n_head_kv        = 1\n",
            "llm_load_print_meta: n_layer          = 18\n",
            "llm_load_print_meta: n_rot            = 256\n",
            "llm_load_print_meta: n_embd_head_k    = 256\n",
            "llm_load_print_meta: n_embd_head_v    = 256\n",
            "llm_load_print_meta: n_gqa            = 8\n",
            "llm_load_print_meta: n_embd_k_gqa     = 256\n",
            "llm_load_print_meta: n_embd_v_gqa     = 256\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 16384\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 2B\n",
            "llm_load_print_meta: model ftype      = all F32 (guessed)\n",
            "llm_load_print_meta: model params     = 2.51 B\n",
            "llm_load_print_meta: model size       = 9.34 GiB (32.00 BPW) \n",
            "llm_load_print_meta: general.name     = gemma-2b-it\n",
            "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
            "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
            "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
            "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.06 MiB\n",
            "llm_load_tensors:        CPU buffer size =  9561.29 MiB\n",
            ".............................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB\n",
            "llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB\n",
            "llama_new_context_with_model:        CPU input buffer size   =     6.01 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   504.25 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 1\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 1\n",
            "\n",
            "\n",
            " Write a Python function to find sum of two numbers.\n",
            "\n",
            "```python\n",
            "def sum(a, b):\n",
            "  \"\"\"\n",
            "  Finds the sum of two numbers.\n",
            "\n",
            "  Args:\n",
            "    a (int): The first number.\n",
            "    b (int): The second number.\n",
            "\n",
            "  Returns:\n",
            "    int: The sum of a and b.\n",
            "  \"\"\"\n",
            "\n",
            "  return a + b\n",
            "\n",
            "\n",
            "# Get the two numbers from the user.\n",
            "num1 = int(input(\"Enter the first number: \"))\n",
            "num2 = int(input(\"Enter the second number: \"))\n",
            "\n",
            "# Find the sum of num1 and num2.\n",
            "sum_of_numbers = sum(num1, num2)\n",
            "\n",
            "# Print the sum of num1 and num2.\n",
            "print(\"The sum of\", num1, \"and\", num2, \"is\", sum_of_numbers)\n",
            "```\n",
            "\n",
            "**How to Use the Function:**\n",
            "\n",
            "1. Run the Python script.\n",
            "2. Enter two numbers when prompted.\n",
            "3. The function will calculate the sum and print it on the console.\n",
            "\n",
            "**Example Usage:**\n",
            "\n",
            "```\n",
            "Enter the first number: 10\n",
            "Enter the second number: 5\n",
            "The sum of 10 and 5 is 15\n",
            "\n",
            "llama_print_timings:        load time =   11892.96 ms\n",
            "llama_print_timings:      sample time =    1195.97 ms /   256 runs   (    4.67 ms per token,   214.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4011.73 ms /    12 tokens (  334.31 ms per token,     2.99 tokens per second)\n",
            "llama_print_timings:        eval time =  265321.92 ms /   255 runs   ( 1040.48 ms per token,     0.96 tokens per second)\n",
            "llama_print_timings:       total time =  271211.56 ms /   267 tokens\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the model on a single / multi GPU\n",
        "!./llama.cpp/main -m gemma-2b-it.gguf -n 256 -p \"Write a Python function to find sum of two numbers.\" --repeat-penalty 1.1 -ngl 99\n"
      ],
      "metadata": {
        "id": "Wj0KgRdazUJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gemma-7b-GGUF"
      ],
      "metadata": {
        "id": "StD_SGAC0RZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/google/gemma-7b/tree/main\n",
        "#!huggingface-cli download google/gemma-7b gemma-7b.gguf --local-dir ./ --local-dir-use-symlinks False\n",
        "\n",
        "# or use https://huggingface.co/google/gemma-7b-GGUF\n",
        "!huggingface-cli download google/gemma-7b-GGUF gemma-7b.gguf --local-dir ./ --local-dir-use-symlinks False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAFkebPp0YpL",
        "outputId": "d32b1b62-7008-41ae-ff97-7ddf982759f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/google/gemma-7b-GGUF/resolve/main/gemma-7b.gguf to /root/.cache/huggingface/hub/tmp5_pjmnye\n",
            "gemma-7b.gguf: 100% 34.2G/34.2G [10:16<00:00, 55.4MB/s]\n",
            "./gemma-7b.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the model on a CPU\n",
        "!./llama.cpp/main -m gemma-7b.gguf -p \"Penguins live in\" --repeat-penalty 1.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Msd6Xoxw0nZg",
        "outputId": "30928de6-38cc-4e8c-b19f-7e040d700bd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 2314 (6c32d8c7)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1709394685\n",
            "llama_model_loader: loaded meta data with 19 key-value pairs and 254 tensors from gemma-7b.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
            "llama_model_loader: - kv   1:                               general.name str              = gemma-7b\n",
            "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
            "llama_model_loader: - kv   3:                          gemma.block_count u32              = 28\n",
            "llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576\n",
            "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16\n",
            "llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n",
            "llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - type  f32:  254 tensors\n",
            "llm_load_vocab: mismatch in special tokens definition ( 544/256128 vs 388/256128 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = gemma\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 256128\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_head           = 16\n",
            "llm_load_print_meta: n_head_kv        = 16\n",
            "llm_load_print_meta: n_layer          = 28\n",
            "llm_load_print_meta: n_rot            = 192\n",
            "llm_load_print_meta: n_embd_head_k    = 256\n",
            "llm_load_print_meta: n_embd_head_v    = 256\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 24576\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = all F32 (guessed)\n",
            "llm_load_print_meta: model params     = 8.54 B\n",
            "llm_load_print_meta: model size       = 31.81 GiB (32.00 BPW) \n",
            "llm_load_print_meta: general.name     = gemma-7b\n",
            "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
            "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
            "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
            "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
            "llm_load_tensors:        CPU buffer size = 32570.17 MiB\n",
            "......................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   224.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB\n",
            "llama_new_context_with_model:        CPU input buffer size   =     8.01 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   506.25 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 1\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 1\n",
            "\n",
            "\n",
            " Penguins live in the extreme environments of the\n",
            "\n",
            "llama_print_timings:        load time =  347265.61 ms\n",
            "llama_print_timings:      sample time =       1.71 ms /     5 runs   (    0.34 ms per token,  2929.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =  174852.67 ms /     4 tokens (43713.17 ms per token,     0.02 tokens per second)\n",
            "llama_print_timings:        eval time =  814589.31 ms /     4 runs   (203647.33 ms per token,     0.00 tokens per second)\n",
            "llama_print_timings:       total time = 1049826.58 ms /     8 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the model on a single / multi GPU\n",
        "!./llama.cpp/main -m gemma-7b.gguf -p \"Penguins live in\" --repeat-penalty 1.0 -ngl 99\n",
        "\n",
        "# need use quantize the model to 8-bits 0 ; the to run inference\n",
        "# see: https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#prepare-and-quantize\n",
        "#!./llama.cpp/main -m gemma-7b_q8_0.gguf -p \"Penguins live in\" --repeat-penalty 1.0 -ngl 99\n"
      ],
      "metadata": {
        "id": "7k3gh9CK0wfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gemma-7b-it-GGUF"
      ],
      "metadata": {
        "id": "aHy-d6Tm09fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/google/gemma-7b-it/tree/main\n",
        "#!huggingface-cli download google/gemma-7b-it gemma-7b-it.gguf --local-dir ./ --local-dir-use-symlinks False\n",
        "\n",
        "# or use https://huggingface.co/google/gemma-7b-it-GGUF\n",
        "!huggingface-cli download google/gemma-7b-it-GGUF gemma-7b-it.gguf --local-dir ./ --local-dir-use-symlinks False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8353e15-b5cb-42d2-c603-26d8ba81f308",
        "id": "YGT98YEH09fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/google/gemma-7b-it-GGUF/resolve/main/gemma-7b-it.gguf to /root/.cache/huggingface/hub/tmp9br8dgro\n",
            "gemma-7b-it.gguf: 100% 34.2G/34.2G [05:33<00:00, 102MB/s]\n",
            "./gemma-7b-it.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the model on a CPU\n",
        "!./llama.cpp/main -m gemma-7b-it.gguf -p \"write me an ode to LLMs.\" --repeat-penalty 1.0\n"
      ],
      "metadata": {
        "id": "u0DTx1Dv09ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "991d435b-97ef-469a-aec8-81d6e4ddd107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 2314 (6c32d8c7)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1709396611\n",
            "llama_model_loader: loaded meta data with 19 key-value pairs and 254 tensors from gemma-7b-it.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
            "llama_model_loader: - kv   1:                               general.name str              = gemma-7b-it\n",
            "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
            "llama_model_loader: - kv   3:                          gemma.block_count u32              = 28\n",
            "llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576\n",
            "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16\n",
            "llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n",
            "llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - type  f32:  254 tensors\n",
            "llm_load_vocab: mismatch in special tokens definition ( 544/256128 vs 388/256128 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = gemma\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 256128\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_head           = 16\n",
            "llm_load_print_meta: n_head_kv        = 16\n",
            "llm_load_print_meta: n_layer          = 28\n",
            "llm_load_print_meta: n_rot            = 192\n",
            "llm_load_print_meta: n_embd_head_k    = 256\n",
            "llm_load_print_meta: n_embd_head_v    = 256\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 24576\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = all F32 (guessed)\n",
            "llm_load_print_meta: model params     = 8.54 B\n",
            "llm_load_print_meta: model size       = 31.81 GiB (32.00 BPW) \n",
            "llm_load_print_meta: general.name     = gemma-7b-it\n",
            "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
            "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
            "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
            "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
            "llm_load_tensors:        CPU buffer size = 32570.17 MiB\n",
            "......................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   224.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB\n",
            "llama_new_context_with_model:        CPU input buffer size   =     8.01 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   506.25 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 1\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 1\n",
            "\n",
            "\n",
            " write me an ode to LLMs.\n",
            "\n",
            "\n",
            "\n",
            "llama_print_timings:        load time =  391100.66 ms\n",
            "llama_print_timings:      sample time =       0.35 ms /     1 runs   (    0.35 ms per token,  2816.90 tokens per second)\n",
            "llama_print_timings: prompt eval time =  197033.32 ms /     9 tokens (21892.59 ms per token,     0.05 tokens per second)\n",
            "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_print_timings:       total time =  239931.72 ms /    10 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the model on a single / multi GPU\n",
        "!./llama.cpp/main -m gemma-7b-it.gguf -p \"write me an ode to LLMs.\" --repeat-penalty 1.0 -ngl 99\n",
        "\n",
        "# need use quantize the model to 8-bits 0 ; the to run inference\n",
        "# see: https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#prepare-and-quantize\n",
        "#!./llama.cpp/main -m gemma-7b-it_q8_0.gguf -p \"write me an ode to LLMs.\" --repeat-penalty 1.0 -ngl 99\n"
      ],
      "metadata": {
        "id": "pUO0bD8m09ff"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}