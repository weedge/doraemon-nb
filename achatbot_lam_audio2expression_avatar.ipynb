{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP+gguPJHgd1H2nq1Y2fZUy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weedge/doraemon-nb/blob/main/achatbot_lam_audio2expression_avatar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBGEeSap28_v"
      },
      "outputs": [],
      "source": [
        "!cd /content && rm -rf achatbot && git clone --recursive https://github.com/ai-bot-pro/achatbot.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/achatbot"
      ],
      "metadata": {
        "id": "zZTN5bWg5wZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6481d695-b32d-4537-e5f3-9da75bb985ce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/achatbot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/pypi_achatbot.sh dev"
      ],
      "metadata": {
        "id": "OqPrkrdWMqoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"dist/achatbot-0.0.19-py3-none-any.whl[fastapi_bot_server,webrtc,silero_vad_analyzer,sense_voice_asr,openai_llm_processor,google_llm_processor,litellm_processor,together_ai,tts_edge,lam_audio2expression_avatar]\""
      ],
      "metadata": {
        "id": "T5LIpJsgL2DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spleeter==2.4.2"
      ],
      "metadata": {
        "id": "jM_EPgKUmZ0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U typing_extensions"
      ],
      "metadata": {
        "id": "0guE28j3usRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf==5.29.4"
      ],
      "metadata": {
        "id": "7KyIcofZwlYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep -E \"accelerate|peft|torch|mmcv|mmdet|mmpose|numpy|typing_extensions|protobuf\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgw0hUYxuaXa",
        "outputId": "f3049ef8-b4c9-4252-ad3a-acbc52c104eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accelerate                            1.8.1\n",
            "numpy                                 1.24.3\n",
            "peft                                  0.15.2\n",
            "protobuf                              5.29.4\n",
            "pytorch-wpe                           0.0.1\n",
            "torch                                 2.6.0+cu124\n",
            "torch-complex                         0.4.4\n",
            "torchao                               0.10.0\n",
            "torchaudio                            2.6.0+cu124\n",
            "torchdata                             0.11.0\n",
            "torchsummary                          1.5.1\n",
            "torchtune                             0.6.1\n",
            "torchvision                           0.21.0+cu124\n",
            "typing_extensions                     4.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# download model weights"
      ],
      "metadata": {
        "id": "JI-rLyFKPfuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/achatbot"
      ],
      "metadata": {
        "id": "zhvesghvRMl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download --quiet FunAudioLLM/SenseVoiceSmall  --local-dir ./models/FunAudioLLM/SenseVoiceSmall"
      ],
      "metadata": {
        "id": "Oiuagwe2IwS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download model weights\n",
        "!wget https://virutalbuy-public.oss-cn-hangzhou.aliyuncs.com/share/aigc3d/data/LAM/LAM_audio2exp_streaming.tar -P ./models/LAM_audio2exp/\n",
        "!tar -xzvf ./models/LAM_audio2exp/LAM_audio2exp_streaming.tar -C ./models/LAM_audio2exp && rm ./models/LAM_audio2exp/LAM_audio2exp_streaming.tar\n",
        "!git clone --depth 1 https://www.modelscope.cn/AI-ModelScope/wav2vec2-base-960h.git ./models/facebook/wav2vec2-base-960h"
      ],
      "metadata": {
        "id": "ltDPmhh3Io-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test"
      ],
      "metadata": {
        "id": "71Lecx0zRo-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/achatbot"
      ],
      "metadata": {
        "id": "KFwVamhMRolE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90847e2a-51e8-4fe7-af39-a8d1100ef4a3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/achatbot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.modules.avatar.lam_audio2expression \\\n",
        "    --audio_path test/audio_files/asr_example_zh.wav \\\n",
        "    --save_json_path resources/avatar/lam_audio2expression/expression.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iTRabaTJSJ9",
        "outputId": "3c5a7056-5eda-4b6a-f1ad-b0134277228d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 16:04:20.871473: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-07-01 16:04:20.921214: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-07-01 16:04:22.253345: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2025-07-01 16:04:24,821 INFO infer.py line 51 7587] => Loading config ...\n",
            "[2025-07-01 16:04:24,821 INFO infer.py line 58 7587] => Building model ...\n",
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at /content/achatbot/models/facebook/wav2vec2-base-960h and are newly initialized: ['lm_head.bias', 'lm_head.weight', 'masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[2025-07-01 16:04:24,985 INFO infer.py line 66 7587] Num params: 97912596\n",
            "[2025-07-01 16:04:25,390 INFO infer.py line 73 7587] Loading weight at: /content/achatbot/models/LAM_audio2exp/pretrained_models/lam_audio2exp_streaming.tar\n",
            "[2025-07-01 16:04:25,739 INFO infer.py line 85 7587] => Loaded weight '/content/achatbot/models/LAM_audio2exp/pretrained_models/lam_audio2exp_streaming.tar'\n",
            "Wav2Vec2Model is using Wav2Vec2SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` . Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "audio=array([ 9.1552734e-05,  9.1552734e-05,  9.1552734e-05, ...,\n",
            "       -1.5258789e-04, -9.1552734e-05, -6.1035156e-05], dtype=float32) sample_rate=16000 audio.shape[0]=88747\n",
            "  0% 0/6 [00:00<?, ?it/s]Inference time 0.02725696563720703\n",
            "output['expression'].shape=(30, 52)\n",
            "Inference time 0.026959896087646484\n",
            "output['expression'].shape=(30, 52)\n",
            "Inference time 0.026858806610107422\n",
            "output['expression'].shape=(30, 52)\n",
            "Inference time 0.02669692039489746\n",
            "output['expression'].shape=(30, 52)\n",
            " 67% 4/6 [00:00<00:00, 37.04it/s]Inference time 0.026690959930419922\n",
            "output['expression'].shape=(30, 52)\n",
            "Inference time 0.02685689926147461\n",
            "output['expression'].shape=(17, 52)\n",
            "100% 6/6 [00:00<00:00, 37.08it/s]\n",
            "all_exp.shape=(167, 52)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test fastapi websocket transport output processor with avatar data frame protobuf\n",
        "!WEIGHT_PATH=./models/LAM_audio2exp/pretrained_models/lam_audio2exp_streaming.tar \\\n",
        "    WAV2VEC_DIR=./models/facebook/wav2vec2-base-960h \\\n",
        "    SLEEP_TO_END_TIME_S=7 \\\n",
        "    python -m unittest test.integration.processors.test_lam_audio2expression_avatar_processor.TestLamAudio2ExpressionFastApiWebsocketProcessor.test_gen\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eWWVnFhJaAE",
        "outputId": "249e3b15-bb92-4755-bc29-d44bbfc6e60b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 16:16:18,576 - chat-bot - INFO - /content/achatbot/src/common/utils/wav.py:39 - read_wav_to_bytes - Channels: 1, Sample Width: 2, Frame Rate: 16000, Number of Frames: 88747\n",
            "2025-07-01 16:16:23.704060: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-07-01 16:16:23.755784: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-07-01 16:16:24.758479: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2025-07-01 16:16:25,762 - numexpr.utils - INFO - /usr/local/lib/python3.11/dist-packages/numexpr/utils.py:164 - _init_num_threads - NumExpr defaulting to 8 threads.\n",
            "2025-07-01 16:16:26,989 - asyncio - WARNING - /usr/lib/python3.11/asyncio/base_events.py:1931 - _run_once - Executing <Task finished name='Task-1' coro=<TestLamAudio2ExpressionFastApiWebsocketProcessor.asyncSetUp() done, defined at /content/achatbot/test/integration/processors/test_lam_audio2expression_avatar_processor.py:222> result=None created at /usr/lib/python3.11/asyncio/runners.py:100> took 8.411 seconds\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m12266\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "2025-07-01 16:16:27,013 - asyncio - WARNING - /usr/lib/python3.11/asyncio/base_events.py:1534 - create_server - error while attempting to bind on address ('::1', 4321, 0, 0): cannot assign requested address\n",
            "2025-07-01 16:16:27,014 - asyncio - INFO - /usr/lib/python3.11/asyncio/base_events.py:1567 - create_server - <Server sockets=(<asyncio.TransportSocket fd=24, family=2, type=1, proto=6, laddr=('127.0.0.1', 4321)>,)> is serving\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://localhost:4321\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     ('127.0.0.1', 54116) - \"WebSocket /\" [accepted]\n",
            "2025-07-01 16:16:28,023 - chat-bot - INFO - /content/achatbot/test/integration/processors/test_lam_audio2expression_avatar_processor.py:238 - websocket_endpoint - accept client: Address(host='127.0.0.1', port=54116)\n",
            "\u001b[32mINFO\u001b[0m:     connection open\n",
            "2025-07-01 16:16:28,031 - chat-bot - INFO - /content/achatbot/src/processors/avatar/help/speech_audio_slicer.py:77 - get_speech_audio_slice - input speech audio , end of speech True, duration 5.548s, current audio length 177538\n",
            "2025-07-01 16:16:28,032 - chat-bot - INFO - /content/achatbot/src/processors/avatar/lam_audio2expression_avatar_processor.py:139 - _audio2expression_loop - audio2expression loop started\n",
            "2025-07-01 16:16:28,034 - chat-bot - INFO - /content/achatbot/test/integration/processors/test_lam_audio2expression_avatar_processor.py:299 - connect2receive - Connection established.\n",
            "2025-07-01 16:16:28,034 - chat-bot - INFO - /content/achatbot/src/processors/avatar/lam_audio2expression_avatar_processor.py:150 - _audio2expression_loop - audio2expression input audio durtaion 1.0\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "2025-07-01 16:16:28,035 - chat-bot - INFO - /content/achatbot/src/processors/avatar/lam_audio2expression_avatar_processor.py:150 - _audio2expression_loop - audio2expression input audio durtaion 1.0\n",
            "2025-07-01 16:16:28,036 - chat-bot - INFO - /content/achatbot/src/processors/avatar/lam_audio2expression_avatar_processor.py:150 - _audio2expression_loop - audio2expression input audio durtaion 1.0\n",
            "2025-07-01 16:16:28,036 - chat-bot - INFO - /content/achatbot/src/processors/avatar/lam_audio2expression_avatar_processor.py:150 - _audio2expression_loop - audio2expression input audio durtaion 1.0\n",
            "2025-07-01 16:16:28,037 - chat-bot - INFO - /content/achatbot/src/processors/avatar/lam_audio2expression_avatar_processor.py:150 - _audio2expression_loop - audio2expression input audio durtaion 1.0\n",
            "2025-07-01 16:16:28,037 - chat-bot - INFO - /content/achatbot/src/processors/avatar/lam_audio2expression_avatar_processor.py:150 - _audio2expression_loop - audio2expression input audio durtaion 1.0\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "msg.type=<WSMsgType.BINARY: 2>\n",
            "2025-07-01 16:16:35,032 - chat-bot - INFO - /content/achatbot/src/processors/avatar/lam_audio2expression_avatar_processor.py:75 - _stop - stop avatar processor, totol session time 7.002\n",
            "2025-07-01 16:16:35,032 - chat-bot - WARNING - /content/achatbot/src/processors/avatar/lam_audio2expression_avatar_processor.py:188 - _audio2expression_loop - audio2expression_loop task cancelled\n",
            "2025-07-01 16:16:35,032 - chat-bot - INFO - /content/achatbot/src/processors/avatar/lam_audio2expression_avatar_processor.py:202 - _audio2expression_loop - audio2expression loop stopped\n",
            "2025-07-01 16:16:35,032 - chat-bot - INFO - /content/achatbot/src/processors/avatar/lam_audio2expression_avatar_processor.py:83 - _stop - avatar processor stopped\n",
            "2025-07-01 16:16:35,033 - chat-bot - INFO - /content/achatbot/test/integration/processors/test_lam_audio2expression_avatar_processor.py:274 - websocket_endpoint - PipelineTask#0 is DONE!!\n",
            "msg.type=<WSMsgType.CLOSE: 8>\n",
            "2025-07-01 16:16:35,035 - chat-bot - INFO - /content/achatbot/test/integration/processors/test_lam_audio2expression_avatar_processor.py:309 - connect2receive - Connection closed by server.\n",
            "2025-07-01 16:16:35,035 - chat-bot - INFO - /content/achatbot/test/integration/processors/test_lam_audio2expression_avatar_processor.py:323 - connect2receive - ---<aiohttp.client_ws.ClientWebSocketResponse object at 0x7ea07262c8d0> Connection closed---\n",
            "2025-07-01 16:16:35,037 - chat-bot - WARNING - /content/achatbot/test/integration/processors/test_lam_audio2expression_avatar_processor.py:291 - run_server - Server task cancelled\n",
            ".\u001b[31mERROR\u001b[0m:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/protocols/websockets/websockets_impl.py\", line 244, in run_asgi\n",
            "    result = await self.app(self.scope, self.asgi_receive, self.asgi_send)  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/middleware/proxy_headers.py\", line 70, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/applications.py\", line 1054, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 152, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 51, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 715, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 735, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 362, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 95, in app\n",
            "    await wrap_app_handling_exceptions(app, session)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 51, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 93, in app\n",
            "    await func(session)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 383, in app\n",
            "    await dependant.call(**solved_result.values)\n",
            "  File \"/content/achatbot/test/integration/processors/test_lam_audio2expression_avatar_processor.py\", line 275, in websocket_endpoint\n",
            "    await self.ws_map[key].close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/websockets.py\", line 180, in close\n",
            "    await self.send({\"type\": \"websocket.close\", \"code\": code, \"reason\": reason or \"\"})\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/websockets.py\", line 85, in send\n",
            "    await self._send(message)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 48, in sender\n",
            "    await send(message)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/protocols/websockets/websockets_impl.py\", line 338, in asgi_send\n",
            "    await self.close(code, reason)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/websockets/legacy/protocol.py\", line 778, in close\n",
            "    await asyncio.shield(self.close_connection_task)\n",
            "asyncio.exceptions.CancelledError\n",
            "\u001b[32mINFO\u001b[0m:     connection closed\n",
            "\u001b[31mERROR\u001b[0m:    Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 700, in lifespan\n",
            "    await receive()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/lifespan/on.py\", line 137, in receive\n",
            "    return await self.receive_queue.get()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/queues.py\", line 158, in get\n",
            "    await getter\n",
            "asyncio.exceptions.CancelledError\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 16.465s\n",
            "\n",
            "OK\n"
          ]
        }
      ]
    }
  ]
}