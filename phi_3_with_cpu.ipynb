{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOXKT9VQPZQxCF+kvXPA/Ha",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weedge/doraemon-nb/blob/main/phi_3_with_cpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M3zLj7kN3f8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc3334a-e3e5-4561-adbd-cea68de3444b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:            x86_64\n",
            "  CPU op-mode(s):        32-bit, 64-bit\n",
            "  Address sizes:         46 bits physical, 48 bits virtual\n",
            "  Byte Order:            Little Endian\n",
            "CPU(s):                  2\n",
            "  On-line CPU(s) list:   0,1\n",
            "Vendor ID:               GenuineIntel\n",
            "  Model name:            Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "    CPU family:          6\n",
            "    Model:               79\n",
            "    Thread(s) per core:  2\n",
            "    Core(s) per socket:  1\n",
            "    Socket(s):           1\n",
            "    Stepping:            0\n",
            "    BogoMIPS:            4400.40\n",
            "    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clf\n",
            "                         lush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_\n",
            "                         good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fm\n",
            "                         a cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hyp\n",
            "                         ervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsb\n",
            "                         ase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsa\n",
            "                         veopt arat md_clear arch_capabilities\n",
            "Virtualization features: \n",
            "  Hypervisor vendor:     KVM\n",
            "  Virtualization type:   full\n",
            "Caches (sum of all):     \n",
            "  L1d:                   32 KiB (1 instance)\n",
            "  L1i:                   32 KiB (1 instance)\n",
            "  L2:                    256 KiB (1 instance)\n",
            "  L3:                    55 MiB (1 instance)\n",
            "NUMA:                    \n",
            "  NUMA node(s):          1\n",
            "  NUMA node0 CPU(s):     0,1\n",
            "Vulnerabilities:         \n",
            "  Gather data sampling:  Not affected\n",
            "  Itlb multihit:         Not affected\n",
            "  L1tf:                  Mitigation; PTE Inversion\n",
            "  Mds:                   Vulnerable; SMT Host state unknown\n",
            "  Meltdown:              Vulnerable\n",
            "  Mmio stale data:       Vulnerable\n",
            "  Retbleed:              Vulnerable\n",
            "  Spec rstack overflow:  Not affected\n",
            "  Spec store bypass:     Vulnerable\n",
            "  Spectre v1:            Vulnerable: __user pointer sanitization and usercopy barriers only; no swap\n",
            "                         gs barriers\n",
            "  Spectre v2:            Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Not affected\n",
            "  Srbds:                 Not affected\n",
            "  Tsx async abort:       Vulnerable\n"
          ]
        }
      ],
      "source": [
        "!lscpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!free -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgmMqbOhgm1Z",
        "outputId": "74f5f3be-bbcc-4eda-d706-8ecbb7870037"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               total        used        free      shared  buff/cache   available\n",
            "Mem:            12Gi       726Mi       8.0Gi       3.0Mi       4.0Gi        11Gi\n",
            "Swap:             0B          0B          0B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AhsPcfBRPghL"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "hf_token=userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl5vO_22PMOa",
        "outputId": "75308c14-4cdc-4344-dcf4-76f51e92c34c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login --token $hf_token --add-to-git-credential"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download microsoft/Phi-3-mini-4k-instruct-gguf Phi-3-mini-4k-instruct-q4.gguf --local-dir . --local-dir-use-symlinks False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PVgePRZ5CP1",
        "outputId": "48a8e36f-cb05-4032-933f-3c07b5e41655"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf to /root/.cache/huggingface/hub/tmpc4tboz56\n",
            "Phi-3-mini-4k-instruct-q4.gguf: 100% 2.32G/2.32G [00:20<00:00, 111MB/s]\n",
            "./Phi-3-mini-4k-instruct-q4.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download microsoft/Phi-3-mini-4k-instruct-gguf Modelfile_q4 --local-dir ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivsc-wav8lit",
        "outputId": "b232aa89-7ac2-4b7e-f744-d02403431f22"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Modelfile_q4 to /root/.cache/huggingface/hub/tmpr1ba2xzn\n",
            "Modelfile_q4: 100% 141/141 [00:00<00:00, 507kB/s]\n",
            "./Modelfile_q4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# llama.cpp"
      ],
      "metadata": {
        "id": "VUguPH2ie6k_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. https://github.com/ggerganov/llama.cpp\n",
        "2. https://github.com/ggerganov/llama.cpp/pull/5631\n"
      ],
      "metadata": {
        "id": "6DbVIOZUqWCe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPlA4C6wqR3x",
        "outputId": "6dad1bd9-10b2-427e-afb3-74217ba2034d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 22816, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 22816 (delta 22), reused 35 (delta 15), pack-reused 22766\u001b[K\n",
            "Receiving objects: 100% (22816/22816), 26.12 MiB | 19.04 MiB/s, done.\n",
            "Resolving deltas: 100% (16119/16119), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && make clean && make -j $(nproc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9Wve0EXnA2A",
        "outputId": "c393db2a-6bfb-4a56-fa0b-14b4e1e3e9f1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I ccache not found. Consider installing it for faster compilation.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE \n",
            "I NVCCFLAGS: -std=c++11 -O3 \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "rm -vrf *.o tests/*.o *.so *.a *.dll benchmark-matmult lookup-create lookup-merge lookup-stats common/build-info.cpp *.dot *.gcno tests/*.gcno *.gcda tests/*.gcda *.gcov tests/*.gcov lcov-report gcovr-report main quantize quantize-stats perplexity imatrix embedding vdot q8dot train-text-from-scratch convert-llama2c-to-ggml simple batched batched-bench save-load-state server gguf gguf-split eval-callback llama-bench libllava.a llava-cli baby-llama beam-search retrieval speculative infill tokenize benchmark-matmult parallel finetune export-lora lookahead lookup passkey gritlm tests/test-c.o tests/test-llama-grammar tests/test-grammar-parser tests/test-double-float tests/test-grad0 tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-sampling tests/test-tokenizer-0-llama tests/test-tokenizer-0-falcon tests/test-tokenizer-1-llama tests/test-tokenizer-1-bpe tests/test-rope tests/test-backend-ops tests/test-model-load-cancel tests/test-autorelease tests/test-json-schema-to-grammar tests/test-grammar-integration\n",
            "rm -vrf ggml-cuda/*.o\n",
            "find examples pocs -type f -name \"*.o\" -delete\n",
            "I ccache not found. Consider installing it for faster compilation.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE \n",
            "I NVCCFLAGS: -std=c++11 -O3 \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c llama.cpp -o llama.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/common.cpp -o common.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/sampling.cpp -o sampling.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/grammar-parser.cpp -o grammar-parser.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/json-schema-to-grammar.cpp -o json-schema-to-grammar.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/console.cpp -o console.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c sgemm.cpp -o sgemm.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c unicode.cpp -o unicode.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c unicode-data.cpp -o unicode-data.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/train.cpp -o train.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/ngram-cache.cpp -o ngram-cache.o\n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/build-info.cpp -o build-info.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/main/main.cpp -o examples/main/main.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/quantize/quantize.cpp -o examples/quantize/quantize.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  build-info.o ggml.o llama.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/quantize/quantize.o -o quantize  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/quantize-stats/quantize-stats.cpp -o examples/quantize-stats/quantize-stats.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o console.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/main/main.o -o main  \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/perplexity/perplexity.cpp -o examples/perplexity/perplexity.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  build-info.o ggml.o llama.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/quantize-stats/quantize-stats.o -o quantize-stats  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/imatrix/imatrix.cpp -o examples/imatrix/imatrix.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/perplexity/perplexity.o -o perplexity  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/embedding/embedding.cpp -o examples/embedding/embedding.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/imatrix/imatrix.o -o imatrix  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c pocs/vdot/vdot.cpp -o pocs/vdot/vdot.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/embedding/embedding.o -o embedding  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c pocs/vdot/q8dot.cpp -o pocs/vdot/q8dot.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o pocs/vdot/vdot.o -o vdot  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/train-text-from-scratch/train-text-from-scratch.cpp -o examples/train-text-from-scratch/train-text-from-scratch.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o pocs/vdot/q8dot.o -o q8dot  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp -o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o train.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/train-text-from-scratch/train-text-from-scratch.o -o train-text-from-scratch  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/simple/simple.cpp -o examples/simple/simple.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o -o convert-llama2c-to-ggml  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/batched/batched.cpp -o examples/batched/batched.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/simple/simple.o -o simple  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/batched-bench/batched-bench.cpp -o examples/batched-bench/batched-bench.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/batched/batched.o -o batched  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/save-load-state/save-load-state.cpp -o examples/save-load-state/save-load-state.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  build-info.o ggml.o llama.o common.o sampling.o grammar-parser.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/batched-bench/batched-bench.o -o batched-bench  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/server/server.cpp -o examples/server/server.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/save-load-state/save-load-state.o -o save-load-state  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/gguf/gguf.cpp -o examples/gguf/gguf.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/gguf/gguf.o -o gguf  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/gguf-split/gguf-split.cpp -o examples/gguf-split/gguf-split.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/gguf-split/gguf-split.o -o gguf-split  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/eval-callback/eval-callback.cpp -o examples/eval-callback/eval-callback.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/eval-callback/eval-callback.o -o eval-callback  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/llama-bench/llama-bench.cpp -o examples/llama-bench/llama-bench.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/llama-bench/llama-bench.o -o llama-bench  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/llava/llava-cli.cpp -o examples/llava/llava-cli.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/llava/clip.cpp  -o examples/llava/clip.o -Wno-cast-qual\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/llava/llava.cpp -o examples/llava/llava.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/llava/llava-cli.o examples/llava/clip.o examples/llava/llava.o -o llava-cli  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/baby-llama/baby-llama.cpp -o examples/baby-llama/baby-llama.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o train.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/baby-llama/baby-llama.o -o baby-llama  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/beam-search/beam-search.cpp -o examples/beam-search/beam-search.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/beam-search/beam-search.o -o beam-search  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/retrieval/retrieval.cpp -o examples/retrieval/retrieval.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/retrieval/retrieval.o -o retrieval  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/speculative/speculative.cpp -o examples/speculative/speculative.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o -Iexamples/server examples/server/server.o -o server   \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/infill/infill.cpp -o examples/infill/infill.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/speculative/speculative.o -o speculative  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/tokenize/tokenize.cpp -o examples/tokenize/tokenize.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o console.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/infill/infill.o -o infill  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/benchmark/benchmark-matmult.cpp -o examples/benchmark/benchmark-matmult.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/tokenize/tokenize.o -o tokenize  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/parallel/parallel.cpp -o examples/parallel/parallel.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  build-info.o ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/benchmark/benchmark-matmult.o -o benchmark-matmult  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/finetune/finetune.cpp -o examples/finetune/finetune.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/parallel/parallel.o -o parallel  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/export-lora/export-lora.cpp -o examples/export-lora/export-lora.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o train.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/finetune/finetune.o -o finetune  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/export-lora/export-lora.o -o export-lora  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/lookahead/lookahead.cpp -o examples/lookahead/lookahead.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup.cpp -o examples/lookup/lookup.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup.o -o lookup  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup-create.cpp -o examples/lookup/lookup-create.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookahead/lookahead.o -o lookahead  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/passkey/passkey.cpp -o examples/passkey/passkey.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup-create.o -o lookup-create  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup-merge.cpp -o examples/lookup/lookup-merge.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/passkey/passkey.o -o passkey  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/gritlm/gritlm.cpp -o examples/gritlm/gritlm.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup-merge.o -o lookup-merge  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup-stats.cpp -o examples/lookup/lookup-stats.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/gritlm/gritlm.o -o gritlm  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup-stats.o -o lookup-stats  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./llama.cpp/main -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqQp6jTMoDZU",
        "outputId": "613d8d3c-eb25-4dd7-ed92-fbc95d8dc49e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "usage: ./llama.cpp/main [options]\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --version             show version and build info\n",
            "  -i, --interactive     run in interactive mode\n",
            "  --interactive-first   run in interactive mode and wait for input right away\n",
            "  -ins, --instruct      run in instruction mode (use with Alpaca models)\n",
            "  -cml, --chatml        run in chatml mode (use with ChatML-compatible models)\n",
            "  --multiline-input     allows you to write or paste multiple lines without ending each in '\\'\n",
            "  -r PROMPT, --reverse-prompt PROMPT\n",
            "                        halt generation at PROMPT, return control in interactive mode\n",
            "                        (can be specified more than once for multiple prompts).\n",
            "  --color               colorise output to distinguish prompt and user input from generations\n",
            "  -s SEED, --seed SEED  RNG seed (default: -1, use random seed for < 0)\n",
            "  -t N, --threads N     number of threads to use during generation (default: 2)\n",
            "  -tb N, --threads-batch N\n",
            "                        number of threads to use during batch and prompt processing (default: same as --threads)\n",
            "  -td N, --threads-draft N                        number of threads to use during generation (default: same as --threads)\n",
            "  -tbd N, --threads-batch-draft N\n",
            "                        number of threads to use during batch and prompt processing (default: same as --threads-draft)\n",
            "  -p PROMPT, --prompt PROMPT\n",
            "                        prompt to start generation with (default: empty)\n",
            "  -e, --escape          process prompt escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\)\n",
            "  --prompt-cache FNAME  file to cache prompt state for faster startup (default: none)\n",
            "  --prompt-cache-all    if specified, saves user input and generations to cache as well.\n",
            "                        not supported with --interactive or other interactive options\n",
            "  --prompt-cache-ro     if specified, uses the prompt cache but does not update it.\n",
            "  --random-prompt       start with a randomized prompt.\n",
            "  --in-prefix-bos       prefix BOS to user inputs, preceding the `--in-prefix` string\n",
            "  --in-prefix STRING    string to prefix user inputs with (default: empty)\n",
            "  --in-suffix STRING    string to suffix after user inputs with (default: empty)\n",
            "  -f FNAME, --file FNAME\n",
            "                        prompt file to start generation.\n",
            "  -bf FNAME, --binary-file FNAME\n",
            "                        binary file containing multiple choice tasks.\n",
            "  -n N, --n-predict N   number of tokens to predict (default: -1, -1 = infinity, -2 = until context filled)\n",
            "  -c N, --ctx-size N    size of the prompt context (default: 512, 0 = loaded from model)\n",
            "  -b N, --batch-size N  logical maximum batch size (default: 2048)\n",
            "  -ub N, --ubatch-size N\n",
            "                        physical maximum batch size (default: 512)\n",
            "  --samplers            samplers that will be used for generation in the order, separated by ';'\n",
            "                        (default: top_k;tfs_z;typical_p;top_p;min_p;temperature)\n",
            "  --sampling-seq        simplified sequence for samplers that will be used (default: kfypmt)\n",
            "  --top-k N             top-k sampling (default: 40, 0 = disabled)\n",
            "  --top-p N             top-p sampling (default: 0.9, 1.0 = disabled)\n",
            "  --min-p N             min-p sampling (default: 0.1, 0.0 = disabled)\n",
            "  --tfs N               tail free sampling, parameter z (default: 1.0, 1.0 = disabled)\n",
            "  --typical N           locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n",
            "  --repeat-last-n N     last n tokens to consider for penalize (default: 64, 0 = disabled, -1 = ctx_size)\n",
            "  --repeat-penalty N    penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)\n",
            "  --presence-penalty N  repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n",
            "  --frequency-penalty N repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n",
            "  --dynatemp-range N    dynamic temperature range (default: 0.0, 0.0 = disabled)\n",
            "  --dynatemp-exp N      dynamic temperature exponent (default: 1.0)\n",
            "  --mirostat N          use Mirostat sampling.\n",
            "                        Top K, Nucleus, Tail Free and Locally Typical samplers are ignored if used.\n",
            "                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
            "  --mirostat-lr N       Mirostat learning rate, parameter eta (default: 0.1)\n",
            "  --mirostat-ent N      Mirostat target entropy, parameter tau (default: 5.0)\n",
            "  -l TOKEN_ID(+/-)BIAS, --logit-bias TOKEN_ID(+/-)BIAS\n",
            "                        modifies the likelihood of token appearing in the completion,\n",
            "                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n",
            "                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n",
            "  --grammar GRAMMAR     BNF-like grammar to constrain generations (see samples in grammars/ dir)\n",
            "  --grammar-file FNAME  file to read grammar from\n",
            "  -j SCHEMA, --json-schema SCHEMA\n",
            "                        JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object.\n",
            "                        For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead\n",
            "  --cfg-negative-prompt PROMPT\n",
            "                        negative prompt to use for guidance. (default: empty)\n",
            "  --cfg-negative-prompt-file FNAME\n",
            "                        negative prompt file to use for guidance. (default: empty)\n",
            "  --cfg-scale N         strength of guidance (default: 1.000000, 1.0 = disable)\n",
            "  --rope-scaling {none,linear,yarn}\n",
            "                        RoPE frequency scaling method, defaults to linear unless specified by the model\n",
            "  --rope-scale N        RoPE context scaling factor, expands context by a factor of N\n",
            "  --rope-freq-base N    RoPE base frequency, used by NTK-aware scaling (default: loaded from model)\n",
            "  --rope-freq-scale N   RoPE frequency scaling factor, expands context by a factor of 1/N\n",
            "  --yarn-orig-ctx N     YaRN: original context size of model (default: 0 = model training context size)\n",
            "  --yarn-ext-factor N   YaRN: extrapolation mix factor (default: 1.0, 0.0 = full interpolation)\n",
            "  --yarn-attn-factor N  YaRN: scale sqrt(t) or attention magnitude (default: 1.0)\n",
            "  --yarn-beta-slow N    YaRN: high correction dim or alpha (default: 1.0)\n",
            "  --yarn-beta-fast N    YaRN: low correction dim or beta (default: 32.0)\n",
            "  --pooling {none,mean,cls}\n",
            "                        pooling type for embeddings, use model default if unspecified\n",
            "  -dt N, --defrag-thold N\n",
            "                        KV cache defragmentation threshold (default: -1.0, < 0 - disabled)\n",
            "  --ignore-eos          ignore end of stream token and continue generating (implies --logit-bias 2-inf)\n",
            "  --penalize-nl         penalize newline tokens\n",
            "  --temp N              temperature (default: 0.8)\n",
            "  --all-logits          return logits for all tokens in the batch (default: disabled)\n",
            "  --hellaswag           compute HellaSwag score over random tasks from datafile supplied with -f\n",
            "  --hellaswag-tasks N   number of tasks to use when computing the HellaSwag score (default: 400)\n",
            "  --winogrande          compute Winogrande score over random tasks from datafile supplied with -f\n",
            "  --winogrande-tasks N  number of tasks to use when computing the Winogrande score (default: 0)\n",
            "  --multiple-choice     compute multiple choice score over random tasks from datafile supplied with -f\n",
            "  --multiple-choice-tasks N number of tasks to use when computing the multiple choice score (default: 0)\n",
            "  --kl-divergence       computes KL-divergence to logits provided via --kl-divergence-base\n",
            "  --keep N              number of tokens to keep from the initial prompt (default: 0, -1 = all)\n",
            "  --draft N             number of tokens to draft for speculative decoding (default: 5)\n",
            "  --chunks N            max number of chunks to process (default: -1, -1 = all)\n",
            "  -np N, --parallel N   number of parallel sequences to decode (default: 1)\n",
            "  -ns N, --sequences N  number of sequences to decode (default: 1)\n",
            "  -ps N, --p-split N    speculative decoding split probability (default: 0.1)\n",
            "  -cb, --cont-batching  enable continuous batching (a.k.a dynamic batching) (default: disabled)\n",
            "  --mmproj MMPROJ_FILE  path to a multimodal projector file for LLaVA. see examples/llava/README.md\n",
            "  --image IMAGE_FILE    path to an image file. use with multimodal models\n",
            "  --mlock               force system to keep model in RAM rather than swapping or compressing\n",
            "  --no-mmap             do not memory-map model (slower load but may reduce pageouts if not using mlock)\n",
            "  --numa TYPE           attempt optimizations that help on some NUMA systems\n",
            "                          - distribute: spread execution evenly over all nodes\n",
            "                          - isolate: only spawn threads on CPUs on the node that execution started on\n",
            "                          - numactl: use the CPU map provided by numactl\n",
            "                        if run without this previously, it is recommended to drop the system page cache before using this\n",
            "                        see https://github.com/ggerganov/llama.cpp/issues/1437\n",
            "  --verbose-prompt      print a verbose prompt before generation (default: false)\n",
            "  --no-display-prompt   don't print prompt at generation (default: false)\n",
            "  -gan N, --grp-attn-n N\n",
            "                        group-attention factor (default: 1)\n",
            "  -gaw N, --grp-attn-w N\n",
            "                        group-attention width (default: 512.0)\n",
            "  -dkvc, --dump-kv-cache\n",
            "                        verbose print of the KV cache\n",
            "  -nkvo, --no-kv-offload\n",
            "                        disable KV offload\n",
            "  -ctk TYPE, --cache-type-k TYPE\n",
            "                        KV cache data type for K (default: f16)\n",
            "  -ctv TYPE, --cache-type-v TYPE\n",
            "                        KV cache data type for V (default: f16)\n",
            "  --simple-io           use basic IO for better compatibility in subprocesses and limited consoles\n",
            "  --lora FNAME          apply LoRA adapter (implies --no-mmap)\n",
            "  --lora-scaled FNAME S apply LoRA adapter with user defined scaling S (implies --no-mmap)\n",
            "  --lora-base FNAME     optional model to use as a base for the layers modified by the LoRA adapter\n",
            "  --control-vector FNAME\n",
            "                        add a control vector\n",
            "  --control-vector-scaled FNAME S\n",
            "                        add a control vector with user defined scaling S\n",
            "  --control-vector-layer-range START END\n",
            "                        layer range to apply the control vector(s) to, start and end inclusive\n",
            "  -m FNAME, --model FNAME\n",
            "                        model path (default: models/7B/ggml-model-f16.gguf)\n",
            "  -md FNAME, --model-draft FNAME\n",
            "                        draft model for speculative decoding (default: unused)\n",
            "  -mu MODEL_URL, --model-url MODEL_URL\n",
            "                        model download url (default: unused)\n",
            "  -hfr REPO, --hf-repo REPO\n",
            "                        Hugging Face model repository (default: unused)\n",
            "  -hff FILE, --hf-file FILE\n",
            "                        Hugging Face model file (default: unused)\n",
            "  -ld LOGDIR, --logdir LOGDIR\n",
            "                        path under which to save YAML logs (no logging if unset)\n",
            "  -lcs FNAME, --lookup-cache-static FNAME\n",
            "                        path to static lookup cache to use for lookup decoding (not updated by generation)\n",
            "  -lcd FNAME, --lookup-cache-dynamic FNAME\n",
            "                        path to dynamic lookup cache to use for lookup decoding (updated by generation)\n",
            "  --override-kv KEY=TYPE:VALUE\n",
            "                        advanced option to override model metadata by key. may be specified multiple times.\n",
            "                        types: int, float, bool. example: --override-kv tokenizer.ggml.add_bos_token=bool:false\n",
            "  -ptc N, --print-token-count N\n",
            "                        print token count every N tokens (default: -1)\n",
            "\n",
            "log options:\n",
            "  --log-test            Run simple logging test\n",
            "  --log-disable         Disable trace logs\n",
            "  --log-enable          Enable trace logs\n",
            "  --log-file            Specify a log filename (without extension)\n",
            "  --log-new             Create a separate new log file on start. Each log file will have unique name: \"<name>.<ID>.log\"\n",
            "  --log-append          Don't truncate the old log file.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the model on a CPU\n",
        "!./llama.cpp/main -m Phi-3-mini-4k-instruct-q4.gguf -n 256 -p \"Write a Python function to find sum of two numbers.\" --repeat-penalty 1.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjtECO2VDRiA",
        "outputId": "d28b602b-5af3-4bb8-a7c9-59b27a128c17"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 2716 (4e96a812)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1713927771\n",
            "llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32064\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 32000\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 323/32064 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32064\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 96\n",
            "llm_load_print_meta: n_embd_head_k    = 96\n",
            "llm_load_print_meta: n_embd_head_v    = 96\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
            "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 3.82 B\n",
            "llm_load_print_meta: model size       = 2.16 GiB (4.85 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  2210.78 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   192.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  192.00 MiB, K (f16):   96.00 MiB, V (f16):   96.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    68.63 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 2048, n_predict = 256, n_keep = 1\n",
            "\n",
            "\n",
            "<s> Write a Python function to find sum of two numbers.\n",
            "<|assistant|> Here is an example of a Python function that calculates the sum of two given numbers:\n",
            "\n",
            "```python\n",
            "def add_numbers(num1, num2):\n",
            "    \"\"\"\n",
            "    This function takes in two numbers as arguments and returns their sum\n",
            "    \n",
            "    Parameters:\n",
            "    - num1 (int or float): The first number\n",
            "    - num2 (int or float): The second number\n",
            "\n",
            "    Returns:\n",
            "    - int or float: The sum of the two given numbers\n",
            "    \"\"\"\n",
            "    return num1 + num2\n",
            "\n",
            "# Example usage\n",
            "num1 = 5\n",
            "num2 = 3\n",
            "sum_of_numbers = add_numbers(num1, num2)\n",
            "print(f\"The sum of {num1} and {num2} is: {sum_of_numbers}\")\n",
            "```\n",
            "\n",
            "This function, `add_numbers`, takes two parameters (num1 and num2), which can be integers or floats. It returns the sum of these two numbers by simply adding them together using the `+` operator. The example usage demonstrates how to call this function with values 5 and 3 as arguments, and print their sum.<|end|><|endoftext|> [end of text]\n",
            "\n",
            "llama_print_timings:        load time =    6035.36 ms\n",
            "llama_print_timings:      sample time =     158.87 ms /   253 runs   (    0.63 ms per token,  1592.49 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8020.97 ms /    12 tokens (  668.41 ms per token,     1.50 tokens per second)\n",
            "llama_print_timings:        eval time =  131277.00 ms /   252 runs   (  520.94 ms per token,     1.92 tokens per second)\n",
            "llama_print_timings:       total time =  139570.25 ms /   264 tokens\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the model on a CPU\n",
        "!./llama.cpp/main -m Phi-3-mini-4k-instruct-q4.gguf -n 256 -p \"How to explain Internet to a medieval knight?\" --repeat-penalty 1.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9luyuupGShH",
        "outputId": "88af70fc-148f-47aa-eb65-0f7ec881203c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 2716 (4e96a812)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1713928652\n",
            "llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32064\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 32000\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 323/32064 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32064\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 96\n",
            "llm_load_print_meta: n_embd_head_k    = 96\n",
            "llm_load_print_meta: n_embd_head_v    = 96\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
            "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 3.82 B\n",
            "llm_load_print_meta: model size       = 2.16 GiB (4.85 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  2210.78 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   192.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  192.00 MiB, K (f16):   96.00 MiB, V (f16):   96.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    68.63 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 2048, n_predict = 256, n_keep = 1\n",
            "\n",
            "\n",
            "<s> How to explain Internet to a medieval knight?\n",
            "<|assistant|> Explaining the concept of the 'Internet' to a medieval knight would require an imaginative approach, considering their vastly different world. Here is one way you might do it:\n",
            "\n",
            "\n",
            "\"Sir Knight, imagine if ye could send messages across great distances instantly, as though by some magic or divine intervention. The Internet is like that, but with our own kind of enchantment - a web of invisible connections between distant lands. These 'webs' allow us to share knowledge, pictures, and even voices from one person to another without ever needing to send any messengers on foot.\n",
            "\n",
            "\n",
            "Just as your messages are delivered by horses or pigeons over vast distances, these Internet messages travel through what we call 'electronic signals', moving at incredible speeds that would make a falcon seem like it's taking its time. This invisible web is accessible to those with the right tools and skills - much as your armor allows you access to protection in battle but requires knowledge on how to wear it correctly.\n",
            "\n",
            "\n",
            "Though we cannot see these connections, they are there, connecting minds across our kingdoms and beyond, making shared understanding possible even when people stand far apart.\"\n",
            "\n",
            "\n",
            "\n",
            "llama_print_timings:        load time =    5665.00 ms\n",
            "llama_print_timings:      sample time =     172.26 ms /   256 runs   (    0.67 ms per token,  1486.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4188.24 ms /    11 tokens (  380.75 ms per token,     2.63 tokens per second)\n",
            "llama_print_timings:        eval time =  118914.51 ms /   255 runs   (  466.33 ms per token,     2.14 tokens per second)\n",
            "llama_print_timings:       total time =  123392.93 ms /   266 tokens\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the model on a CPU\n",
        "!./llama.cpp/main -m Phi-3-mini-4k-instruct-q4.gguf -n 256 -p \"How to explain Internet for a medieval knight?\" --repeat-penalty 1.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVkkycxOHj64",
        "outputId": "4c04a240-482f-4d5a-9abe-7769a8fd0f48"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 2716 (4e96a812)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1713929625\n",
            "llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32064\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 32000\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 323/32064 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32064\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 96\n",
            "llm_load_print_meta: n_embd_head_k    = 96\n",
            "llm_load_print_meta: n_embd_head_v    = 96\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
            "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 3.82 B\n",
            "llm_load_print_meta: model size       = 2.16 GiB (4.85 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  2210.78 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   192.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  192.00 MiB, K (f16):   96.00 MiB, V (f16):   96.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    68.63 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 2048, n_predict = 256, n_keep = 1\n",
            "\n",
            "\n",
            "<s> How to explain Internet for a medieval knight?\n",
            "<|assistant|> Dear Sir Knight,\n",
            "\n",
            "Imagine the world as one vast kingdom with countless territories. The internet is like an enormous network of messengers traveling between these realms at incredible speed. It's a magical system connecting all lands through invisible threads called 'Ether'.\n",
            "\n",
            "1. Town Criers and Messengers: Picture your town crier, who shares news with everyone in the village square. Now envision thousands upon thousands of them working together across every corner of this kingdom, delivering messages faster than any horse could run. That's our internet!\n",
            "\n",
            "2. Scrolls & Books: In your time, information is stored on scrolls and books. The internet allows vast libraries to be held within a single device called the 'computer', holding limitless scrolls of knowledge that one can access with but a few clicks - as if turning pages in an instant!\n",
            "\n",
            "3. Royal Messengers (Couriers): Your royal messengers deliver crucial information across kingdoms, often taking weeks or even months to reach their destination. The internet delivers messages instantly anywhere around the globe, connecting people like never before.\n",
            "\n",
            "4. Maps and Navigation: Medieval explorers use maps\n",
            "llama_print_timings:        load time =     827.84 ms\n",
            "llama_print_timings:      sample time =     167.18 ms /   256 runs   (    0.65 ms per token,  1531.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2606.62 ms /    11 tokens (  236.97 ms per token,     4.22 tokens per second)\n",
            "llama_print_timings:        eval time =  100335.87 ms /   255 runs   (  393.47 ms per token,     2.54 tokens per second)\n",
            "llama_print_timings:       total time =  103221.16 ms /   266 tokens\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## use llama-cpp-python bind"
      ],
      "metadata": {
        "id": "lAhqFgWnFNOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKE6_DHkEy5f",
        "outputId": "814c39e3-1a03-483f-e3d1-97030da07eff"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.64.tar.gz (37.4 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m37.4/37.4 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.11.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.64-cp310-cp310-linux_x86_64.whl size=3315607 sha256=7fdf6eefdff97a8ef0b2980d3fd78bf13a0afdabe25857b1a026453e2a8fb39a\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/69/6b/2fb423c0e39b66cc7f9b84ea837dbdc1eee8ea5f624e3eaeb6\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "\n",
        "llm = Llama(\n",
        "  model_path=\"./Phi-3-mini-4k-instruct-q4.gguf\",  # path to GGUF file\n",
        "  n_ctx=4096,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
        "  #n_threads=2, # The number of CPU threads to use, tailor to your system and the resulting performance\n",
        "  n_gpu_layers=0, # The number of layers to offload to GPU, if you have GPU acceleration available. Set to 0 if no GPU acceleration is available on your system.\n",
        ")\n",
        "\n",
        "prompt = \"How to explain Internet to a medieval knight?\"\n",
        "\n",
        "# Simple inference example\n",
        "output = llm(\n",
        "  f\"<|user|>\\n{prompt}<|end|>\\n<|assistant|>\",\n",
        "  max_tokens=256,  # Generate up to 256 tokens\n",
        "  stop=[\"<|end|>\"],\n",
        "  echo=True,  # Whether to echo the prompt\n",
        ")\n",
        "\n",
        "print(output['choices'][0]['text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_0BBBw4E2jt",
        "outputId": "80f8bd34-63c5-4133-96c4-dcffe433572b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from ./Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32064\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 32000\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 323/32064 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32064\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 96\n",
            "llm_load_print_meta: n_embd_head_k    = 96\n",
            "llm_load_print_meta: n_embd_head_v    = 96\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
            "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 3.82 B\n",
            "llm_load_print_meta: model size       = 2.16 GiB (4.85 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  2210.78 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   288.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{{'<|' + message['role'] + '|>' + '\\n' + message['content'] + '<|end|>\\n' }}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% else %}{{ eos_token }}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.vocab_size': '32064', 'general.file_type': '15', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '3072', 'llama.feed_forward_length': '8192', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '96', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32'}\n",
            "Using gguf chat template: {{ bos_token }}{% for message in messages %}{{'<|' + message['role'] + '|>' + '\n",
            "' + message['content'] + '<|end|>\n",
            "' }}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
            "' }}{% else %}{{ eos_token }}{% endif %}\n",
            "Using chat eos_token: <|endoftext|>\n",
            "Using chat bos_token: <s>\n",
            "\n",
            "llama_print_timings:        load time =    8622.06 ms\n",
            "llama_print_timings:      sample time =     195.22 ms /   256 runs   (    0.76 ms per token,  1311.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8621.97 ms /    16 tokens (  538.87 ms per token,     1.86 tokens per second)\n",
            "llama_print_timings:        eval time =  100738.91 ms /   255 runs   (  395.05 ms per token,     2.53 tokens per second)\n",
            "llama_print_timings:       total time =  110712.16 ms /   271 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|user|>\n",
            "How to explain Internet to a medieval knight?<|end|>\n",
            "<|assistant|> My noble knight, imagine the vast kingdom of knowledge and communication you are accustomed to in your time. The Internet is like an invisible web that connects all corners of our world together at once, allowing for swift exchange of information and messages without any physical presence or travel. \n",
            "\n",
            "Just as messengers carry important news from castle to castle, so too does the Internet transmit data between devices - computers, tablets, smartphones  which are like tiny kingdoms in themselves. These devices can share tales (text), images, and even moving pictures instantly across great distances.\n",
            "\n",
            "However, unlike your trusted messengers who carry scrolls or parchment, this web of information travels through the air itself! It's not something you can see with your eyes but think of it as an unseen force that binds distant lands together in a dance of data. \n",
            "\n",
            "But remember, knight, even though powerful and seemingly magical, the Internet should be wielded wisely. As you would ensure to guard against spies or deceivers within your kingdom, so too must we protect our information from those who might misuse it for harm.\n",
            "\n",
            "In essence, consider the internet as a grand library accessible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ollama"
      ],
      "metadata": {
        "id": "HEmZBUUr8NsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jowviRGY8PjT",
        "outputId": "df5a1cf5-cba0-4cde-9c04-2527da2cbc88"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Downloading ollama...\n",
            "############################################################################################# 100.0%\n",
            ">>> Installing ollama to /usr/local/bin...\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI4viN8R9THE",
        "outputId": "4cdc73bf-2069-485b-f42c-55a14740a858"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage:\n",
            "  ollama [flags]\n",
            "  ollama [command]\n",
            "\n",
            "Available Commands:\n",
            "  serve       Start ollama\n",
            "  create      Create a model from a Modelfile\n",
            "  show        Show information for a model\n",
            "  run         Run a model\n",
            "  pull        Pull a model from a registry\n",
            "  push        Push a model to a registry\n",
            "  list        List models\n",
            "  cp          Copy a model\n",
            "  rm          Remove a model\n",
            "  help        Help about any command\n",
            "\n",
            "Flags:\n",
            "  -h, --help      help for ollama\n",
            "  -v, --version   Show version information\n",
            "\n",
            "Use \"ollama [command] --help\" for more information about a command.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve >> ollama_serve.log 2>&1 &"
      ],
      "metadata": {
        "id": "c-XxW2Rg9l2Z"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps -ef | grep ollama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdfKRBoa-tO7",
        "outputId": "bea8081c-cc1f-4ade-f653-f321214efab4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        9231       1 88 02:50 ?        00:00:11 ollama serve\n",
            "root        9293     161  0 02:50 ?        00:00:00 /bin/bash -c ps -ef | grep ollama\n",
            "root        9295    9293  0 02:50 ?        00:00:00 grep ollama\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama create phi3 -f Modelfile_q4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zd9orDGm-1SQ",
        "outputId": "28b550bf-54c1-4de6-a1e4-716d216814de"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25ltransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer \n",
            "creating template layer \n",
            "creating parameters layer \n",
            "creating config layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer \n",
            "creating template layer \n",
            "creating parameters layer \n",
            "creating config layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer \n",
            "creating template layer \n",
            "creating parameters layer \n",
            "creating config layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer \n",
            "creating template layer \n",
            "creating parameters layer \n",
            "creating config layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer \n",
            "creating template layer \n",
            "creating parameters layer \n",
            "creating config layer  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
            "creating model layer \n",
            "creating template layer \n",
            "creating parameters layer \n",
            "creating config layer \n",
            "using already created layer sha256:4fed7364ee3e0c7cb4fe0880148bfdfcd1b630981efa0802a6b62ee52e7da97e \n",
            "writing layer sha256:01b3ffca693d57e3115098224af9cb05f3f882fa39da905a2be11a7effa75969 \n",
            "writing layer sha256:3cbbc2eb6fa45ce5ae5649b03a5f5316314dccb10573d719a0673798286f3995 \n",
            "writing layer sha256:b95aba3b5a67b0b5ecb580273e4518ba6f40ecc640daa7e93e74e157335a83ed \n",
            "writing manifest \n",
            "success \u001b[?25h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tail -n 30 ollama_serve.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uj-5TPo1-w6a",
        "outputId": "4d08b14b-1771-496c-b93c-61cc958d6f56"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time=2024-04-24T02:50:12.920Z level=INFO source=images.go:817 msg=\"total blobs: 5\"\n",
            "time=2024-04-24T02:50:12.920Z level=INFO source=images.go:824 msg=\"total unused blobs removed: 0\"\n",
            "time=2024-04-24T02:50:12.921Z level=INFO source=routes.go:1143 msg=\"Listening on 127.0.0.1:11434 (version 0.1.32)\"\n",
            "time=2024-04-24T02:50:12.921Z level=INFO source=payload.go:28 msg=\"extracting embedded files\" dir=/tmp/ollama4121674356/runners\n",
            "time=2024-04-24T02:50:21.072Z level=INFO source=payload.go:41 msg=\"Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60002]\"\n",
            "time=2024-04-24T02:50:21.072Z level=INFO source=gpu.go:121 msg=\"Detecting GPU type\"\n",
            "time=2024-04-24T02:50:21.072Z level=INFO source=gpu.go:268 msg=\"Searching for GPU management library libcudart.so*\"\n",
            "time=2024-04-24T02:50:21.080Z level=INFO source=gpu.go:314 msg=\"Discovered GPU libraries: [/tmp/ollama4121674356/runners/cuda_v11/libcudart.so.11.0 /usr/local/cuda/lib64/libcudart.so.12.2.140]\"\n",
            "time=2024-04-24T02:50:21.081Z level=INFO source=gpu.go:343 msg=\"Unable to load cudart CUDA management library /tmp/ollama4121674356/runners/cuda_v11/libcudart.so.11.0: your nvidia driver is too old or missing, please upgrade to run ollama\"\n",
            "time=2024-04-24T02:50:21.081Z level=INFO source=gpu.go:343 msg=\"Unable to load cudart CUDA management library /usr/local/cuda/lib64/libcudart.so.12.2.140: your nvidia driver is too old or missing, please upgrade to run ollama\"\n",
            "time=2024-04-24T02:50:21.081Z level=INFO source=gpu.go:268 msg=\"Searching for GPU management library libnvidia-ml.so\"\n",
            "time=2024-04-24T02:50:21.088Z level=INFO source=gpu.go:314 msg=\"Discovered GPU libraries: []\"\n",
            "time=2024-04-24T02:50:21.088Z level=INFO source=cpu_common.go:11 msg=\"CPU has AVX2\"\n",
            "time=2024-04-24T02:50:21.088Z level=INFO source=routes.go:1164 msg=\"no GPU detected\"\n",
            "[GIN] 2024/04/24 - 02:50:42 | 200 |       69.94s |       127.0.0.1 | HEAD     \"/\"\n",
            "[GIN] 2024/04/24 - 02:50:58 | 200 |      65.397s |       127.0.0.1 | POST     \"/api/blobs/sha256:4fed7364ee3e0c7cb4fe0880148bfdfcd1b630981efa0802a6b62ee52e7da97e\"\n",
            "[GIN] 2024/04/24 - 02:51:20 | 200 |  22.02702344s |       127.0.0.1 | POST     \"/api/create\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama run phi3 \"What is the capital of France?\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdJ7fSVsBLtI",
        "outputId": "0e6547a8-3209-42c6-c653-93b389454bd7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G \u001b[?25h\u001b[?25l\u001b[?25l\u001b[2K\u001b[1G\u001b[?25h\u001b[2K\u001b[1G\u001b[?25h The\u001b[?25l\u001b[?25h capital\u001b[?25l\u001b[?25h of\u001b[?25l\u001b[?25h France\u001b[?25l\u001b[?25h is\u001b[?25l\u001b[?25h Paris\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h It\u001b[?25l\u001b[?25h'\u001b[?25l\u001b[?25hs\u001b[?25l\u001b[?25h not\u001b[?25l\u001b[?25h only\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h largest\u001b[?25l\u001b[?25h city\u001b[?25l\u001b[?25h in\u001b[?25l\u001b[?25h France\u001b[?25l\u001b[?25h but\u001b[?25l\u001b[?25h also\u001b[?25l\u001b[?25h a\u001b[?25l\u001b[?25h major\u001b[?25l\u001b[?25h European\u001b[?25l\u001b[?25h center\u001b[?25l\u001b[?25h for\u001b[?25l\u001b[?25h art\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h fashion\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h g\u001b[?25l\u001b[?25hastr\u001b[?25l\u001b[?25honomy\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h culture\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h Its\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h9\u001b[?25l\u001b[?25hth\u001b[?25l\u001b[?25h-\u001b[?25l\u001b[?25hcentury\u001b[?25l\u001b[?25h metropol\u001b[?25l\u001b[?25hitan\u001b[?25l\u001b[?25h area\u001b[?25l\u001b[?25h is\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h largest\u001b[?25l\u001b[?25h economy\u001b[?25l\u001b[?25h in\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h Euro\u001b[?25l\u001b[?25hzone\u001b[?25l\u001b[?25h outside\u001b[?25l\u001b[?25h Germany\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h Kn\u001b[?25l\u001b[?25hown\u001b[?25l\u001b[?25h as\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h \"\u001b[?25l\u001b[?25hCity\u001b[?25l\u001b[?25h of\u001b[?25l\u001b[?25h Light\u001b[?25l\u001b[?25h\"\u001b[?25l\u001b[?25h (\u001b[?25l\u001b[?25hL\u001b[?25l\u001b[?25hux\u001b[?25l\u001b[?25hemb\u001b[?25l\u001b[?25hourg\u001b[?25l\u001b[?25h),\u001b[?25l\u001b[?25h Paris\u001b[?25l\u001b[?25h is\u001b[?25l\u001b[?25h famous\u001b[?25l\u001b[?25h for\u001b[?25l\u001b[?25h its\u001b[?25l\u001b[?25h ca\u001b[?25l\u001b[?25hfe\u001b[?25l\u001b[?25h culture\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h land\u001b[?25l\u001b[?25hmarks\u001b[?25l\u001b[?25h like\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h E\u001b[?25l\u001b[?25hiff\u001b[?25l\u001b[?25hel\u001b[?25l\u001b[?25h Tower\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h Notre\u001b[?25l\u001b[?25h-\u001b[?25l\u001b[?25hD\u001b[?25l\u001b[?25hame\u001b[?25l\u001b[?25h C\u001b[?25l\u001b[?25hathedral\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h is\u001b[?25l\u001b[?25h often\u001b[?25l\u001b[?25h referred\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h as\u001b[?25l\u001b[?25h \"\u001b[?25l\u001b[?25hThe\u001b[?25l\u001b[?25h City\u001b[?25l\u001b[?25h of\u001b[?25l\u001b[?25h Love\u001b[?25l\u001b[?25h.\"\u001b[?25l\u001b[?25h\n",
            "\n",
            "\u001b[?25l\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tail -n 200 ollama_serve.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtEe4hpXBoQI",
        "outputId": "2c19276e-cbf1-4a9a-c3e1-193d84f324b7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time=2024-04-24T02:50:12.920Z level=INFO source=images.go:817 msg=\"total blobs: 5\"\n",
            "time=2024-04-24T02:50:12.920Z level=INFO source=images.go:824 msg=\"total unused blobs removed: 0\"\n",
            "time=2024-04-24T02:50:12.921Z level=INFO source=routes.go:1143 msg=\"Listening on 127.0.0.1:11434 (version 0.1.32)\"\n",
            "time=2024-04-24T02:50:12.921Z level=INFO source=payload.go:28 msg=\"extracting embedded files\" dir=/tmp/ollama4121674356/runners\n",
            "time=2024-04-24T02:50:21.072Z level=INFO source=payload.go:41 msg=\"Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60002]\"\n",
            "time=2024-04-24T02:50:21.072Z level=INFO source=gpu.go:121 msg=\"Detecting GPU type\"\n",
            "time=2024-04-24T02:50:21.072Z level=INFO source=gpu.go:268 msg=\"Searching for GPU management library libcudart.so*\"\n",
            "time=2024-04-24T02:50:21.080Z level=INFO source=gpu.go:314 msg=\"Discovered GPU libraries: [/tmp/ollama4121674356/runners/cuda_v11/libcudart.so.11.0 /usr/local/cuda/lib64/libcudart.so.12.2.140]\"\n",
            "time=2024-04-24T02:50:21.081Z level=INFO source=gpu.go:343 msg=\"Unable to load cudart CUDA management library /tmp/ollama4121674356/runners/cuda_v11/libcudart.so.11.0: your nvidia driver is too old or missing, please upgrade to run ollama\"\n",
            "time=2024-04-24T02:50:21.081Z level=INFO source=gpu.go:343 msg=\"Unable to load cudart CUDA management library /usr/local/cuda/lib64/libcudart.so.12.2.140: your nvidia driver is too old or missing, please upgrade to run ollama\"\n",
            "time=2024-04-24T02:50:21.081Z level=INFO source=gpu.go:268 msg=\"Searching for GPU management library libnvidia-ml.so\"\n",
            "time=2024-04-24T02:50:21.088Z level=INFO source=gpu.go:314 msg=\"Discovered GPU libraries: []\"\n",
            "time=2024-04-24T02:50:21.088Z level=INFO source=cpu_common.go:11 msg=\"CPU has AVX2\"\n",
            "time=2024-04-24T02:50:21.088Z level=INFO source=routes.go:1164 msg=\"no GPU detected\"\n",
            "[GIN] 2024/04/24 - 02:50:42 | 200 |       69.94s |       127.0.0.1 | HEAD     \"/\"\n",
            "[GIN] 2024/04/24 - 02:50:58 | 200 |      65.397s |       127.0.0.1 | POST     \"/api/blobs/sha256:4fed7364ee3e0c7cb4fe0880148bfdfcd1b630981efa0802a6b62ee52e7da97e\"\n",
            "[GIN] 2024/04/24 - 02:51:20 | 200 |  22.02702344s |       127.0.0.1 | POST     \"/api/create\"\n",
            "[GIN] 2024/04/24 - 02:52:39 | 200 |      41.915s |       127.0.0.1 | HEAD     \"/\"\n",
            "[GIN] 2024/04/24 - 02:52:39 | 200 |    4.011972ms |       127.0.0.1 | POST     \"/api/show\"\n",
            "time=2024-04-24T02:52:39.871Z level=INFO source=gpu.go:121 msg=\"Detecting GPU type\"\n",
            "time=2024-04-24T02:52:39.872Z level=INFO source=gpu.go:268 msg=\"Searching for GPU management library libcudart.so*\"\n",
            "time=2024-04-24T02:52:39.877Z level=INFO source=gpu.go:314 msg=\"Discovered GPU libraries: [/tmp/ollama4121674356/runners/cuda_v11/libcudart.so.11.0 /usr/local/cuda/lib64/libcudart.so.12.2.140]\"\n",
            "time=2024-04-24T02:52:39.888Z level=INFO source=gpu.go:343 msg=\"Unable to load cudart CUDA management library /tmp/ollama4121674356/runners/cuda_v11/libcudart.so.11.0: your nvidia driver is too old or missing, please upgrade to run ollama\"\n",
            "time=2024-04-24T02:52:39.888Z level=INFO source=gpu.go:343 msg=\"Unable to load cudart CUDA management library /usr/local/cuda/lib64/libcudart.so.12.2.140: your nvidia driver is too old or missing, please upgrade to run ollama\"\n",
            "time=2024-04-24T02:52:39.888Z level=INFO source=gpu.go:268 msg=\"Searching for GPU management library libnvidia-ml.so\"\n",
            "time=2024-04-24T02:52:39.893Z level=INFO source=gpu.go:314 msg=\"Discovered GPU libraries: []\"\n",
            "time=2024-04-24T02:52:39.893Z level=INFO source=cpu_common.go:11 msg=\"CPU has AVX2\"\n",
            "time=2024-04-24T02:52:39.893Z level=INFO source=gpu.go:121 msg=\"Detecting GPU type\"\n",
            "time=2024-04-24T02:52:39.893Z level=INFO source=gpu.go:268 msg=\"Searching for GPU management library libcudart.so*\"\n",
            "time=2024-04-24T02:52:39.898Z level=INFO source=gpu.go:314 msg=\"Discovered GPU libraries: [/tmp/ollama4121674356/runners/cuda_v11/libcudart.so.11.0 /usr/local/cuda/lib64/libcudart.so.12.2.140]\"\n",
            "time=2024-04-24T02:52:39.899Z level=INFO source=gpu.go:343 msg=\"Unable to load cudart CUDA management library /tmp/ollama4121674356/runners/cuda_v11/libcudart.so.11.0: your nvidia driver is too old or missing, please upgrade to run ollama\"\n",
            "time=2024-04-24T02:52:39.899Z level=INFO source=gpu.go:343 msg=\"Unable to load cudart CUDA management library /usr/local/cuda/lib64/libcudart.so.12.2.140: your nvidia driver is too old or missing, please upgrade to run ollama\"\n",
            "time=2024-04-24T02:52:39.899Z level=INFO source=gpu.go:268 msg=\"Searching for GPU management library libnvidia-ml.so\"\n",
            "time=2024-04-24T02:52:39.903Z level=INFO source=gpu.go:314 msg=\"Discovered GPU libraries: []\"\n",
            "time=2024-04-24T02:52:39.903Z level=INFO source=cpu_common.go:11 msg=\"CPU has AVX2\"\n",
            "time=2024-04-24T02:52:39.903Z level=INFO source=server.go:127 msg=\"offload to gpu\" reallayers=0 layers=0 required=\"3981.9 MiB\" used=\"329.1 MiB\" available=\"0 B\" kv=\"1536.0 MiB\" fulloffload=\"288.0 MiB\" partialoffload=\"329.1 MiB\"\n",
            "time=2024-04-24T02:52:39.904Z level=INFO source=server.go:264 msg=\"starting llama server\" cmd=\"/tmp/ollama4121674356/runners/cpu_avx2/ollama_llama_server --model /root/.ollama/models/blobs/sha256-4fed7364ee3e0c7cb4fe0880148bfdfcd1b630981efa0802a6b62ee52e7da97e --ctx-size 4096 --batch-size 512 --embedding --log-disable --n-gpu-layers 0 --port 36013\"\n",
            "time=2024-04-24T02:52:39.905Z level=INFO source=server.go:389 msg=\"waiting for llama runner to start responding\"\n",
            "{\"function\":\"server_params_parse\",\"level\":\"INFO\",\"line\":2603,\"msg\":\"logging to file is disabled.\",\"tid\":\"136599932254080\",\"timestamp\":1713927159}\n",
            "{\"function\":\"server_params_parse\",\"level\":\"WARN\",\"line\":2380,\"msg\":\"Not compiled with GPU offload support, --n-gpu-layers option will be ignored. See main README.md for information on enabling GPU BLAS support\",\"n_gpu_layers\":-1,\"tid\":\"136599932254080\",\"timestamp\":1713927159}\n",
            "{\"build\":1,\"commit\":\"7593639\",\"function\":\"main\",\"level\":\"INFO\",\"line\":2819,\"msg\":\"build info\",\"tid\":\"136599932254080\",\"timestamp\":1713927159}\n",
            "{\"function\":\"main\",\"level\":\"INFO\",\"line\":2822,\"msg\":\"system info\",\"n_threads\":2,\"n_threads_batch\":-1,\"system_info\":\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \",\"tid\":\"136599932254080\",\"timestamp\":1713927159,\"total_threads\":2}\n",
            "llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-4fed7364ee3e0c7cb4fe0880148bfdfcd1b630981efa0802a6b62ee52e7da97e (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32064\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 32000\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 323/32064 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32064\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 96\n",
            "llm_load_print_meta: n_embd_head_k    = 96\n",
            "llm_load_print_meta: n_embd_head_v    = 96\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
            "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 3.82 B\n",
            "llm_load_print_meta: model size       = 2.16 GiB (4.85 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
            "llm_load_tensors:        CPU buffer size =  2210.78 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.13 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   288.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "{\"function\":\"initialize\",\"level\":\"INFO\",\"line\":448,\"msg\":\"initializing slots\",\"n_slots\":1,\"tid\":\"136599932254080\",\"timestamp\":1713927161}\n",
            "{\"function\":\"initialize\",\"level\":\"INFO\",\"line\":457,\"msg\":\"new slot\",\"n_ctx_slot\":4096,\"slot_id\":0,\"tid\":\"136599932254080\",\"timestamp\":1713927161}\n",
            "{\"function\":\"main\",\"level\":\"INFO\",\"line\":3064,\"msg\":\"model loaded\",\"tid\":\"136599932254080\",\"timestamp\":1713927161}\n",
            "{\"function\":\"validate_model_chat_template\",\"level\":\"ERR\",\"line\":437,\"msg\":\"The chat template comes with this model is not yet supported, falling back to chatml. This may cause the model to output suboptimal responses\",\"tid\":\"136599932254080\",\"timestamp\":1713927161}\n",
            "{\"function\":\"main\",\"hostname\":\"127.0.0.1\",\"level\":\"INFO\",\"line\":3267,\"msg\":\"HTTP server listening\",\"n_threads_http\":\"3\",\"port\":\"36013\",\"tid\":\"136599932254080\",\"timestamp\":1713927161}\n",
            "{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1578,\"msg\":\"all slots are idle and system prompt is empty, clear the KV cache\",\"tid\":\"136599932254080\",\"timestamp\":1713927161}\n",
            "{\"function\":\"process_single_task\",\"level\":\"INFO\",\"line\":1506,\"msg\":\"slot data\",\"n_idle_slots\":1,\"n_processing_slots\":0,\"task_id\":0,\"tid\":\"136599932254080\",\"timestamp\":1713927161}\n",
            "{\"function\":\"process_single_task\",\"level\":\"INFO\",\"line\":1506,\"msg\":\"slot data\",\"n_idle_slots\":1,\"n_processing_slots\":0,\"task_id\":1,\"tid\":\"136599932254080\",\"timestamp\":1713927161}\n",
            "{\"function\":\"process_single_task\",\"level\":\"INFO\",\"line\":1506,\"msg\":\"slot data\",\"n_idle_slots\":1,\"n_processing_slots\":0,\"task_id\":2,\"tid\":\"136599932254080\",\"timestamp\":1713927161}\n",
            "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2734,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/health\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":49122,\"status\":200,\"tid\":\"136595596375616\",\"timestamp\":1713927161}\n",
            "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2734,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/health\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":49132,\"status\":200,\"tid\":\"136595587982912\",\"timestamp\":1713927161}\n",
            "{\"function\":\"process_single_task\",\"level\":\"INFO\",\"line\":1506,\"msg\":\"slot data\",\"n_idle_slots\":1,\"n_processing_slots\":0,\"task_id\":3,\"tid\":\"136599932254080\",\"timestamp\":1713927161}\n",
            "{\"function\":\"process_single_task\",\"level\":\"INFO\",\"line\":1506,\"msg\":\"slot data\",\"n_idle_slots\":1,\"n_processing_slots\":0,\"task_id\":4,\"tid\":\"136599932254080\",\"timestamp\":1713927161}\n",
            "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2734,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/health\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":49144,\"status\":200,\"tid\":\"136595596375616\",\"timestamp\":1713927161}\n",
            "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2734,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/health\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":49146,\"status\":200,\"tid\":\"136595587982912\",\"timestamp\":1713927161}\n",
            "{\"function\":\"process_single_task\",\"level\":\"INFO\",\"line\":1506,\"msg\":\"slot data\",\"n_idle_slots\":1,\"n_processing_slots\":0,\"task_id\":5,\"tid\":\"136599932254080\",\"timestamp\":1713927161}\n",
            "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2734,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/health\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":49158,\"status\":200,\"tid\":\"136595596375616\",\"timestamp\":1713927161}\n",
            "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2734,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/health\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":49138,\"status\":200,\"tid\":\"136595579590208\",\"timestamp\":1713927161}\n",
            "{\"function\":\"process_single_task\",\"level\":\"INFO\",\"line\":1506,\"msg\":\"slot data\",\"n_idle_slots\":1,\"n_processing_slots\":0,\"task_id\":6,\"tid\":\"136599932254080\",\"timestamp\":1713927161}\n",
            "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2734,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/health\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":49214,\"status\":200,\"tid\":\"136595587982912\",\"timestamp\":1713927161}\n",
            "{\"function\":\"process_single_task\",\"level\":\"INFO\",\"line\":1506,\"msg\":\"slot data\",\"n_idle_slots\":1,\"n_processing_slots\":0,\"task_id\":7,\"tid\":\"136599932254080\",\"timestamp\":1713927161}\n",
            "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2734,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/health\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":49214,\"status\":200,\"tid\":\"136595587982912\",\"timestamp\":1713927161}\n",
            "{\"function\":\"launch_slot_with_data\",\"level\":\"INFO\",\"line\":830,\"msg\":\"slot is processing task\",\"slot_id\":0,\"task_id\":8,\"tid\":\"136599932254080\",\"timestamp\":1713927162}\n",
            "{\"function\":\"update_slots\",\"ga_i\":0,\"level\":\"INFO\",\"line\":1809,\"msg\":\"slot progression\",\"n_past\":0,\"n_past_se\":0,\"n_prompt_tokens_processed\":13,\"slot_id\":0,\"task_id\":8,\"tid\":\"136599932254080\",\"timestamp\":1713927162}\n",
            "{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1836,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"slot_id\":0,\"task_id\":8,\"tid\":\"136599932254080\",\"timestamp\":1713927162}\n",
            "{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":269,\"msg\":\"prompt eval time     =    3324.60 ms /    13 tokens (  255.74 ms per token,     3.91 tokens per second)\",\"n_prompt_tokens_processed\":13,\"n_tokens_second\":3.910249546110264,\"slot_id\":0,\"t_prompt_processing\":3324.596,\"t_token\":255.73815384615384,\"task_id\":8,\"tid\":\"136599932254080\",\"timestamp\":1713927222}\n",
            "{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":283,\"msg\":\"generation eval time =   57039.96 ms /   109 runs   (  523.30 ms per token,     1.91 tokens per second)\",\"n_decoded\":109,\"n_tokens_second\":1.9109411655226385,\"slot_id\":0,\"t_token\":523.3023486238532,\"t_token_generation\":57039.956,\"task_id\":8,\"tid\":\"136599932254080\",\"timestamp\":1713927222}\n",
            "{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":293,\"msg\":\"          total time =   60364.55 ms\",\"slot_id\":0,\"t_prompt_processing\":3324.596,\"t_token_generation\":57039.956,\"t_total\":60364.551999999996,\"task_id\":8,\"tid\":\"136599932254080\",\"timestamp\":1713927222}\n",
            "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2734,\"method\":\"POST\",\"msg\":\"request\",\"params\":{},\"path\":\"/completion\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":49214,\"status\":200,\"tid\":\"136595587982912\",\"timestamp\":1713927222}\n",
            "{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1640,\"msg\":\"slot released\",\"n_cache_tokens\":122,\"n_ctx\":4096,\"n_past\":121,\"n_system_tokens\":0,\"slot_id\":0,\"task_id\":8,\"tid\":\"136599932254080\",\"timestamp\":1713927222,\"truncated\":false}\n",
            "{\"function\":\"process_single_task\",\"level\":\"INFO\",\"line\":1506,\"msg\":\"slot data\",\"n_idle_slots\":1,\"n_processing_slots\":0,\"task_id\":120,\"tid\":\"136599932254080\",\"timestamp\":1713927222}\n",
            "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2734,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/health\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":49214,\"status\":200,\"tid\":\"136595587982912\",\"timestamp\":1713927222}\n",
            "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2734,\"method\":\"POST\",\"msg\":\"request\",\"params\":{},\"path\":\"/tokenize\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":49214,\"status\":200,\"tid\":\"136595587982912\",\"timestamp\":1713927222}\n",
            "[GIN] 2024/04/24 - 02:53:42 | 200 |          1m2s |       127.0.0.1 | POST     \"/api/generate\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llamafile"
      ],
      "metadata": {
        "id": "N5G-9ebRLLWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Mozilla-Ocho/llamafile/releases/download/0.7.3/llamafile-0.7.3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3uH6Fn2LKyb",
        "outputId": "5bd0a6fe-c381-4e51-80d1-d637ac071c9b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-24 03:36:23--  https://github.com/Mozilla-Ocho/llamafile/releases/download/0.7.3/llamafile-0.7.3\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/689773665/c0d692b0-e9d2-49ff-be8a-8401980e02cf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240424%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240424T033624Z&X-Amz-Expires=300&X-Amz-Signature=5d41127b6147bc3b99ecb8536f854b7ca635c54ed3b38c2cffb1e2cef3937328&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=689773665&response-content-disposition=attachment%3B%20filename%3Dllamafile-0.7.3&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-04-24 03:36:24--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/689773665/c0d692b0-e9d2-49ff-be8a-8401980e02cf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240424%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240424T033624Z&X-Amz-Expires=300&X-Amz-Signature=5d41127b6147bc3b99ecb8536f854b7ca635c54ed3b38c2cffb1e2cef3937328&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=689773665&response-content-disposition=attachment%3B%20filename%3Dllamafile-0.7.3&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24270244 (23M) [application/octet-stream]\n",
            "Saving to: llamafile-0.7.3\n",
            "\n",
            "llamafile-0.7.3     100%[===================>]  23.15M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-04-24 03:36:24 (159 MB/s) - llamafile-0.7.3 saved [24270244/24270244]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x llamafile-0.7.3\n"
      ],
      "metadata": {
        "id": "LI_M1KonL4Uh"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./llamafile-0.7.3 -ngl 9999 -m Phi-3-mini-4k-instruct-q4.gguf --temp 0.6 -p \"<|user|>\\nHow to explain Internet for a medieval knight?\\n<|end|>\\n<|assistant|>\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrUHAzPOLSRM",
        "outputId": "2f3cfa48-57d8-43f9-e626-a49b3f59413c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import_cuda_impl: initializing gpu module...\n",
            "extracting /zip/llama.cpp/ggml.h to /root/.llamafile/ggml.h\n",
            "extracting /zip/llamafile/compcap.cu to /root/.llamafile/compcap.cu\n",
            "extracting /zip/llamafile/llamafile.h to /root/.llamafile/llamafile.h\n",
            "extracting /zip/llamafile/tinyblas.h to /root/.llamafile/tinyblas.h\n",
            "extracting /zip/llamafile/tinyblas.cu to /root/.llamafile/tinyblas.cu\n",
            "extracting /zip/llama.cpp/ggml-impl.h to /root/.llamafile/ggml-impl.h\n",
            "extracting /zip/llama.cpp/ggml-cuda.h to /root/.llamafile/ggml-cuda.h\n",
            "extracting /zip/llama.cpp/ggml-alloc.h to /root/.llamafile/ggml-alloc.h\n",
            "extracting /zip/llama.cpp/ggml-common.h to /root/.llamafile/ggml-common.h\n",
            "extracting /zip/llama.cpp/ggml-backend.h to /root/.llamafile/ggml-backend.h\n",
            "extracting /zip/llama.cpp/ggml-backend-impl.h to /root/.llamafile/ggml-backend-impl.h\n",
            "extracting /zip/llama.cpp/ggml-cuda.cu to /root/.llamafile/ggml-cuda.cu\n",
            "get_rocm_bin_path: note: amdclang++ not found on $PATH\n",
            "get_rocm_bin_path: note: $HIP_PATH/bin/amdclang++ does not exist\n",
            "get_rocm_bin_path: note: /opt/rocm/bin/amdclang++ does not exist\n",
            "get_rocm_bin_path: note: hipInfo not found on $PATH\n",
            "get_rocm_bin_path: note: $HIP_PATH/bin/hipInfo does not exist\n",
            "get_rocm_bin_path: note: /opt/rocm/bin/hipInfo does not exist\n",
            "get_rocm_bin_path: note: rocminfo not found on $PATH\n",
            "get_rocm_bin_path: note: $HIP_PATH/bin/rocminfo does not exist\n",
            "get_rocm_bin_path: note: /opt/rocm/bin/rocminfo does not exist\n",
            "get_amd_offload_arch_flag: warning: can't find hipInfo/rocminfo commands for AMD GPU detection\n",
            "llamafile_log_command: hipcc -O3 -fPIC -shared -DNDEBUG --offload-arch=native -march=native -mtune=native -DGGML_BUILD=1 -DGGML_SHARED=1 -Wno-return-type -Wno-unused-result -DGGML_USE_HIPBLAS -DGGML_CUDA_MMV_Y=1 -DGGML_MULTIPLATFORM -DGGML_CUDA_DMMV_X=32 -DIGNORE4 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DIGNORE -o /root/.llamafile/ggml-rocm.so.973xmt /root/.llamafile/ggml-cuda.cu -lhipblas -lrocblas\n",
            "hipcc: No such file or directory\n",
            "extract_cuda_dso: note: prebuilt binary /zip/ggml-rocm.so not found\n",
            "compile_nvidia: note: building ggml-cuda with nvcc -arch=native...\n",
            "llamafile_log_command: /usr/local/cuda/bin/nvcc -arch=native --shared --forward-unknown-to-host-compiler --compiler-options \"-fPIC -O3 -march=native -mtune=native\" -DNDEBUG -DGGML_BUILD=1 -DGGML_SHARED=1 -DGGML_CUDA_MMV_Y=1 -DGGML_MULTIPLATFORM -DGGML_CUDA_DMMV_X=32 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -o /root/.llamafile/ggml-cuda.so.4fcyhq /root/.llamafile/ggml-cuda.cu -lcublas -lcuda\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "\u001b[01m\u001b[0m\u001b[01m/root/.llamafile/ggml-cuda.cu(364)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1305-D: function declared with \"noreturn\" does return\n",
            "  }\n",
            "  ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/root/.llamafile/ggml-cuda.cu(364)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1305-D: function declared with \"noreturn\" does return\n",
            "  }\n",
            "  ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[K/root/.llamafile/ggml-cuda.cu:\u001b[m\u001b[K In function \u001b[01m\u001b[Kvoid ggml_cuda_error(const char*, const char*, const char*, int, const char*)\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/root/.llamafile/ggml-cuda.cu:364:1:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\u001b[01m\u001b[Knoreturn\u001b[m\u001b[K function does return\n",
            "  364 | \u001b[01;35m\u001b[K}\u001b[m\u001b[K\n",
            "      | \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/root/.llamafile/ggml-cuda.cu:\u001b[m\u001b[K In function \u001b[01m\u001b[Kint64_t get_row_rounding(ggml_type, const std::array<float, 16>&)\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/root/.llamafile/ggml-cuda.cu:1152:18:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcontrol reaches end of non-void function [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wreturn-type\u0007-Wreturn-type\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 1152 | \u001b[01;35m\u001b[K            GGML_ASS\u001b[m\u001b[KERT(false);\n",
            "      |             \u001b[01;35m\u001b[K~~~~~^~~\u001b[m\u001b[K\n",
            "link_cuda_dso: note: dynamically linking /root/.llamafile/ggml-cuda.so\n",
            "link_cuda_dso: warning: libcuda.so.1: cannot open shared object file: No such file or directory: failed to load library\n",
            "warning: --n-gpu-layers 9999 was passed but no GPUs were found; falling back to CPU inference\n",
            "Log start\n",
            "main: llamafile version 0.7.3\n",
            "main: seed  = 1713929997\n",
            "llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32064\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 32000\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 323/32064 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32064\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 96\n",
            "llm_load_print_meta: n_embd_head_k    = 96\n",
            "llm_load_print_meta: n_embd_head_v    = 96\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
            "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 3.82 B\n",
            "llm_load_print_meta: model size       = 2.16 GiB (4.85 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
            "\u001b[Kllm_load_tensors:        CPU buffer size =  2210.78 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   192.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  192.00 MiB, K (f16):   96.00 MiB, V (f16):   96.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =    62.63 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    68.63 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1060\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.600\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 2048, n_predict = -1, n_keep = 1\n",
            "\n",
            "\n",
            "<|user|>\\nHow to explain Internet for a medieval knight?\\n<|end|>\\n<|assistant|> My noble knight, I shall endeavor to elucidate the concept of the Internet in a manner that resonates with your understanding of the world. Imagine, if you will, a grand tapestry woven with invisible threads that connect all corners of the realm. This tapestry is not of silk or wool, but a vast network of information and communication, much like the messengers who carry your orders across the kingdom.\\n\\nThe Internet is akin to a grand library, where every book ever written, every scroll and parchment, resides within its walls. However, this library is not bound by the physical constraints of space and time; it is accessible to all who seek knowledge. Just as you might send a message to your fellow knights or the king, information travels through this Internet, reaching distant lands in mere moments.\\n\\nThe Internet is also a gathering place, where people from all walks of life can meet and exchange ideas, much like the great councils and forums you might attend. Though it does not involve physical presence, the connections made are as real as the bonds forged between you and your fellow knights.\\n\\nYet, beware, for this tapestry is not without its shadows. Just as a castle's walls can be breached by an enemy, the Internet can be vulnerable to those who would use it for mischief. However, with wisdom and caution, the Internet can be a powerful tool for enlightenment, progress, and unity.\\n\\nI hope this humble explanation has shed some light on the mysterious workings of this Internet, a marvel of our age that transcends the boundaries of time and space.\\n\\n(Note: Keep in mind that a medieval knight's understanding of the world would be vastly different from our own, and this explanation is intended to bridge that gap in a way that is both respectful and imaginative.)\"\n",
            "}<|endoftext|> [end of text]\n",
            "\n",
            "\n",
            "llama_print_timings:        load time =     863.86 ms\n",
            "llama_print_timings:      sample time =      22.24 ms /   422 runs   (    0.05 ms per token, 18971.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6491.88 ms /    20 tokens (  324.59 ms per token,     3.08 tokens per second)\n",
            "llama_print_timings:        eval time =  161196.11 ms /   421 runs   (  382.89 ms per token,     2.61 tokens per second)\n",
            "llama_print_timings:       total time =  167916.28 ms /   441 tokens\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!./llamafile-0.7.3 -ngl 9999 -m Phi-3-mini-4k-instruct-q4.gguf\n"
      ],
      "metadata": {
        "id": "kdOU7XO6NF2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ONNX\n",
        "\n",
        "- https://github.com/microsoft/onnxruntime\n",
        "- https://github.com/microsoft/onnxruntime-genai\n",
        "- https://github.com/microsoft/onnxruntime-inference-examples"
      ],
      "metadata": {
        "id": "oxf8i94kN1V3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-onnx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gph64f3AO0bm",
        "outputId": "e90d4f9c-0a47-45b1-898d-02ed94ce4a7a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Phi-3-mini-4k-instruct-onnx'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (94/94), done.\u001b[K\n",
            "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
            "remote: Total 97 (delta 42), reused 0 (delta 0), pack-reused 3\u001b[K\n",
            "Unpacking objects: 100% (97/97), 504.62 KiB | 3.04 MiB/s, done.\n",
            "Filtering content: 100% (15/15), 12.31 GiB | 32.63 MiB/s, done.\n",
            "Encountered 1 file(s) that may not have been copied correctly on Windows:\n",
            "\tcuda/cuda-fp16/phi3-mini-4k-instruct-cuda-fp16.onnx.data\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install --pre onnxruntime-genai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AF1Ud4k1PCko",
        "outputId": "9964590b-064f-43a4-f8ce-078199ec5469"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Collecting onnxruntime-genai\n",
            "  Downloading onnxruntime_genai-0.2.0rc3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnxruntime-genai\n",
            "Successfully installed onnxruntime-genai-0.2.0rc3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/microsoft/onnxruntime-genai/main/examples/python/model-qa.py -O model-qa.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZckWXyBPa8E",
        "outputId": "1be73ac1-9737-4df8-93ea-5c84664fbcbd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-24 13:40:53--  https://raw.githubusercontent.com/microsoft/onnxruntime-genai/main/examples/python/model-qa.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4094 (4.0K) [text/plain]\n",
            "Saving to: model-qa.py\n",
            "\n",
            "\rmodel-qa.py           0%[                    ]       0  --.-KB/s               \rmodel-qa.py         100%[===================>]   4.00K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-24 13:40:53 (48.7 MB/s) - model-qa.py saved [4094/4094]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python model-qa.py -m Phi-3-mini-4k-instruct-onnx/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -g -r 1.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHCXTxrMPeyL",
        "outputId": "4292b7e8-4996-4736-ab57-3baff68a0eb3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <|user|>How to explain Internet for a medieval knight?<|end|><|assistant|>\n",
            "\n",
            "Output:  My dear Sir Knight,\n",
            "\n",
            "Imagine the vast kingdom of knowledge and communication spread across an endless expanse. This is what we call the \"Internet.\" It's like having access to every library in your realm at once, but even more extensive. The internet connects people from all corners of our world through invisible threads called 'data.' These data are messages or information that can be sent and received instantly by anyone with the right tools.\n",
            "\n",
            "Think of it as a magical network where you can send letters without needing messengers on horseback. Instead, these letters travel through thin air (or rather, cables and wireless signals) connecting distant lands within seconds. People share their thoughts, stories, music, artwork, and much more - just like how minstrels would sing tales around campfires back home.\n",
            "\n",
            "However, this magic comes with its own rules and challenges:\n",
            "\n",
            "1. To use the internet, one must have special devices known as computers, tablets, or smartphones. They act as portals into this mystical realm.\n",
            "2. There exist gatekeepers who protect the integrity of the information shared; they ensure only truthful and helpful content reaches those seeking knowledge.\n",
            "3. Just as there were laws governing chivalry and honor during your time, so too does the internet require adherence to certain codes of conduct when interacting with others online.\n",
            "4. While the internet offers great opportunities for learning and connection, it also has potential dangers such as misinformation and scams. Therefore, caution should always be exercised while navigating this new territory.\n",
            "\n",
            "In essence, my noble friend, the internet is a powerful tool that allows us to communicate, learn, create, and explore possibilities beyond the physical boundaries of our world. But remember, with great power comes great responsibility. Use it wisely and judiciously, and may it serve you well in your quest for knowledge and understanding.\n",
            "\n",
            "Prompt length: 15, New tokens: 413, Time to first: 6.82s, Prompt tokens per second: 2.20 tps, New tokens per second: 0.71 tps\n",
            "Input: \n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}