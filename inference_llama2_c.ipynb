{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weedge/doraemon-nb/blob/main/inference_llama2_c.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLdoj4cz-xal"
      },
      "source": [
        "# llama2.c\n",
        "\n",
        "- https://github.com/karpathy/llama2.c"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ulimit -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0JTm59kivp6",
        "outputId": "bc67821e-14ef-4e46-b833-89550dcefcc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "real-time non-blocking time  (microseconds, -R) unlimited\n",
            "core file size              (blocks, -c) unlimited\n",
            "data seg size               (kbytes, -d) unlimited\n",
            "scheduling priority                 (-e) 0\n",
            "file size                   (blocks, -f) unlimited\n",
            "pending signals                     (-i) 208844\n",
            "max locked memory           (kbytes, -l) 8192\n",
            "max memory size             (kbytes, -m) unlimited\n",
            "open files                          (-n) 1048576\n",
            "pipe size                (512 bytes, -p) 8\n",
            "POSIX message queues         (bytes, -q) 819200\n",
            "real-time priority                  (-r) 0\n",
            "stack size                  (kbytes, -s) 8192\n",
            "cpu time                   (seconds, -t) unlimited\n",
            "max user processes                  (-u) unlimited\n",
            "virtual memory              (kbytes, -v) unlimited\n",
            "file locks                          (-x) unlimited\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lscpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9XNS5Tio3is",
        "outputId": "fc14d4bc-60d6-4ce5-bad6-bd0cb907b355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:            x86_64\n",
            "  CPU op-mode(s):        32-bit, 64-bit\n",
            "  Address sizes:         46 bits physical, 48 bits virtual\n",
            "  Byte Order:            Little Endian\n",
            "CPU(s):                  8\n",
            "  On-line CPU(s) list:   0-7\n",
            "Vendor ID:               GenuineIntel\n",
            "  Model name:            Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "    CPU family:          6\n",
            "    Model:               79\n",
            "    Thread(s) per core:  2\n",
            "    Core(s) per socket:  4\n",
            "    Socket(s):           1\n",
            "    Stepping:            0\n",
            "    BogoMIPS:            4399.99\n",
            "    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clf\n",
            "                         lush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_\n",
            "                         good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fm\n",
            "                         a cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hyp\n",
            "                         ervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsb\n",
            "                         ase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsa\n",
            "                         veopt arat md_clear arch_capabilities\n",
            "Virtualization features: \n",
            "  Hypervisor vendor:     KVM\n",
            "  Virtualization type:   full\n",
            "Caches (sum of all):     \n",
            "  L1d:                   128 KiB (4 instances)\n",
            "  L1i:                   128 KiB (4 instances)\n",
            "  L2:                    1 MiB (4 instances)\n",
            "  L3:                    55 MiB (1 instance)\n",
            "NUMA:                    \n",
            "  NUMA node(s):          1\n",
            "  NUMA node0 CPU(s):     0-7\n",
            "Vulnerabilities:         \n",
            "  Gather data sampling:  Not affected\n",
            "  Itlb multihit:         Not affected\n",
            "  L1tf:                  Mitigation; PTE Inversion\n",
            "  Mds:                   Vulnerable; SMT Host state unknown\n",
            "  Meltdown:              Vulnerable\n",
            "  Mmio stale data:       Vulnerable\n",
            "  Retbleed:              Vulnerable\n",
            "  Spec rstack overflow:  Not affected\n",
            "  Spec store bypass:     Vulnerable\n",
            "  Spectre v1:            Vulnerable: __user pointer sanitization and usercopy barriers only; no swap\n",
            "                         gs barriers\n",
            "  Spectre v2:            Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Not affected\n",
            "  Srbds:                 Not affected\n",
            "  Tsx async abort:       Vulnerable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Une3Ozlnu1B7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a583be6d-4922-407d-9b5f-9a7dfc57e215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama2.c'...\n",
            "remote: Enumerating objects: 1434, done.\u001b[K\n",
            "remote: Total 1434 (delta 0), reused 0 (delta 0), pack-reused 1434\u001b[K\n",
            "Receiving objects: 100% (1434/1434), 1.17 MiB | 21.73 MiB/s, done.\n",
            "Resolving deltas: 100% (874/874), done.\n",
            "/content/llama2.c\n"
          ]
        }
      ],
      "source": [
        "#@title Clone Project\n",
        "\n",
        "!git clone https://github.com/karpathy/llama2.c.git\n",
        "%cd llama2.c"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgR50KffU3wR",
        "outputId": "299ae597-673e-41c0-a8f8-b0ec6cbe3a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.23.5)\n",
            "Collecting pytest==7.4.0 (from -r requirements.txt (line 2))\n",
            "  Downloading pytest-7.4.0-py3-none-any.whl (323 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.6/323.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.31.0)\n",
            "Collecting sentencepiece==0.1.99 (from -r requirements.txt (line 4))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==2.0.1 (from -r requirements.txt (line 5))\n",
            "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.64.1 (from -r requirements.txt (line 6))\n",
            "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb==0.15.5 (from -r requirements.txt (line 7))\n",
            "  Downloading wandb-0.15.5-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest==7.4.0->-r requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest==7.4.0->-r requirements.txt (line 2)) (23.2)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest==7.4.0->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest==7.4.0->-r requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest==7.4.0->-r requirements.txt (line 2)) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from Requests==2.31.0->-r requirements.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from Requests==2.31.0->-r requirements.txt (line 3)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from Requests==2.31.0->-r requirements.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from Requests==2.31.0->-r requirements.txt (line 3)) (2023.11.17)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 5)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 5)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 5)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 5)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 5)) (3.1.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0 (from torch==2.0.1->-r requirements.txt (line 5))\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.5->-r requirements.txt (line 7)) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb==0.15.5->-r requirements.txt (line 7))\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.5->-r requirements.txt (line 7)) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb==0.15.5->-r requirements.txt (line 7))\n",
            "  Downloading sentry_sdk-1.39.1-py2.py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb==0.15.5->-r requirements.txt (line 7))\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.5->-r requirements.txt (line 7)) (6.0.1)\n",
            "Collecting pathtools (from wandb==0.15.5->-r requirements.txt (line 7))\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb==0.15.5->-r requirements.txt (line 7))\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.5->-r requirements.txt (line 7)) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.5->-r requirements.txt (line 7)) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.5->-r requirements.txt (line 7)) (3.20.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 5)) (0.42.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 5)) (3.27.9)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 5))\n",
            "  Downloading lit-17.0.6.tar.gz (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb==0.15.5->-r requirements.txt (line 7)) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb==0.15.5->-r requirements.txt (line 7))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->-r requirements.txt (line 5)) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->-r requirements.txt (line 5)) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.15.5->-r requirements.txt (line 7))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools, lit\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8793 sha256=1a77622419fd5f7487c2bcedc62bd7e34659eec4706de9ccc1e3b20df676caa6\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-17.0.6-py3-none-any.whl size=93255 sha256=e1e8acb76ebf91b07554b460177a049412f5200274c43f9d490c24b0344a673c\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/dd/04/47d42976a6a86dc2ab66d7518621ae96f43452c8841d74758a\n",
            "Successfully built pathtools lit\n",
            "Installing collected packages: sentencepiece, pathtools, lit, tqdm, smmap, setproctitle, sentry-sdk, pytest, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, docker-pycreds, nvidia-cusolver-cu11, nvidia-cudnn-cu11, gitdb, GitPython, wandb, triton, torch\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.1\n",
            "    Uninstalling tqdm-4.66.1:\n",
            "      Successfully uninstalled tqdm-4.66.1\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 7.4.3\n",
            "    Uninstalling pytest-7.4.3:\n",
            "      Successfully uninstalled pytest-7.4.3\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 lit-17.0.6 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pathtools-0.1.2 pytest-7.4.0 sentencepiece-0.1.99 sentry-sdk-1.39.1 setproctitle-1.3.3 smmap-5.0.1 torch-2.0.1 tqdm-4.64.1 triton-2.0.0 wandb-0.15.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f3TNc6fMwk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a04dcf1-c3a2-477d-c9ad-60fe8862d063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gcc -Ofast -o run run.c -lm\n",
            "gcc -Ofast -o runq runq.c -lm\n"
          ]
        }
      ],
      "source": [
        "#@title Build with optimizer fast\n",
        "\n",
        "!make runfast"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Build with optimizer fast with openmp\n",
        "\n",
        "!make runomp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaUDIZPQwDvZ",
        "outputId": "122ecf95-a17c-433c-c9b2-14c34f60c117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gcc -Ofast -fopenmp -march=native run.c  -lm  -o run\n",
            "gcc -Ofast -fopenmp -march=native runq.c  -lm  -o runq\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thm0ZBrtSgoC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8e62e7a-e2bb-46fc-e791-b44d8e237a94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download_url: https://huggingface.co/karpathy/tinyllamas/resolve/main/stories110M.bin\n",
            "--2023-12-21 04:49:47--  https://huggingface.co/karpathy/tinyllamas/resolve/main/stories110M.bin\n",
            "Resolving huggingface.co (huggingface.co)... 18.238.49.70, 18.238.49.112, 18.238.49.117, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.238.49.70|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/88/4b/884bade32e5ee32eea725c5087af1358179a1bea94a4f6abc3c0470c9610ac38/515267168726a1ed1317a64a408492e6af3b67c1f71c5bd98c01d9d721803a24?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27stories110M.bin%3B+filename%3D%22stories110M.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1703393387&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzM5MzM4N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy84OC80Yi84ODRiYWRlMzJlNWVlMzJlZWE3MjVjNTA4N2FmMTM1ODE3OWExYmVhOTRhNGY2YWJjM2MwNDcwYzk2MTBhYzM4LzUxNTI2NzE2ODcyNmExZWQxMzE3YTY0YTQwODQ5MmU2YWYzYjY3YzFmNzFjNWJkOThjMDFkOWQ3MjE4MDNhMjQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=L7ZSQoaz6O7uyGjEUnN7AMjKWlA-7GvFwIw5hBJMuCegECrn-SvRyyYYVvCZ1pfaSA-zm11Lto8j6K-Z99K3ftfb2QweQT9ku7uxakszi0w9lM6aA42dzdv-GSJYXpbihCox%7ESa5fy6KXYEoJUD13c6f1mfFKEsIfuxPUzlFEAbRTKCPmcKtJu2oDdtp0W0ueN4vth988P%7EnSLcSfwrj5Dt3wwcwGsYBAIkKoYALhHMb4P29nSjI5bp6voiVvGeIjZI7ShSjD7XKmtVV-3bJ5%7EvgbPL7sihf7-XFaMOvigkBF%7EupjlfAl7L2eOrHK997Kg%7Ez6Y9eMVxw8qwsUOPbGw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-12-21 04:49:47--  https://cdn-lfs.huggingface.co/repos/88/4b/884bade32e5ee32eea725c5087af1358179a1bea94a4f6abc3c0470c9610ac38/515267168726a1ed1317a64a408492e6af3b67c1f71c5bd98c01d9d721803a24?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27stories110M.bin%3B+filename%3D%22stories110M.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1703393387&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzM5MzM4N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy84OC80Yi84ODRiYWRlMzJlNWVlMzJlZWE3MjVjNTA4N2FmMTM1ODE3OWExYmVhOTRhNGY2YWJjM2MwNDcwYzk2MTBhYzM4LzUxNTI2NzE2ODcyNmExZWQxMzE3YTY0YTQwODQ5MmU2YWYzYjY3YzFmNzFjNWJkOThjMDFkOWQ3MjE4MDNhMjQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=L7ZSQoaz6O7uyGjEUnN7AMjKWlA-7GvFwIw5hBJMuCegECrn-SvRyyYYVvCZ1pfaSA-zm11Lto8j6K-Z99K3ftfb2QweQT9ku7uxakszi0w9lM6aA42dzdv-GSJYXpbihCox%7ESa5fy6KXYEoJUD13c6f1mfFKEsIfuxPUzlFEAbRTKCPmcKtJu2oDdtp0W0ueN4vth988P%7EnSLcSfwrj5Dt3wwcwGsYBAIkKoYALhHMb4P29nSjI5bp6voiVvGeIjZI7ShSjD7XKmtVV-3bJ5%7EvgbPL7sihf7-XFaMOvigkBF%7EupjlfAl7L2eOrHK997Kg%7Ez6Y9eMVxw8qwsUOPbGw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.64.36, 108.138.64.121, 108.138.64.111, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.64.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 438381596 (418M) [application/octet-stream]\n",
            "Saving to: ‘stories110M.bin’\n",
            "\n",
            "stories110M.bin     100%[===================>] 418.07M  36.7MB/s    in 9.4s    \n",
            "\n",
            "2023-12-21 04:49:57 (44.3 MB/s) - ‘stories110M.bin’ saved [438381596/438381596]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@title Pick Your Model\n",
        "#@markdown https://github.com/karpathy/llama2.c?#models\n",
        "\n",
        "#@markdown Choose model https://huggingface.co/karpathy/tinyllamas\n",
        "model = \"stories110M\" #@param [\"stories15M\", \"stories42M\", \"stories110M\"]\n",
        "\n",
        "download_url = \"\"\n",
        "\n",
        "if(model == \"stories15M\"):\n",
        "  download_url = \"https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.bin\"\n",
        "if(model == \"stories42M\"):\n",
        "  download_url = \"https://huggingface.co/karpathy/tinyllamas/resolve/main/stories42M.bin\"\n",
        "if(model == \"stories110M\"):\n",
        "  download_url = \"https://huggingface.co/karpathy/tinyllamas/resolve/main/stories110M.bin\"\n",
        "\n",
        "print(f\"download_url: {download_url}\")\n",
        "\n",
        "!wget $download_url\n",
        "\n",
        "model_file = model + \".bin\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgAc3KjuT-NM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "1bf616fa-6bc7-4573-9a15-80101eeb0659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: stories110M.bin, max_token: 256, temperature: 0.8, top_p: 0.9, prompt: One day, Lily met a Shoggoth\n",
            "----------------------------\n",
            "\n",
            "./run stories110M.bin -t 0.8 -p 0.9 -n 256 -i \"One day, Lily met a Shoggoth\"\n",
            "One day, Lily met a Shoggoth at the beach. She thought he was very funny and asked him if he wanted to play. The Shoggoth said yes and they started to play together.\n",
            "Lily asked the Shoggoth what he liked to do. He said he liked to explore the beach and find things. He said he liked to eat shrimp. Lily thought that sounded yummy. \n",
            "The Shoggoth asked Lily if she wanted to try some of his shrimp. She said yes and she was very excited. He gave her a little bit and she tried it. It was so yummy! \n",
            "The Shoggoth asked Lily if she wanted to keep playing. She said yes and they had lots of fun together. She never forgot the yummy shrimp she tried.\n",
            "achieved tok/s: 12.868440\n"
          ]
        }
      ],
      "source": [
        "#@title Generate Stories\n",
        "\n",
        "# Generate args\n",
        "max_token = 256 #@param {type:\"slider\", min:32, max:1024, step:32}\n",
        "temperature = 0.8 #@param {type:\"slider\", min:0.0, max:1, step:0.05}\n",
        "top_p = 0.9 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "prompt = \"One day, Lily met a Shoggoth\" #@param {type:\"string\"}\n",
        "\n",
        "print(f\"model: {model_file}, max_token: {max_token}, temperature: {temperature}, top_p: {top_p}, prompt: {prompt}\")\n",
        "print(f\"----------------------------\\n\")\n",
        "\n",
        "cmd = f'./run {model_file} -t {temperature} -p {top_p} -n {max_token} -i \"{prompt}\"'\n",
        "print(cmd)\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "关于采样的快速说明，为了获得最佳结果，建议使用 -t 1.0 -p 0.9 ，即温度temperature 1.0（默认），使用 进行 top-p 0.9（默认）采样。直观上，top-p 保证了概率很小的 token 不会被采样，所以我们在采样过程中不会“倒霉unlucky”，之后也不太可能“出轨off the rails”。\n",
        "\n",
        "一般而言，要控制样本的多样性，请使用温度（即-t在 0 和 1 之间变化并保持 top-p 关闭-p 0）；\n",
        "\n",
        "或者 top-p 值（即-p在 0 和 1 之间变化并保持-t 1），但不能同时使用两者。\n",
        "\n",
        "关于 LLM 抽样策略的很好的解释参考如下：\n",
        "\n",
        "- https://peterchng.com/blog/2023/05/02/token-selection-strategies-top-k-top-p-and-temperature/\n",
        "- https://docs.cohere.com/docs/controlling-generation-with-top-k-top-p\n",
        "- https://huggingface.co/blog/how-to-generate\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EC1EYjo8uV2P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZYNHQn2Mwk9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "157cdeb2fe4449de966ff9f66678368d",
            "8a1c137915ce407c9664aaa9d09783a1",
            "90a1f3a5f34e4999a9d93566bb235144",
            "89dfc79597204cb4b3f2796581125682",
            "41318164b0be46b481672fc5f13c8cda",
            "6d08b07a280f413baff17581fa4f3e7e",
            "b1aed9178ab04ecdbfb5c7b1a6235f11",
            "4e7728e39a1b4f72a189487478640fce",
            "96138946da7244179dcd5b8a3275e557",
            "dede50bce2fa4c39bde7a75d85e546f2",
            "1164c52222964166b26224778db0cb2b"
          ]
        },
        "outputId": "7758f0eb-a1e4-495f-ae7b-c6d147cbdc0e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "157cdeb2fe4449de966ff9f66678368d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Llama-2-7b/models--meta-llama--Llama-2-7b/snapshots/1c9f047f0e1dbe2e1be6f15f5107bf9f74bb425f\n",
            "{'dim': 4096, 'multiple_of': 256, 'n_heads': 32, 'n_layers': 32, 'norm_eps': 1e-05, 'vocab_size': -1}\n",
            "wrote llama2_7b.bin\n"
          ]
        }
      ],
      "source": [
        "#@title Run Meta's Llama 2 models\n",
        "\n",
        "#@markdown input your huggingface [access token](https://huggingface.co/settings/tokens) to download Meta's Llama 2 models.\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "token = userdata.get('HF_TOKEN')\n",
        "#token = \"\" #@param {type:\"string\"}\n",
        "\n",
        "path = snapshot_download(repo_id=\"meta-llama/Llama-2-7b\",cache_dir=\"Llama-2-7b\", use_auth_token=token)\n",
        "print(path)\n",
        "\n",
        "!python export.py llama2_7b.bin --meta-llama $path\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh llama2_7b.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fvghepjyKlN",
        "outputId": "ec48f8cf-f502-44bf-969e-516a2738993d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 26G Dec 21 04:26 llama2_7b.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"./run llama2_7b.bin\\n\")\n",
        "!./run llama2_7b.bin\n",
        "\n",
        "# run on 8 * Intel(R) Xeon(R) CPU @ 2.20GHz; so slow 2.452token/10s , SLA bad;\n",
        "# maybe train a small LLM like stories model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEOV2xfPjz5g",
        "outputId": "bfdd8bd3-b9b7-4ad6-b174-19d9dd8f6a60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./run llama2_7b.bin\n",
            "\n",
            "Lily’s Story: “I didn’t have any connection with the baby”\n",
            "“I’m going to tell you about something that happened to me, and it’s going to be very hard for me to talk about.” (Caroline, Professional Birth Story Teller, Listening Service).\n",
            "Listening to the stories that people tell you about their births is the hardest and the most wonderful thing to do. You may never hear the details of the story, but just listening to it will make you appreciate life and love in a new way. I think that each birth story is beautiful and amazing in its own right, but for the purpose of this post, I am going to tell you a story about the realities of birth and the impact that it can have on everyone involved.\n",
            "Lily’s Story:\n",
            "I’m going to tell you about something that happened to me, and it’s going to be very hard for me to talk about.”\n",
            "Lily, aged 40+\n",
            "How was the pregnancy?\n",
            "“It was really good. We didn’t have any complications, and there was no danger for the baby. We had a natural birth plan, so we were very\n",
            "achieved tok/s: 0.245207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!(OMP_NUM_THREADS=8 ./run llama2_7b.bin)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCWR60AgVzGO",
        "outputId": "93726c1c-a2f8-4747-caf7-8d42cd0e37d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Miranda had been right about that. She knew it without a doubt. If she died, the plague would spread, and if the plague spread, millions would die.\n",
            "And if Miranda was right, he was certain it would spread, and so far she'd been right about everything, so he did the only thing he could do.\n",
            "He looked at her with a frightened pleading gaze.\n",
            "\"Miranda, you have to be wrong. That's something I have to believe. You can't be wrong.\"\n",
            "She blinked and stared down at her glass. A second later, she took a sip of the strong, bitter drink. Her look was filled with more pain than he'd ever seen on her face, and he wondered if she thought the plague might not spread after all. He shook his head, feeling an ache in his chest as he looked at her.\n",
            "Then, as if remembering her strange amulet, she suddenly reached out a hand and pulled his arm around her shoulders. It was a loving gesture, a surprising gesture, and it gave Dirk pause.\n",
            "He wished it could be as simple as that.\n",
            "If he could,\n",
            "achieved tok/s: 0.983591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Meta's Llama2-chat\n",
        "!python export.py llama2_7b_chat.bin --meta-llama $path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNvsmr41Esca",
        "outputId": "916d04f4-5c5b-4784-ab7f-0c7178a42a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'dim': 4096, 'multiple_of': 256, 'n_heads': 32, 'n_layers': 32, 'norm_eps': 1e-05, 'vocab_size': -1}\n",
            "wrote llama2_7b_chat.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!OMP_NUM_THREADS=8 ./run llama2_7b_chat.bin -m chat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E02fXON1E7Dj",
        "outputId": "8fa8262f-0fde-4b1a-b85d-78287fc3f830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter system prompt (optional): \n",
            "User: One day, Lily met a Shoggoth\n",
            "Assistant: \n",
            "[INST] A Kraken washed up in Singapore [/INST]\n",
            "[INST] Not everyone is good at voodoo [/INST]\n",
            "[INST] Not for everyone [/INST]\n",
            "[INST] Old man yells at Shoggoth [/INST]\n",
            "[INST] Lily stepped into an American horror [/INST]\n",
            "[INST] Lily was lost [/INST]\n",
            "[INST] [/INST]\n",
            "This is insane, almost too much content...\n",
            "Probably worth a few points though.\n",
            "[Voidfall] Found a 6/6 Abyssal Goliath\n",
            "[Mythos] Catheterized an elder\n",
            "[Delirium] Can't quite pull together the Tomb Guardian I need...yet\n",
            "This thread is hilarious. It feels like several posts were done simultaneously.\n",
            "\n",
            "<s>\n",
            "User: :q\n",
            "Assistant: \n",
            "Labels: Intricate, Tangled in Purple\n",
            "wow. just wow. it's amazing!! sooo detailed and beautiful.\n",
            "Wow! Great work :) Love it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "上面的（默认）脚本run.c使用 float32 前向传递，其中前向传递的整个计算都保存在 fp32 中。就参考代码而言，这很容易理解，但它有以下缺点：\n",
        "\n",
        "模型检查点文件非常大（每个单独的权重需要 4 个字节），并且前向传递相对较慢。实践中采用的（非常）常见的推理优化是将模型参数量化为较低的精度，放弃一点正确性，以换取较小的检查点大小和更快的前向传递（因为大多数推理使用整数算术）。\n",
        "\n",
        "根据经验，LLM 可以容忍低至 4 位（甚至更低）的精度，但我们在这里使用 int8，因为它是一种“安全”设置，可以为我们带来好处，但不会牺牲太多模型精度。\n",
        "\n",
        "仅对参与 matmul 的权重进行量化。所有其他参数（例如，尤其是 RMSNorm 中的比例scale和偏差bias）都保存在 float32 中，因为这些层非常敏感。\n",
        "\n",
        "现在，如果您所追求的只是减少检查点大小，则可以量化权重，保存检查点，然后在 run.c 中对它们进行反量化，并正常进行 float32 推理，然后就到此为止了。这完全没问题。\n",
        "\n",
        "但在这里，我们更进一步（按照标准做法）并另外量化前向传递中的激活。这就需要我们在运行时动态地在float32和int8之间进行量化和反量化，这就增加了开销。但好处是，现在大多数计算（尤其是 matmul！）都使用纯整数算术，其中权重和激活都以 int8 形式输入。这就是加速的根本来源。我们使用的版本是“Q8_0”量化（llama.cpp 术语），其中 0 表示权重量化围绕 0 对称，量化范围为 [-127, 127]。\n",
        "\n",
        "量化前向传播在runq.c中实现。要使用它，我们必须以量化格式导出模型。为了将其量化导出，我们改为使用版本 2 导出：\n",
        "\n",
        "```\n",
        "python export.py llama2_7b_q80.bin --version 2 --meta-llama path/to/llama/model/7B\n",
        "```"
      ],
      "metadata": {
        "id": "j7v6P2rswpz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Meta's Llama 2 models with int8 quantization\n",
        "!python export.py llama2_7b_q80.bin --version 2  --meta-llama $path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOXjTKUNofZq",
        "outputId": "a83e7c1d-3275-44c7-825e-05f87e241a82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'dim': 4096, 'multiple_of': 256, 'n_heads': 32, 'n_layers': 32, 'norm_eps': 1e-05, 'vocab_size': -1}\n",
            "1/226 quantized (32000, 4096) to Q8_0 with max error 0.0010248618200421333\n",
            "2/226 quantized (4096, 4096) to Q8_0 with max error 0.0030364990234375\n",
            "3/226 quantized (4096, 4096) to Q8_0 with max error 0.0017772279679775238\n",
            "4/226 quantized (4096, 4096) to Q8_0 with max error 0.004190757870674133\n",
            "5/226 quantized (4096, 4096) to Q8_0 with max error 0.003016674891114235\n",
            "6/226 quantized (4096, 4096) to Q8_0 with max error 0.0026576723903417587\n",
            "7/226 quantized (4096, 4096) to Q8_0 with max error 0.002516377717256546\n",
            "8/226 quantized (4096, 4096) to Q8_0 with max error 0.002410888671875\n",
            "9/226 quantized (4096, 4096) to Q8_0 with max error 0.002872016280889511\n",
            "10/226 quantized (4096, 4096) to Q8_0 with max error 0.0021741949021816254\n",
            "11/226 quantized (4096, 4096) to Q8_0 with max error 0.0030253250151872635\n",
            "12/226 quantized (4096, 4096) to Q8_0 with max error 0.0018147137016057968\n",
            "13/226 quantized (4096, 4096) to Q8_0 with max error 0.0023375973105430603\n",
            "14/226 quantized (4096, 4096) to Q8_0 with max error 0.002032903954386711\n",
            "15/226 quantized (4096, 4096) to Q8_0 with max error 0.0026336461305618286\n",
            "16/226 quantized (4096, 4096) to Q8_0 with max error 0.002233792096376419\n",
            "17/226 quantized (4096, 4096) to Q8_0 with max error 0.0021866923198103905\n",
            "18/226 quantized (4096, 4096) to Q8_0 with max error 0.00390625\n",
            "19/226 quantized (4096, 4096) to Q8_0 with max error 0.0036707594990730286\n",
            "20/226 quantized (4096, 4096) to Q8_0 with max error 0.003020036965608597\n",
            "21/226 quantized (4096, 4096) to Q8_0 with max error 0.0027739759534597397\n",
            "22/226 quantized (4096, 4096) to Q8_0 with max error 0.004138857126235962\n",
            "23/226 quantized (4096, 4096) to Q8_0 with max error 0.0035063978284597397\n",
            "24/226 quantized (4096, 4096) to Q8_0 with max error 0.003364142496138811\n",
            "25/226 quantized (4096, 4096) to Q8_0 with max error 0.0032113157212734222\n",
            "26/226 quantized (4096, 4096) to Q8_0 with max error 0.003852423280477524\n",
            "27/226 quantized (4096, 4096) to Q8_0 with max error 0.00252695195376873\n",
            "28/226 quantized (4096, 4096) to Q8_0 with max error 0.003386249765753746\n",
            "29/226 quantized (4096, 4096) to Q8_0 with max error 0.0029844753444194794\n",
            "30/226 quantized (4096, 4096) to Q8_0 with max error 0.00296176690608263\n",
            "31/226 quantized (4096, 4096) to Q8_0 with max error 0.00308828242123127\n",
            "32/226 quantized (4096, 4096) to Q8_0 with max error 0.0031776726245880127\n",
            "33/226 quantized (4096, 4096) to Q8_0 with max error 0.002616340294480324\n",
            "34/226 quantized (4096, 4096) to Q8_0 with max error 0.003168061375617981\n",
            "35/226 quantized (4096, 4096) to Q8_0 with max error 0.002253494691103697\n",
            "36/226 quantized (4096, 4096) to Q8_0 with max error 0.0014268774539232254\n",
            "37/226 quantized (4096, 4096) to Q8_0 with max error 0.0015897974371910095\n",
            "38/226 quantized (4096, 4096) to Q8_0 with max error 0.0020112767815589905\n",
            "39/226 quantized (4096, 4096) to Q8_0 with max error 0.0013033673167228699\n",
            "40/226 quantized (4096, 4096) to Q8_0 with max error 0.001204364001750946\n",
            "41/226 quantized (4096, 4096) to Q8_0 with max error 0.0012437701225280762\n",
            "42/226 quantized (4096, 4096) to Q8_0 with max error 0.0011793728917837143\n",
            "43/226 quantized (4096, 4096) to Q8_0 with max error 0.0013600746169686317\n",
            "44/226 quantized (4096, 4096) to Q8_0 with max error 0.001126505434513092\n",
            "45/226 quantized (4096, 4096) to Q8_0 with max error 0.0010652318596839905\n",
            "46/226 quantized (4096, 4096) to Q8_0 with max error 0.0011721635237336159\n",
            "47/226 quantized (4096, 4096) to Q8_0 with max error 0.0011836974881589413\n",
            "48/226 quantized (4096, 4096) to Q8_0 with max error 0.0012370431795716286\n",
            "49/226 quantized (4096, 4096) to Q8_0 with max error 0.0010073184967041016\n",
            "50/226 quantized (4096, 4096) to Q8_0 with max error 0.0011168941855430603\n",
            "51/226 quantized (4096, 4096) to Q8_0 with max error 0.0011957120150327682\n",
            "52/226 quantized (4096, 4096) to Q8_0 with max error 0.0012286328710615635\n",
            "53/226 quantized (4096, 4096) to Q8_0 with max error 0.0011721635237336159\n",
            "54/226 quantized (4096, 4096) to Q8_0 with max error 0.0012831799685955048\n",
            "55/226 quantized (4096, 4096) to Q8_0 with max error 0.001465023960918188\n",
            "56/226 quantized (4096, 4096) to Q8_0 with max error 0.001230795867741108\n",
            "57/226 quantized (4096, 4096) to Q8_0 with max error 0.0012130141258239746\n",
            "58/226 quantized (4096, 4096) to Q8_0 with max error 0.0012514609843492508\n",
            "59/226 quantized (4096, 4096) to Q8_0 with max error 0.0014360081404447556\n",
            "60/226 quantized (4096, 4096) to Q8_0 with max error 0.001378338783979416\n",
            "61/226 quantized (4096, 4096) to Q8_0 with max error 0.0010995930060744286\n",
            "62/226 quantized (4096, 4096) to Q8_0 with max error 0.001432165503501892\n",
            "63/226 quantized (4096, 4096) to Q8_0 with max error 0.0014398526400327682\n",
            "64/226 quantized (4096, 4096) to Q8_0 with max error 0.0014492245391011238\n",
            "65/226 quantized (4096, 4096) to Q8_0 with max error 0.0015801861882209778\n",
            "66/226 quantized (4096, 4096) to Q8_0 with max error 0.0005267281085252762\n",
            "67/226 quantized (4096, 4096) to Q8_0 with max error 0.0004959702491760254\n",
            "68/226 quantized (4096, 4096) to Q8_0 with max error 0.0005769503768533468\n",
            "69/226 quantized (4096, 4096) to Q8_0 with max error 0.0005457121878862381\n",
            "70/226 quantized (4096, 4096) to Q8_0 with max error 0.0005580872530117631\n",
            "71/226 quantized (4096, 4096) to Q8_0 with max error 0.0010223388671875\n",
            "72/226 quantized (4096, 4096) to Q8_0 with max error 0.0011267473455518484\n",
            "73/226 quantized (4096, 4096) to Q8_0 with max error 0.0008857306092977524\n",
            "74/226 quantized (4096, 4096) to Q8_0 with max error 0.0008150842040777206\n",
            "75/226 quantized (4096, 4096) to Q8_0 with max error 0.0009698346257209778\n",
            "76/226 quantized (4096, 4096) to Q8_0 with max error 0.0010216180235147476\n",
            "77/226 quantized (4096, 4096) to Q8_0 with max error 0.0011721635237336159\n",
            "78/226 quantized (4096, 4096) to Q8_0 with max error 0.00098419189453125\n",
            "79/226 quantized (4096, 4096) to Q8_0 with max error 0.0008987076580524445\n",
            "80/226 quantized (4096, 4096) to Q8_0 with max error 0.0013069694396108389\n",
            "81/226 quantized (4096, 4096) to Q8_0 with max error 0.0008875027997419238\n",
            "82/226 quantized (4096, 4096) to Q8_0 with max error 0.0009361915290355682\n",
            "83/226 quantized (4096, 4096) to Q8_0 with max error 0.0008352687582373619\n",
            "84/226 quantized (4096, 4096) to Q8_0 with max error 0.0008792425505816936\n",
            "85/226 quantized (4096, 4096) to Q8_0 with max error 0.0008665071800351143\n",
            "86/226 quantized (4096, 4096) to Q8_0 with max error 0.0006151574198156595\n",
            "87/226 quantized (4096, 4096) to Q8_0 with max error 0.0006992612034082413\n",
            "88/226 quantized (4096, 4096) to Q8_0 with max error 0.0006008598720654845\n",
            "89/226 quantized (4096, 4096) to Q8_0 with max error 0.0005959337577223778\n",
            "90/226 quantized (4096, 4096) to Q8_0 with max error 0.0006997436285018921\n",
            "91/226 quantized (4096, 4096) to Q8_0 with max error 0.000494047999382019\n",
            "92/226 quantized (4096, 4096) to Q8_0 with max error 0.0007718303240835667\n",
            "93/226 quantized (4096, 4096) to Q8_0 with max error 0.0005089463666081429\n",
            "94/226 quantized (4096, 4096) to Q8_0 with max error 0.0006526429206132889\n",
            "95/226 quantized (4096, 4096) to Q8_0 with max error 0.0004993351176381111\n",
            "96/226 quantized (4096, 4096) to Q8_0 with max error 0.0018305741250514984\n",
            "97/226 quantized (4096, 4096) to Q8_0 with max error 0.0015239566564559937\n",
            "98/226 quantized (4096, 4096) to Q8_0 with max error 0.0017243623733520508\n",
            "99/226 quantized (4096, 4096) to Q8_0 with max error 0.0023193359375\n",
            "100/226 quantized (4096, 4096) to Q8_0 with max error 0.002368360757827759\n",
            "101/226 quantized (4096, 4096) to Q8_0 with max error 0.0021107569336891174\n",
            "102/226 quantized (4096, 4096) to Q8_0 with max error 0.002387579530477524\n",
            "103/226 quantized (4096, 4096) to Q8_0 with max error 0.0020069479942321777\n",
            "104/226 quantized (4096, 4096) to Q8_0 with max error 0.0023337528109550476\n",
            "105/226 quantized (4096, 4096) to Q8_0 with max error 0.0021991729736328125\n",
            "106/226 quantized (4096, 4096) to Q8_0 with max error 0.0029604434967041016\n",
            "107/226 quantized (4096, 4096) to Q8_0 with max error 0.0020223408937454224\n",
            "108/226 quantized (4096, 4096) to Q8_0 with max error 0.001614794135093689\n",
            "109/226 quantized (4096, 4096) to Q8_0 with max error 0.002245306968688965\n",
            "110/226 quantized (4096, 4096) to Q8_0 with max error 0.0020300187170505524\n",
            "111/226 quantized (4096, 4096) to Q8_0 with max error 0.0019954144954681396\n",
            "112/226 quantized (4096, 4096) to Q8_0 with max error 0.002276092767715454\n",
            "113/226 quantized (4096, 4096) to Q8_0 with max error 0.0018930509686470032\n",
            "114/226 quantized (4096, 4096) to Q8_0 with max error 0.001676306128501892\n",
            "115/226 quantized (4096, 4096) to Q8_0 with max error 0.0019223690032958984\n",
            "116/226 quantized (4096, 4096) to Q8_0 with max error 0.0022184178233146667\n",
            "117/226 quantized (4096, 4096) to Q8_0 with max error 0.002602890133857727\n",
            "118/226 quantized (4096, 4096) to Q8_0 with max error 0.0022760629653930664\n",
            "119/226 quantized (4096, 4096) to Q8_0 with max error 0.002168416976928711\n",
            "120/226 quantized (4096, 4096) to Q8_0 with max error 0.00302964448928833\n",
            "121/226 quantized (4096, 4096) to Q8_0 with max error 0.0018916130065917969\n",
            "122/226 quantized (4096, 4096) to Q8_0 with max error 0.0027989596128463745\n",
            "123/226 quantized (4096, 4096) to Q8_0 with max error 0.002525029703974724\n",
            "124/226 quantized (4096, 4096) to Q8_0 with max error 0.002706676721572876\n",
            "125/226 quantized (4096, 4096) to Q8_0 with max error 0.0024875402450561523\n",
            "126/226 quantized (4096, 4096) to Q8_0 with max error 0.0030450373888015747\n",
            "127/226 quantized (4096, 4096) to Q8_0 with max error 0.002491384744644165\n",
            "128/226 quantized (4096, 4096) to Q8_0 with max error 0.0024452507495880127\n",
            "129/226 quantized (4096, 4096) to Q8_0 with max error 0.005113497376441956\n",
            "130/226 quantized (11008, 4096) to Q8_0 with max error 0.0034429598599672318\n",
            "131/226 quantized (11008, 4096) to Q8_0 with max error 0.004259966313838959\n",
            "132/226 quantized (11008, 4096) to Q8_0 with max error 0.0020599365234375\n",
            "133/226 quantized (11008, 4096) to Q8_0 with max error 0.002258777618408203\n",
            "134/226 quantized (11008, 4096) to Q8_0 with max error 0.0020003430545330048\n",
            "135/226 quantized (11008, 4096) to Q8_0 with max error 0.0017320532351732254\n",
            "136/226 quantized (11008, 4096) to Q8_0 with max error 0.0016326904296875\n",
            "137/226 quantized (11008, 4096) to Q8_0 with max error 0.0017647333443164825\n",
            "138/226 quantized (11008, 4096) to Q8_0 with max error 0.0015369318425655365\n",
            "139/226 quantized (11008, 4096) to Q8_0 with max error 0.00147247314453125\n",
            "140/226 quantized (11008, 4096) to Q8_0 with max error 0.0019838809967041016\n",
            "141/226 quantized (11008, 4096) to Q8_0 with max error 0.001556396484375\n",
            "142/226 quantized (11008, 4096) to Q8_0 with max error 0.00138092041015625\n",
            "143/226 quantized (11008, 4096) to Q8_0 with max error 0.0014446582645177841\n",
            "144/226 quantized (11008, 4096) to Q8_0 with max error 0.0013437308371067047\n",
            "145/226 quantized (11008, 4096) to Q8_0 with max error 0.0016134667675942183\n",
            "146/226 quantized (11008, 4096) to Q8_0 with max error 0.001867581158876419\n",
            "147/226 quantized (11008, 4096) to Q8_0 with max error 0.0015869140625\n",
            "148/226 quantized (11008, 4096) to Q8_0 with max error 0.001448504626750946\n",
            "149/226 quantized (11008, 4096) to Q8_0 with max error 0.0014994442462921143\n",
            "150/226 quantized (11008, 4096) to Q8_0 with max error 0.0012603518553078175\n",
            "151/226 quantized (11008, 4096) to Q8_0 with max error 0.0011536604724824429\n",
            "152/226 quantized (11008, 4096) to Q8_0 with max error 0.0014497051015496254\n",
            "153/226 quantized (11008, 4096) to Q8_0 with max error 0.0016263220459222794\n",
            "154/226 quantized (11008, 4096) to Q8_0 with max error 0.0013668015599250793\n",
            "155/226 quantized (11008, 4096) to Q8_0 with max error 0.002172272652387619\n",
            "156/226 quantized (11008, 4096) to Q8_0 with max error 0.0016043956857174635\n",
            "157/226 quantized (11008, 4096) to Q8_0 with max error 0.00248874444514513\n",
            "158/226 quantized (11008, 4096) to Q8_0 with max error 0.0015628840774297714\n",
            "159/226 quantized (11008, 4096) to Q8_0 with max error 0.001821441575884819\n",
            "160/226 quantized (11008, 4096) to Q8_0 with max error 0.00312960147857666\n",
            "161/226 quantized (11008, 4096) to Q8_0 with max error 0.0014840662479400635\n",
            "162/226 quantized (4096, 11008) to Q8_0 with max error 0.0020112767815589905\n",
            "163/226 quantized (4096, 11008) to Q8_0 with max error 0.006138598546385765\n",
            "164/226 quantized (4096, 11008) to Q8_0 with max error 0.0035037542693316936\n",
            "165/226 quantized (4096, 11008) to Q8_0 with max error 0.0026509426534175873\n",
            "166/226 quantized (4096, 11008) to Q8_0 with max error 0.0036659538745880127\n",
            "167/226 quantized (4096, 11008) to Q8_0 with max error 0.0027364902198314667\n",
            "168/226 quantized (4096, 11008) to Q8_0 with max error 0.003614051267504692\n",
            "169/226 quantized (4096, 11008) to Q8_0 with max error 0.003644809126853943\n",
            "170/226 quantized (4096, 11008) to Q8_0 with max error 0.003102697432041168\n",
            "171/226 quantized (4096, 11008) to Q8_0 with max error 0.002606727182865143\n",
            "172/226 quantized (4096, 11008) to Q8_0 with max error 0.003913938999176025\n",
            "173/226 quantized (4096, 11008) to Q8_0 with max error 0.002893161028623581\n",
            "174/226 quantized (4096, 11008) to Q8_0 with max error 0.0025211842730641365\n",
            "175/226 quantized (4096, 11008) to Q8_0 with max error 0.004164807498455048\n",
            "176/226 quantized (4096, 11008) to Q8_0 with max error 0.0025836611166596413\n",
            "177/226 quantized (4096, 11008) to Q8_0 with max error 0.004133090376853943\n",
            "178/226 quantized (4096, 11008) to Q8_0 with max error 0.0029719769954681396\n",
            "179/226 quantized (4096, 11008) to Q8_0 with max error 0.004445954225957394\n",
            "180/226 quantized (4096, 11008) to Q8_0 with max error 0.004265733063220978\n",
            "181/226 quantized (4096, 11008) to Q8_0 with max error 0.003715936094522476\n",
            "182/226 quantized (4096, 11008) to Q8_0 with max error 0.00421142578125\n",
            "183/226 quantized (4096, 11008) to Q8_0 with max error 0.002871055155992508\n",
            "184/226 quantized (4096, 11008) to Q8_0 with max error 0.0015253983438014984\n",
            "185/226 quantized (4096, 11008) to Q8_0 with max error 0.005128875374794006\n",
            "186/226 quantized (4096, 11008) to Q8_0 with max error 0.0028196312487125397\n",
            "187/226 quantized (4096, 11008) to Q8_0 with max error 0.004042737185955048\n",
            "188/226 quantized (4096, 11008) to Q8_0 with max error 0.0022433996200561523\n",
            "189/226 quantized (4096, 11008) to Q8_0 with max error 0.0029950477182865143\n",
            "190/226 quantized (4096, 11008) to Q8_0 with max error 0.003936767578125\n",
            "191/226 quantized (4096, 11008) to Q8_0 with max error 0.0038142167031764984\n",
            "192/226 quantized (4096, 11008) to Q8_0 with max error 0.005024106241762638\n",
            "193/226 quantized (4096, 11008) to Q8_0 with max error 0.006980113685131073\n",
            "194/226 quantized (11008, 4096) to Q8_0 with max error 0.0013295579701662064\n",
            "195/226 quantized (11008, 4096) to Q8_0 with max error 0.001595567911863327\n",
            "196/226 quantized (11008, 4096) to Q8_0 with max error 0.001787801505997777\n",
            "197/226 quantized (11008, 4096) to Q8_0 with max error 0.0019831620156764984\n",
            "198/226 quantized (11008, 4096) to Q8_0 with max error 0.0018637347966432571\n",
            "199/226 quantized (11008, 4096) to Q8_0 with max error 0.0019073486328125\n",
            "200/226 quantized (11008, 4096) to Q8_0 with max error 0.002149207517504692\n",
            "201/226 quantized (11008, 4096) to Q8_0 with max error 0.0013722698204219341\n",
            "202/226 quantized (11008, 4096) to Q8_0 with max error 0.00244140625\n",
            "203/226 quantized (11008, 4096) to Q8_0 with max error 0.0022957869805395603\n",
            "204/226 quantized (11008, 4096) to Q8_0 with max error 0.00148773193359375\n",
            "205/226 quantized (11008, 4096) to Q8_0 with max error 0.00164031982421875\n",
            "206/226 quantized (11008, 4096) to Q8_0 with max error 0.0009409990161657333\n",
            "207/226 quantized (11008, 4096) to Q8_0 with max error 0.0016863965429365635\n",
            "208/226 quantized (11008, 4096) to Q8_0 with max error 0.001865176483988762\n",
            "209/226 quantized (11008, 4096) to Q8_0 with max error 0.0016818309668451548\n",
            "210/226 quantized (11008, 4096) to Q8_0 with max error 0.0018213826697319746\n",
            "211/226 quantized (11008, 4096) to Q8_0 with max error 0.0013105738908052444\n",
            "212/226 quantized (11008, 4096) to Q8_0 with max error 0.001276091206818819\n",
            "213/226 quantized (11008, 4096) to Q8_0 with max error 0.002227783203125\n",
            "214/226 quantized (11008, 4096) to Q8_0 with max error 0.0012514609843492508\n",
            "215/226 quantized (11008, 4096) to Q8_0 with max error 0.0009160079061985016\n",
            "216/226 quantized (11008, 4096) to Q8_0 with max error 0.0010188538581132889\n",
            "217/226 quantized (11008, 4096) to Q8_0 with max error 0.0011504166759550571\n",
            "218/226 quantized (11008, 4096) to Q8_0 with max error 0.0007751947268843651\n",
            "219/226 quantized (11008, 4096) to Q8_0 with max error 0.0009794458746910095\n",
            "220/226 quantized (11008, 4096) to Q8_0 with max error 0.0014398526400327682\n",
            "221/226 quantized (11008, 4096) to Q8_0 with max error 0.0013543087989091873\n",
            "222/226 quantized (11008, 4096) to Q8_0 with max error 0.001435527577996254\n",
            "223/226 quantized (11008, 4096) to Q8_0 with max error 0.0020492374897003174\n",
            "224/226 quantized (11008, 4096) to Q8_0 with max error 0.00302964448928833\n",
            "225/226 quantized (11008, 4096) to Q8_0 with max error 0.002961406484246254\n",
            "226/226 quantized (32000, 4096) to Q8_0 with max error 0.0013672839850187302\n",
            "max quantization group error across all weights: 0.006980113685131073\n",
            "wrote llama2_7b_q80.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lhg *.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCTvSHqqVCD7",
        "outputId": "1b07194e-1a67-4b17-ca76-87c771763bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root  26G Dec 21 04:26 llama2_7b.bin\n",
            "-rw-r--r-- 1 root  26G Dec 21 06:51 llama2_7b_chat.bin\n",
            "-rw-r--r-- 1 root 6.7G Dec 21 07:42 llama2_7b_q80.bin\n",
            "-rw-r--r-- 1 root 419M Jul 26 23:48 stories110M.bin\n",
            "-rw-r--r-- 1 root  58M Jul 26 22:39 stories15M.bin\n",
            "-rw-r--r-- 1 root 424K Dec 21 03:25 tokenizer.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./runq llama2_7b_q80.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbl32CtbcR46",
        "outputId": "b192aa77-5a81-4d5c-c241-83075dc1d300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assisting David Warner in his 2005 retirement. Australian Cricket Team Image: AAP Image/Brendon Thorne\n",
            "Mike Coward\n",
            "David Warner has been talking about the death of Australian cricket in recent weeks, with his in-depth knowledge of cricket’s own NRL 9s, the global T20 circuit, and his global links through Warner Media.\n",
            "In the light of Cricket Australia’s financial woes, he is concerned that the necessary commitment to the reinvigoration of the Australian team’s performance at the next World Cup might be hindered by current matters – especially due to his involvement in Warner Media.\n",
            "A look at the structure of the league in the years since his retirement, including the importance of a significant contribution to the revival of Australian cricket, will be required to establish whether or not there is a specific justification for his recent remarks.\n",
            "Who are the managers of the Twenty20 World Cup?\n",
            "The ICC is in charge of this competition.\n",
            "Its Supervisory Board has the authority to approve the host nation’s final list of players after receiving a comprehensive list from the Contest’s Executive\n",
            "achieved tok/s: 1.852457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!OMP_NUM_THREADS=8 ./runq llama2_7b_q80.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5fqSNpjgFeB",
        "outputId": "320d1c94-42be-4822-f4d9-dd85c1a871df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hate the heat and humidity? Well here’s a heartfelt tale of New Orleans manhood and swamp-country hoo-hah!\n",
            "But you’re not gonna find any snakes or moccasins or gross bambi here! No siree, we have good old fashioned pork to the skinny ladies (and men!) by the dozen! Now ain’t that something to hear….?!?\n",
            "Damon Lewis is on a mission to find true love. Too bad he has to search around Louisiana for lovey-dovey bunnies to match his special needs. Can this average-Joe ever find someone?\n",
            "Now if he’s gay and if it’s time to introduce Damon to the Chicken Ranch, because every great Chicken Ranch has a slew of boy-tenderness by the bunch of bitches! And who wouldn’t want that kind of swamp-lovin’ at the Ranch?\n",
            "He’s already endured enough of his in-laws, but this is worse – an adult that can’t get the dumb-ass women to leave him alone – so he\n",
            "achieved tok/s: 1.749499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# export huggingface models\n",
        "\n",
        "可以加载任何使用 Llama 2 架构的 Huggingface 模型。请参阅脚本export.py和--hf导出模型 .bin 文件的标志。"
      ],
      "metadata": {
        "id": "wp1WU9B2d6xU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q einops transformers_stream_generator"
      ],
      "metadata": {
        "id": "fSLdMFPNkGGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/llama2.c\n",
        "# 51G memory OOM\n",
        "# !python export.py baichuan_7b.bin --hf baichuan-inc/Baichuan-7B\n",
        "\n",
        "# no rms_norm_eps, don't like LLama2 model struct\n",
        "#!python export.py qwen_7b.bin --hf Qwen/Qwen-7B\n",
        "\n",
        "# no num_hidden_layers, don't like LLama2 model struct\n",
        "#!python export.py chatglm3_6b.bin --hf THUDM/chatglm3-6b\n",
        "\n",
        "!python export.py chinese-llama2-1_3b.bin --hf hfl/chinese-llama-2-1.3b\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5iHwQ1ad_t0",
        "outputId": "4941bf55-cff9-46f3-f388-b4f09770e8e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json: 100% 671/671 [00:00<00:00, 2.68MB/s]\n",
            "pytorch_model.bin: 100% 2.53G/2.53G [00:57<00:00, 43.6MB/s]\n",
            "generation_config.json: 100% 170/170 [00:00<00:00, 666kB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "wrote chinese-llama2-1_3b.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lgh chinese-llama2-1_3b.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvvFoiPvvhw1",
        "outputId": "d1599078-8de5-4be5-ee67-d2917075adf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root 4.8G Dec 21 09:25 chinese-llama2-1_3b.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!(OMP_NUM_THREADS=8 ./run llama2_7b.bin -i \"你好\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyjhI_I7uuor",
        "outputId": "ce73e86f-e6b1-4d86-b83b-f2874cad6b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "你好!\n",
            "Konnichiwa.\n",
            "Follow\n",
            "Say Hello\n",
            "World\n",
            "Learn all about me.\n",
            "You will see I love to sing and act and dance.\n",
            "I do lots of other fun things.\n",
            "I have a dog called Chloe,\n",
            "and we live on a farm.\n",
            "But I don't think I live on a farm in China.\n",
            "I don't think I live in Japan,\n",
            "but maybe I do.\n",
            "Follow\n",
            "</s>\n",
            "\n",
            "achieved tok/s: 1.378480\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# llama2.c-zh"
      ],
      "metadata": {
        "id": "-9WwoRiB1hiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtuU241Yrofh",
        "outputId": "7aaa46a4-fc17-4c70-dee2-06400d77b74b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/chenyangMl/llama2.c-zh.git\n",
        "%cd llama2.c-zh"
      ],
      "metadata": {
        "id": "D6963FG21jR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!make runomp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cC6fvkD2KCl",
        "outputId": "2f03e221-0a01-43e5-9734-8d74d72fd075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gcc -Ofast -fopenmp -march=native run.c  -lm  -o run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/52AI/tinyllamas_zh/resolve/main/stories15M-llama2-enzh.bin\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykabQuYz2OOQ",
        "outputId": "51963bce-bc4d-41e1-e6da-0f9427edcac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-21 10:08:20--  https://huggingface.co/52AI/tinyllamas_zh/resolve/main/stories15M-llama2-enzh.bin\n",
            "Resolving huggingface.co (huggingface.co)... 99.84.108.70, 99.84.108.87, 99.84.108.129, ...\n",
            "Connecting to huggingface.co (huggingface.co)|99.84.108.70|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/7e/7a/7e7a7ed4c2609b07cca1da298493c37bacce3551bdd392271801941d3e3cbbba/dfbb2b20e1504b2ec944e7a8a95847c2906461863b8869d84e8eb78ca7bf1abf?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27stories15M-llama2-enzh.bin%3B+filename%3D%22stories15M-llama2-enzh.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1703412500&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzQxMjUwMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy83ZS83YS83ZTdhN2VkNGMyNjA5YjA3Y2NhMWRhMjk4NDkzYzM3YmFjY2UzNTUxYmRkMzkyMjcxODAxOTQxZDNlM2NiYmJhL2RmYmIyYjIwZTE1MDRiMmVjOTQ0ZTdhOGE5NTg0N2MyOTA2NDYxODYzYjg4NjlkODRlOGViNzhjYTdiZjFhYmY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=rD34qDou8kP-XY4YT%7EfXedsWDHMFKBgfJc%7E7zcVHonZcvAXXUbuWSBMBIKle%7EVKr9XhQV5boSNX9XFzV%7EPBUV90y6TFNrCaMC7MVUSpHeQMlCpoFAN3qJS6Ndilf8D8ccq6%7Eqnma4qTxeZ1ttdDDPxb1ZxYdRtKjyporx5X2jJHcQRf0CZzM%7E102luFAKqM0Lr0CNpBP1qLHx153lZgdNihs4JPWWk9sqBl-DS6U2Ib1%7EAEW6RBFVicKWv7NfG8Kt7G4esc0trHPBBFFDJEfyvm15SmsFPEZuWHqYLCSJr%7EZIVEbDiUPVPFeJb1BMFrX5D9xzbN-pj09HExkvPX%7E2Q__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-12-21 10:08:20--  https://cdn-lfs.huggingface.co/repos/7e/7a/7e7a7ed4c2609b07cca1da298493c37bacce3551bdd392271801941d3e3cbbba/dfbb2b20e1504b2ec944e7a8a95847c2906461863b8869d84e8eb78ca7bf1abf?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27stories15M-llama2-enzh.bin%3B+filename%3D%22stories15M-llama2-enzh.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1703412500&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzQxMjUwMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy83ZS83YS83ZTdhN2VkNGMyNjA5YjA3Y2NhMWRhMjk4NDkzYzM3YmFjY2UzNTUxYmRkMzkyMjcxODAxOTQxZDNlM2NiYmJhL2RmYmIyYjIwZTE1MDRiMmVjOTQ0ZTdhOGE5NTg0N2MyOTA2NDYxODYzYjg4NjlkODRlOGViNzhjYTdiZjFhYmY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=rD34qDou8kP-XY4YT%7EfXedsWDHMFKBgfJc%7E7zcVHonZcvAXXUbuWSBMBIKle%7EVKr9XhQV5boSNX9XFzV%7EPBUV90y6TFNrCaMC7MVUSpHeQMlCpoFAN3qJS6Ndilf8D8ccq6%7Eqnma4qTxeZ1ttdDDPxb1ZxYdRtKjyporx5X2jJHcQRf0CZzM%7E102luFAKqM0Lr0CNpBP1qLHx153lZgdNihs4JPWWk9sqBl-DS6U2Ib1%7EAEW6RBFVicKWv7NfG8Kt7G4esc0trHPBBFFDJEfyvm15SmsFPEZuWHqYLCSJr%7EZIVEbDiUPVPFeJb1BMFrX5D9xzbN-pj09HExkvPX%7E2Q__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.160.41.65, 18.160.41.57, 18.160.41.39, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.160.41.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87653020 (84M) [application/octet-stream]\n",
            "Saving to: ‘stories15M-llama2-enzh.bin’\n",
            "\n",
            "stories15M-llama2-e 100%[===================>]  83.59M  52.3MB/s    in 1.6s    \n",
            "\n",
            "2023-12-21 10:08:22 (52.3 MB/s) - ‘stories15M-llama2-enzh.bin’ saved [87653020/87653020]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!OMP_NUM_THREADS=1 ./run stories15M-llama2-enzh.bin -k tokenizers/llama2enzh/tokenizer.bin -i \"从前，有一个小女孩\" -t 0.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSxF8LA72iis",
        "outputId": "6db5ea10-59b9-4ddd-e7ba-535f5658e35b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "从前，有一个小女孩，名叫莉莉。有一天，莉莉的妈妈给她做了一份加了糖留的美味佳肴。但正当莉莉准备把它们全部吃掉之后，她发现味道很好。\n",
            "莉莉的妈妈很生气并告诉莉莉，她晚餐必须吃一片豆子。莉莉不喜欢豆子的味道，她开始哭泣。\n",
            "莉莉小姐知道搅拌豆子的一股以后，她希望晚饭清楚地会发生。最后，莉莉的妈妈给了她一篮子豆子，并告诉她明天来吃。莉莉又高兴了，她知道煮豆子是她自己做的，真相也很重要。\n",
            "achieved tok/s: 96.913137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./run stories15M-llama2-enzh.bin -k tokenizers/llama2enzh/tokenizer.bin -i \"从前，有一个小女孩\" -t 0.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7YguYQ72P_h",
        "outputId": "9bad8528-8161-4103-8f6e-7a88c1c5c670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "从前，有一个小女孩，名叫莉莉。她喜欢用紫色蜡笔画画和搅拌。有一天，莉莉的妈妈让她带着钱包去公园。莉莉很高兴能帮忙，但又不知道路上的一切。\n",
            "在公园里，莉莉遇到了一个名叫麦克斯的男孩。马克斯是无害的，但莉莉有点害怕。莉莉告诉他不要害怕，因为这会伤害他们。麦克斯坚持说，如果她趴着，他会很温柔。\n",
            "莉莉考虑了马克斯所说的话，决定勇敢地鼓励他。麦克斯一开始很害怕，但莉莉鼓励他这么做。她带领麦克斯来到公园并与他和他的妹妹一起玩耍。\n",
            "在公园玩完后，莉莉和麦克斯去买一些冰淇淋。麦克斯也很高兴，因为他让孩子们高兴了，他们在公园里玩得很开心。这个故事的寓意是，即使事情很可怕，我们也可以鼓励别人并向他们表明他们对其他人的健康做好准备。\n",
            "achieved tok/s: 355.123675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/52AI/tinyllamas_zh/resolve/main/stories15M-baichuan.bin\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e98ULJOx3Uu6",
        "outputId": "03c46f9d-96d9-4106-ed6b-44b4842aae71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-21 10:13:12--  https://huggingface.co/52AI/tinyllamas_zh/resolve/main/stories15M-baichuan.bin\n",
            "Resolving huggingface.co (huggingface.co)... 18.238.49.70, 18.238.49.117, 18.238.49.112, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.238.49.70|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/7e/7a/7e7a7ed4c2609b07cca1da298493c37bacce3551bdd392271801941d3e3cbbba/5e397f896e34d4760aaddf8ad5443f9fbcfafeb48c3dfaa53bc2f286d6335034?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27stories15M-baichuan.bin%3B+filename%3D%22stories15M-baichuan.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1703412793&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzQxMjc5M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy83ZS83YS83ZTdhN2VkNGMyNjA5YjA3Y2NhMWRhMjk4NDkzYzM3YmFjY2UzNTUxYmRkMzkyMjcxODAxOTQxZDNlM2NiYmJhLzVlMzk3Zjg5NmUzNGQ0NzYwYWFkZGY4YWQ1NDQzZjlmYmNmYWZlYjQ4YzNkZmFhNTNiYzJmMjg2ZDYzMzUwMzQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=XZZo86bfqqOSMmVOXE52w9tFu5pxHAJqbuIiH-%7Ekg3BzVvhGpZ4SH30SwNQw6oCehUCx4Gq6-c1FTsvMl2h8phADn1ASWDOoABRAH%7Ec1DqEEcpqFFY88P6N16c4XXPZ9tct%7EdL6CKsCc-YG%7Enaurj7kl-VB8a-E-7ApF9LhsYZz7X4m7cmE8b01EDludmhH1m%7E2NgcBqavd9pCwBU6-ZdcnMpCniN6Pq3I0T5a6jCP52COq4ahlriChQlwWm3OJSqoW8L-B2xWWUaiwYvO7RSYlkmq3wYoFkuajz5i4gpwuiSQG15PHPAi8oQXaSyd4tt4XnPn-8FinxONA8Fm8mVw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-12-21 10:13:13--  https://cdn-lfs.huggingface.co/repos/7e/7a/7e7a7ed4c2609b07cca1da298493c37bacce3551bdd392271801941d3e3cbbba/5e397f896e34d4760aaddf8ad5443f9fbcfafeb48c3dfaa53bc2f286d6335034?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27stories15M-baichuan.bin%3B+filename%3D%22stories15M-baichuan.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1703412793&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzQxMjc5M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy83ZS83YS83ZTdhN2VkNGMyNjA5YjA3Y2NhMWRhMjk4NDkzYzM3YmFjY2UzNTUxYmRkMzkyMjcxODAxOTQxZDNlM2NiYmJhLzVlMzk3Zjg5NmUzNGQ0NzYwYWFkZGY4YWQ1NDQzZjlmYmNmYWZlYjQ4YzNkZmFhNTNiYzJmMjg2ZDYzMzUwMzQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=XZZo86bfqqOSMmVOXE52w9tFu5pxHAJqbuIiH-%7Ekg3BzVvhGpZ4SH30SwNQw6oCehUCx4Gq6-c1FTsvMl2h8phADn1ASWDOoABRAH%7Ec1DqEEcpqFFY88P6N16c4XXPZ9tct%7EdL6CKsCc-YG%7Enaurj7kl-VB8a-E-7ApF9LhsYZz7X4m7cmE8b01EDludmhH1m%7E2NgcBqavd9pCwBU6-ZdcnMpCniN6Pq3I0T5a6jCP52COq4ahlriChQlwWm3OJSqoW8L-B2xWWUaiwYvO7RSYlkmq3wYoFkuajz5i4gpwuiSQG15PHPAi8oQXaSyd4tt4XnPn-8FinxONA8Fm8mVw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.160.41.39, 18.160.41.65, 18.160.41.57, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.160.41.39|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 97680028 (93M) [application/octet-stream]\n",
            "Saving to: ‘stories15M-baichuan.bin’\n",
            "\n",
            "stories15M-baichuan 100%[===================>]  93.15M  42.3MB/s    in 2.2s    \n",
            "\n",
            "2023-12-21 10:13:15 (42.3 MB/s) - ‘stories15M-baichuan.bin’ saved [97680028/97680028]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./run stories15M-baichuan.bin -k tokenizers/baichuan/tokenizer.bin -i \"One day, Lily met a Shoggoth\" -t 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4ZFfFRr3YF2",
        "outputId": "b8a66e34-ea6c-4c88-ab6a-742a78c27310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One day, Lily met a Shoggoth.bbbs explained that if she resolved this lesson times, she was feeling extra confident and brave. He felt envious of reason, but stayed strong and determined. \n",
            "The next day寿命 had learned fromopard Will, and with theerts they got to have a singing shooting blood. Gr wished anyone would dance too.\n",
            "The moral of the story is that you can still be brave, no matter how綊 twiately, it will never come up. Sometimes, long andising can still be a great result.\n",
            "achieved tok/s: 342.342342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentencepiece import SentencePieceProcessor\n",
        "sp_model_llama2 = SentencePieceProcessor(model_file=\"tokenizers/llama2en/tokenizer.model\")\n",
        "sp_model_llamaenzh = SentencePieceProcessor(model_file=\"tokenizers/llama2enzh/tokenizer.model\")\n",
        "sp_model_baichaun = SentencePieceProcessor(model_file=\"tokenizers/baichuan/tokenizer.model\")\n",
        "s = \"从前，有一个小女孩，名叫莉莉。她喜欢在外面玩耍，欣赏美丽的花朵。\"\n",
        "\n",
        "llama2_en = sp_model_llama2.encode(s)\n",
        "baichuan_enzh = sp_model_baichaun.encode(s)\n",
        "llama2_enzh = sp_model_llamaenzh.encode(s)\n",
        "\n",
        "print(f\"llama2_en={len(llama2_en)}, llama2_enzh={len(llama2_enzh)} baichuan-enzh={len(baichuan_enzh)}\")\n",
        "print(f\"llama2_en={sp_model_llama2.vocab_size()}, llama2_enzh={sp_model_llamaenzh.vocab_size()} \\\n",
        "      baichuan-enzh={sp_model_baichaun.vocab_size()}\")\n",
        "print(\"llama2_en\",llama2_en)\n",
        "print(\"llama2_enzh\",llama2_enzh)\n",
        "print(f\"编码一个中文字符消耗tokens, llam2-en={len(llama2_en)/len(s):.2}, llama2_enzh={len(llama2_enzh)/len(s):.2}\")\n",
        "#-----------------------------------\n",
        "#llama2_en=57, llama2_enzh=19 baichuan-enzh=23\n",
        "#llama2_en=32000, llama2_enzh=55296       baichuan-enzh=64000\n",
        "#llama2_en [29871, 31594, 30658, 30214, 30417, 30287, 30502, 30446, 30647, 232, 176, 172, 30214, 30548, 232, 146, 174, 235, 145, 140, 235, 145, 140, 30267, 232, 168, 188, 31823, 233, 175, 165, 30505, 31066, 30806, 234, 145, 172, 235, 131, 144, 30214, 233, 175, 166, 235, 184, 146, 30630, 231, 187, 192, 30210, 30830, 233, 159, 184, 30267]\n",
        "#llama2_enzh [29871, 40870, 30214, 32952, 41677, 30214, 40148, 34595, 34595, 30267, 32008, 32123, 40729, 42754, 30214, 35186, 37973, 46892, 30267]\n",
        "#编码一个中文字符消耗tokens, llam2-en=1.8, llama2_enzh=0.59"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrxfIF-V3y8K",
        "outputId": "289da9de-fa6d-4efa-c2c2-f4aa5b3bf86f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llama2_en=57, llama2_enzh=19 baichuan-enzh=23\n",
            "llama2_en=32000, llama2_enzh=55296       baichuan-enzh=64000\n",
            "llama2_en [29871, 31594, 30658, 30214, 30417, 30287, 30502, 30446, 30647, 232, 176, 172, 30214, 30548, 232, 146, 174, 235, 145, 140, 235, 145, 140, 30267, 232, 168, 188, 31823, 233, 175, 165, 30505, 31066, 30806, 234, 145, 172, 235, 131, 144, 30214, 233, 175, 166, 235, 184, 146, 30630, 231, 187, 192, 30210, 30830, 233, 159, 184, 30267]\n",
            "llama2_enzh [29871, 40870, 30214, 32952, 41677, 30214, 40148, 34595, 34595, 30267, 32008, 32123, 40729, 42754, 30214, 35186, 37973, 46892, 30267]\n",
            "编码一个中文字符消耗tokens, llam2-en=1.8, llama2_enzh=0.59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use baichuan tokenizer from https://huggingface.co/baichuan-inc/Baichuan-7B/blob/main/tokenizer.model;\n",
        "# then use tokenizer.py export to *.bin\n",
        "# run inference with tokenizer\n",
        "!./run stories15M-baichuan.bin -k tokenizers/baichuan/tokenizer.bin\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adaPYZXqYVXT",
        "outputId": "53c625cc-c274-4c71-ff75-3728e08ec940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "从前，有一支笨手笨脚的蜡烛。它很绿，很酷。那人一份喜欢玩火车的报纸。他们玩得很开心。\n",
            "有一天，一只小鸟看见那人，知道这块蜡烛是绿色的。小鸟想用自己的蜡烛让蜡烛提供家园。于是，这只鸟把它放在自己的蜡烛下面。它添加了一些树枝和树叶。\n",
            "朋友们在热气的蜡烛中嬉笑玩耍。他们都在一起玩。外面，那个人给了它一个大大的拥抱，并在他们周围玩耍。从那天起，推着大蜡烛，并弄得很更多。\n",
            "achieved tok/s: 253.258845\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use chatglm tokenizer from https://huggingface.co/THUDM/chatglm3-6b/blob/main/tokenizer.model\n",
        "!wget https://huggingface.co/THUDM/chatglm3-6b/resolve/main/tokenizer.model -O tokenizers/chatglm3-tokenizer.model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQrIWwPGYpRe",
        "outputId": "3008a480-7720-41bc-a3a5-560115d8d660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-21 14:12:26--  https://huggingface.co/THUDM/chatglm3-6b/resolve/main/tokenizer.model\n",
            "Resolving huggingface.co (huggingface.co)... 99.84.108.87, 99.84.108.55, 99.84.108.129, ...\n",
            "Connecting to huggingface.co (huggingface.co)|99.84.108.87|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.huggingface.co/repos/01/5a/015a4eda6314fbcdb0aecc00e73f604e0f398b10ef843cf3e7bfcf8d0b9c1d7d/e7dc4c393423b76e4373e5157ddc34803a0189ba96b21ddbb40269d31468a6f2?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27tokenizer.model%3B+filename%3D%22tokenizer.model%22%3B&Expires=1703427146&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzQyNzE0Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzAxLzVhLzAxNWE0ZWRhNjMxNGZiY2RiMGFlY2MwMGU3M2Y2MDRlMGYzOThiMTBlZjg0M2NmM2U3YmZjZjhkMGI5YzFkN2QvZTdkYzRjMzkzNDIzYjc2ZTQzNzNlNTE1N2RkYzM0ODAzYTAxODliYTk2YjIxZGRiYjQwMjY5ZDMxNDY4YTZmMj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=TfiSrfHIYnOvf8nLBBuvzco6MGB7Sb-CkssDqfqmgcbbR836BKpCJpJxMULzChr%7EOoAbFsUesDWgeZHNSDL4wdghbwY88hzkmxIlPINBS2E-3WsdQf4Lw1jjRFBF9vym0BPWPZF7aho-WiVifqQy%7E%7EKeMLcfylDAAsfstTxt50PRKKkhTgzlTeEnzhHUBaVrCkG42TfklAQZBzlu4dChkGRiwnvBtjsGeE58rbhfYLs1OVang0Z5GlpqLh%7Eu-av5mA3X4jwIbPBPcL47PbXIm8LPxhfU8LkWfG72Ns3AcsgZTGImN5L-3uV3Mhvrj4rpypt%7E2CQ4bqh2MmTs6ONwlQ__&Key-Pair-Id=KCD77M1F0VK2B [following]\n",
            "--2023-12-21 14:12:26--  https://cdn-lfs-us-1.huggingface.co/repos/01/5a/015a4eda6314fbcdb0aecc00e73f604e0f398b10ef843cf3e7bfcf8d0b9c1d7d/e7dc4c393423b76e4373e5157ddc34803a0189ba96b21ddbb40269d31468a6f2?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27tokenizer.model%3B+filename%3D%22tokenizer.model%22%3B&Expires=1703427146&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzQyNzE0Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzAxLzVhLzAxNWE0ZWRhNjMxNGZiY2RiMGFlY2MwMGU3M2Y2MDRlMGYzOThiMTBlZjg0M2NmM2U3YmZjZjhkMGI5YzFkN2QvZTdkYzRjMzkzNDIzYjc2ZTQzNzNlNTE1N2RkYzM0ODAzYTAxODliYTk2YjIxZGRiYjQwMjY5ZDMxNDY4YTZmMj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=TfiSrfHIYnOvf8nLBBuvzco6MGB7Sb-CkssDqfqmgcbbR836BKpCJpJxMULzChr%7EOoAbFsUesDWgeZHNSDL4wdghbwY88hzkmxIlPINBS2E-3WsdQf4Lw1jjRFBF9vym0BPWPZF7aho-WiVifqQy%7E%7EKeMLcfylDAAsfstTxt50PRKKkhTgzlTeEnzhHUBaVrCkG42TfklAQZBzlu4dChkGRiwnvBtjsGeE58rbhfYLs1OVang0Z5GlpqLh%7Eu-av5mA3X4jwIbPBPcL47PbXIm8LPxhfU8LkWfG72Ns3AcsgZTGImN5L-3uV3Mhvrj4rpypt%7E2CQ4bqh2MmTs6ONwlQ__&Key-Pair-Id=KCD77M1F0VK2B\n",
            "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 18.238.49.21, 18.238.49.76, 18.238.49.97, ...\n",
            "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|18.238.49.21|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1018370 (995K) [application/octet-stream]\n",
            "Saving to: ‘tokenizers/chatglm3-tokenizer.model’\n",
            "\n",
            "\r          tokenizer   0%[                    ]       0  --.-KB/s               \rtokenizers/chatglm3 100%[===================>] 994.50K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-12-21 14:12:26 (26.5 MB/s) - ‘tokenizers/chatglm3-tokenizer.model’ saved [1018370/1018370]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat config.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYcnV5lbm3WK",
        "outputId": "a1f1f79f-2b62-40a7-f13f-3ae6dcb955b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\"\"\"\n",
            "tokenizer 和 训练数据配置文件\n",
            "\"\"\"\n",
            "LANGUAGE = \"enzh\"  # [en, zh, enzh]\n",
            "\n",
            "# https://huggingface.co/baichuan-inc/Baichuan-7B/blob/main/tokenizer.model\n",
            "#TOKENIZER_MODEL = \"tokenizers/baichuan/tokenizer.model\" # the baichuan sentencepiece tokenizer model\n",
            "#TOKENIZER_BIN = \"tokenizers/baichuan/tokenizer.bin\" # binary version of the tokenizer for inference in C\n",
            "\n",
            "# https://huggingface.co/ziqingyang/chinese-llama-2-7b/blob/main/tokenizer.model\n",
            "# TOKENIZER_MODEL = \"tokenizers/llama2enzh/tokenizer.model\" # the llama2.\n",
            "# TOKENIZER_BIN = \"tokenizers/llama2enzh/tokenizer.bin\" # binary version of the tokenizer for inference in C\n",
            "\n",
            "# base llama2，\n",
            "# TOKENIZER_MODEL = \"tokenizers/llama2en/tokenizer.model\" # the llama2-enzh.\n",
            "# TOKENIZER_BIN = \"tokenizers/llama2en/tokenizer.bin\" # binary version of the tokenizer for inference in C\n",
            "\n",
            "#自定义中文词表(红楼梦.txt)\n",
            "# TOKENIZER_MODEL = \"tokenizers/custom_tokenizer/meng.model\" # the llama2-zh.\n",
            "# TOKENIZER_BIN = \"tokenizers/custom_tokenizer/meng.bin\" # binary version of the tokenizer for inference in C\n",
            "\n",
            "# https://huggingface.co/THUDM/chatglm3-6b/blob/main/tokenizer.model\n",
            "TOKENIZER_MODEL = \"tokenizers/chatglm3-tokenizer.model\"\n",
            "TOKENIZER_BIN = \"tokenizers/chatglm3-tokenizer.bin\" # binary version of the tokenizer for inference in C"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/llama2.c-zh\n",
        "!python tokenizer.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9WkWI28n8VX",
        "outputId": "3ef83d5a-63c1-48ae-eb8b-fd780690b590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama2.c-zh\n",
            "Loaded SentencePiece model from tokenizers/chatglm3-tokenizer.model\n",
            "#words: 64789 - BOS ID: 1 - EOS ID: 2\n",
            "TOKENIZER_BIN is saved = tokenizers/chatglm3-tokenizer.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ha~ so bad, need train chatglm3 model with chatglm3-tokenizer\n",
        "!./run stories15M-llama2-enzh.bin -k tokenizers/chatglm3-tokenizer.bin -i \"从前，有一个小女孩\" -t 0.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdnHStz9uVh3",
        "outputId": "23ac5381-500a-4672-b38b-e81223663bf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "从前，有一个小女孩 Ved waterfront心上是一个总会 Gad一個網站主持风险 waterfrontς联合土豆 Gad疫苗接种联合亲人 waterfrontς日期问题 CUEdge好奇 Gad心上 Gad三大 Rash盛世读了老人Tagged较长國外 waterfrontiku心上体力 Gad幸福這麼 waterfrontς隐患 Asc演讲 undes并对工作会议途径经历了 waterfront\n",
            "achieved tok/s: 302.083333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown pre-train/fine-tunning Children's books datasets with chatglm3 or llama2 model struct\n",
        "\n",
        "# see train notebook"
      ],
      "metadata": {
        "id": "FIZ7Sgyv1ZBB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "157cdeb2fe4449de966ff9f66678368d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a1c137915ce407c9664aaa9d09783a1",
              "IPY_MODEL_90a1f3a5f34e4999a9d93566bb235144",
              "IPY_MODEL_89dfc79597204cb4b3f2796581125682"
            ],
            "layout": "IPY_MODEL_41318164b0be46b481672fc5f13c8cda"
          }
        },
        "8a1c137915ce407c9664aaa9d09783a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d08b07a280f413baff17581fa4f3e7e",
            "placeholder": "​",
            "style": "IPY_MODEL_b1aed9178ab04ecdbfb5c7b1a6235f11",
            "value": "Fetching 10 files: 100%"
          }
        },
        "90a1f3a5f34e4999a9d93566bb235144": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e7728e39a1b4f72a189487478640fce",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96138946da7244179dcd5b8a3275e557",
            "value": 10
          }
        },
        "89dfc79597204cb4b3f2796581125682": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dede50bce2fa4c39bde7a75d85e546f2",
            "placeholder": "​",
            "style": "IPY_MODEL_1164c52222964166b26224778db0cb2b",
            "value": " 10/10 [00:00&lt;00:00, 630.82it/s]"
          }
        },
        "41318164b0be46b481672fc5f13c8cda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d08b07a280f413baff17581fa4f3e7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1aed9178ab04ecdbfb5c7b1a6235f11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e7728e39a1b4f72a189487478640fce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96138946da7244179dcd5b8a3275e557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dede50bce2fa4c39bde7a75d85e546f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1164c52222964166b26224778db0cb2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}