{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weedge/doraemon-nb/blob/main/nano_rl_r1_zero_grpo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbcLlibCL__k"
      },
      "source": [
        "这个项目，名为 **nanoAhaMoment: 单文件“LLM 强化学习”库**，致力于提供一个极致精简、高效且完全透明的大型语言模型强化学习训练实现。\n",
        "\n",
        "---\n",
        "\n",
        "### 项目亮点：\n",
        "\n",
        "* **单 GPU 运行**：无需昂贵的硬件，在单个 GPU 上即可高效训练。（3B使用的A100-80GB, 这里使用A100-40GB/L4来训练1.5B/0.5B模型）\n",
        "* **零外部 RL 库依赖**：告别 `TRL` 或 `Verl` 等复杂库，所有代码都是手写，确保你对每一个细节都了如指掌。\n",
        "* **极致效率**：在简化代码的同时，依然保持训练的高效率。\n",
        "* **支持 3B 基础模型**：适用于参数量适中的基础大型语言模型。（colab中使用0.5B作为测试训练调试）\n",
        "* **R1-Zero 训练的全参数微调实现**：直接实现 R1-Zero 的训练范式，且支持模型全参数微调。\n",
        "\n",
        "---\n",
        "\n",
        "### 设计理念：\n",
        "\n",
        "**nanoAhaMoment** 的灵感来源于 [TinyZero](https://github.com/Jiayi-Pan/TinyZero) 和 [Mini-R1](https://www.philschmid.de/mini-deepseek-r1) 这类项目，但它更注重于：\n",
        "\n",
        "* **更简单**：代码结构直观，易于理解。\n",
        "* **更清晰**：消除不必要的抽象，让每一行代码都清晰可见。\n",
        "* **更快**：在代码精简的同时，不牺牲运行效率。\n",
        "\n",
        "这个项目的核心目标是为大型语言模型的强化学习训练提供一个**透明、可控且易于修改**的基础，让你可以真正理解并掌控整个训练过程。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79wcH4qxL__n"
      },
      "source": [
        "以下是对您提供英文内容的中文翻译：\n",
        "\n",
        "R1-Zero 可以说是 DeepSeek R1 论文中更有趣的贡献。其核心思想是：取一个刚刚预训练好的大型语言模型（直接从无监督预训练的“烤箱”中取出），并使用强化学习继续对其进行训练，*无需*任何人类反馈或监督。结果呢？模型开始展现出涌现行为，例如自我反思、验证、回溯，这些行为是研究人员至少从 O1 开始就试图通过手工技巧和归纳偏置注入到大型语言模型中的。\n",
        "\n",
        "在本 notebook 中，我们将**从头开始**构建一个 R1-Zero 风格的训练循环。目标是为 RL 风格的大型语言模型训练创建一个清晰、可修改的基础；一个让您对每个运动部件以及它们如何协同工作一目了然的基础。非常适合进行尝试、扩展或修改。\n",
        "\n",
        "---\n",
        "\n",
        "### 为什么是另一个 R1-Zero 实现？\n",
        "\n",
        "已经有很棒的实现，例如 [TinyZero](https://github.com/Jiayi-Pan/TinyZero) 和 [Mini-R1](https://www.philschmid.de/mini-deepseek-r1)。但它们依赖于成熟的 RL 库（如 `trl` 或 `verl`）来处理训练。\n",
        "\n",
        "这些库的存在是有充分理由的；大型语言模型的高效 RL 训练处于可扩展训练和快速推理的十字路口。要实现这一点需要大量的工程。但这S也意味着内部结构通常被抽象化，难以阅读，甚至更难调整。\n",
        "\n",
        "这个 notebook 则不同：**没有抽象，没有隐藏**。您将看到一切，从上到下。一个轻量级、可读的代码库，同时仍遵循最佳实践并在单个 GPU 上高效运行。\n",
        "\n",
        "### 这个 notebook 到底是什么？\n",
        "\n",
        "我们将使用 RL 训练一个基础大型语言模型来解决一个推理密集的算法任务。设置如下：\n",
        "\n",
        "- **模型**：Qwen2.5 3B-Base、Qwen2.5 1.5B-Base、Qwen2.5 0.5B-Base，1.5B和0.5B模型主要用于测试\n",
        "- **数据集**：Countdown-Tasks-3to4\n",
        "- **算法**：GRPO（策略梯度的一种变体）\n",
        "\n",
        "是的，这个任务有点像玩具——但它抓住了 R1-Zero 的精髓：自我反思、验证、回溯，甚至语言切换等涌现行为。这种设置非常适合快速原型设计和实验。\n",
        "\n",
        "### 这个 notebook 适合谁？\n",
        "\n",
        "- 任何对大型语言模型 RL 训练感兴趣的人\n",
        "- 研究人员，尤其是学术界探索语言模型推理的研究人员\n",
        "\n",
        "### 在开始之前我应该了解什么？\n",
        "\n",
        "- 熟悉 HuggingFace Transformers 库\n",
        "- 具有微调大型语言模型的经验\n",
        "- 熟悉策略梯度方法（有帮助但非必需）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWY5cezxL__o"
      },
      "source": [
        "## R1-Zero Recipe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wafNXBnHL__p"
      },
      "source": [
        "这个项目的核心目标是训练一个基础大型语言模型（LLM），使其能够进行**推理**，并自主地**重新评估**和**改进**其输出，而这一切都无需人工监督。我们将在此 notebook 中实现 DeepSeek R1 论文中提出的一种出奇简单的训练方法。\n",
        "\n",
        "---\n",
        "\n",
        "## 训练方法\n",
        "\n",
        "以下是该方法的概要步骤：\n",
        "\n",
        "1.  **初始化**：首先，准备一个基础 LLM 和一个数据集。该数据集只包含问题提示（prompts）及其**最终答案**，不包含任何中间推理步骤。\n",
        "2.  **迭代训练**：对于从 $i = 0$ 到 `NUM_ITERATIONS` 的每个迭代周期：\n",
        "    * **采样提示**：从数据集中随机抽取一批 $N$ 个提示，记作 $\\{x_i\\}_{i=1}^N$。\n",
        "    * **生成响应**：对于每个提示 $x_i$，模型会生成 $G$ 个不同的响应：\n",
        "        $$y_1, y_2, \\cdots, y_G \\sim \\pi_\\theta(y|x)$$\n",
        "        这 $G$ 个响应在 GRPO 算法中被称为一个“组”（group）。\n",
        "    * **计算奖励与优势**：为每个生成的响应计算一个奖励 $R_i$，并对这些奖励进行归一化，以计算每个组内的 **GRPO 优势**。\n",
        "    * **构建训练样本**：创建包含 $N \\times G$ 个“回合”（episodes）的列表。每个回合是一个 $(x_i, y_i)$ 对，并附带其对应的优势值。\n",
        "    * **估计策略梯度**：利用这些回合数据来估计**策略梯度** $\\vec{g}_{pg}$。\n",
        "    * **更新模型参数**：根据估计出的策略梯度更新模型参数：\n",
        "        $$\\theta \\leftarrow \\theta + \\eta \\vec{g}_{pg}$$\n",
        "\n",
        "---\n",
        "\n",
        "## 代码结构概览\n",
        "\n",
        "您将看到的代码结构严格遵循上述训练方法，主要由三个核心组件构成：\n",
        "\n",
        "1.  **回合生成（Episode Generation）**：\n",
        "    * 负责在每个强化学习迭代中生成 $(x, y)$ 对及其对应的优势值。\n",
        "\n",
        "2.  **奖励计算（Reward Calculation）**：\n",
        "    * 用于计算每个生成响应的奖励。\n",
        "\n",
        "3.  **策略梯度估计（Policy Gradient Estimation）**：\n",
        "    * 利用生成的回合数据来估计策略梯度并执行模型更新。\n",
        "\n",
        "最终，这三个组件将协同工作，形成一个简单的循环，逐步训练模型，使其通过强化学习发展出强大的推理能力。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLJPm4YvL__p"
      },
      "source": [
        "## Checkpoint Playground"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw8HndgWL__q"
      },
      "source": [
        "在 `notebooks/checkpoint_playground.ipynb` 文件中，您可以加载我们已经用这个 notebook 训练好的模型，并以交互方式测试模型的推理能力。这个 notebook 允许您输入自定义提示（prompts）并观察模型的响应。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# install"
      ],
      "metadata": {
        "id": "73GsAnmV_APQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "安装完，需要重启会话"
      ],
      "metadata": {
        "id": "cn2_sjlfYRCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q vllm==0.7.3 deepspeed==0.16.4 datasets==3.3.2 accelerate==1.4.0\n"
      ],
      "metadata": {
        "id": "hdOgr4td_66c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q flash-attn --no-build-isolation\n"
      ],
      "metadata": {
        "id": "HK6-4LVK_EDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep -E \"torch|transformers|datasets|deepspeed|vllm|wandb|numba|flash|accelerate|numpy\""
      ],
      "metadata": {
        "id": "2AzvjpWn_qVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# run"
      ],
      "metadata": {
        "id": "S1N47ot2_Cs3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FnbozfiL__r"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Set the environment variables for HuggingFace\n",
        "# This is done to ensure that the cache directory for HuggingFace is set to a specific location,\n",
        "# preventing the storage from being overwhelmed with model files and other data.\n",
        "SCRATCH =  \"/content/scratch\"\n",
        "os.environ[\"HF_HOME\"] = f\"{SCRATCH}/hf_home\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import socket\n",
        "from pathlib import Path\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import wandb\n",
        "from datasets import Dataset\n",
        "from deepspeed import DeepSpeedEngine\n",
        "from transformers import AutoTokenizer, PreTrainedModel\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "DEFAULT_SYSTEM_MESSAGE = \"You are a helpful assistant. You first think about the reasoning process in the mind and then provide the user with the answer.\"\n",
        "DEFAULT_PROMPT_TEMPLATE = \"Using the numbers {numbers}, create an equation that equals {target}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final equation and answer in <answer> </answer> tags, for example <answer>(1 + 2) / (3 * 5)</answer>.\"\n",
        "\n",
        "\n",
        "def create_prompt(\n",
        "    numbers: List[int],\n",
        "    target: int,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    system_message: str = DEFAULT_SYSTEM_MESSAGE,\n",
        "    prompt_template: str = DEFAULT_PROMPT_TEMPLATE,\n",
        ") -> str:\n",
        "    prefix = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt_template.format(numbers=numbers, target=target),\n",
        "        },\n",
        "        {\"role\": \"assistant\", \"content\": \"Let me solve this step by step.\\n<think>\"},\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(prefix, tokenize=False, continue_final_message=True)\n",
        "\n",
        "\n",
        "def prepare_model_inputs(\n",
        "    query_token_ids: List[List[int]],\n",
        "    response_token_ids: List[List[int]],\n",
        "    advantages: List[List[float]],\n",
        "    device: torch.device,\n",
        ") -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Prepare padded model inputs with attention masks, labels, and advantages.\n",
        "    Args:\n",
        "        query_token_ids: List of query token ids\n",
        "        response_token_ids: List of response token ids\n",
        "        advantages: List of lists of advantage values, matching response_token_ids structure\n",
        "        device: Device to move the tensors to\n",
        "    Returns:\n",
        "        Dict with input_ids, attention_mask, labels, and advantages\n",
        "\n",
        "    Example:\n",
        "        >>> query_token_ids = [[1, 2, 3], [4, 5]]\n",
        "        >>> response_token_ids = [[6, 7], [8]]\n",
        "        >>> advantages = [[0.5, 0.8], [0.3]]\n",
        "        >>> outputs = prepare_model_inputs(query_token_ids, response_token_ids, advantages, \"cuda\")\n",
        "        >>> outputs\n",
        "        {\n",
        "            'input_ids': tensor([\n",
        "                [1, 2, 3, 6, 7],\n",
        "                [4, 5, 8, 0, 0]\n",
        "            ]),\n",
        "            'attention_mask': tensor([\n",
        "                [1, 1, 1, 1, 1],\n",
        "                [1, 1, 1, 0, 0]\n",
        "            ]),\n",
        "            'labels': tensor([\n",
        "                [-100, -100, -100, 6, 7],\n",
        "                [-100, -100, 8, -100, -100]\n",
        "            ]),\n",
        "            'advantages': tensor([\n",
        "                [0.0, 0.0, 0.0, 0.5, 0.5],\n",
        "                [0.0, 0.0, 0.0, 0.9, 0.0]\n",
        "            ])\n",
        "        }\n",
        "    \"\"\"\n",
        "    max_seq_len = max(len(q) + len(r) for q, r in zip(query_token_ids, response_token_ids))\n",
        "    inputs = {\"input_ids\": [], \"attention_mask\": [], \"labels\": [], \"advantages\": []}\n",
        "\n",
        "    pad_token_id = 0  # Doesn't matter, will be masked\n",
        "    ignore_index = -100\n",
        "\n",
        "    for query, response, advantage in zip(query_token_ids, response_token_ids, advantages):\n",
        "        combined_ids = query + response\n",
        "        seq_len = len(combined_ids)\n",
        "\n",
        "        # Create padded sequences\n",
        "        input_ids = combined_ids + [pad_token_id] * (max_seq_len - seq_len)\n",
        "        attention_mask = [1] * seq_len + [0] * (max_seq_len - seq_len)\n",
        "        labels = [ignore_index] * len(query) + response + [ignore_index] * (max_seq_len - seq_len)\n",
        "        advantages_seq = [0.0] * len(query) + advantage + [0.0] * (max_seq_len - seq_len)\n",
        "\n",
        "        assert len(input_ids) == max_seq_len\n",
        "        assert len(attention_mask) == max_seq_len\n",
        "        assert len(labels) == max_seq_len\n",
        "        assert len(advantages_seq) == max_seq_len\n",
        "\n",
        "        inputs[\"input_ids\"].append(input_ids)\n",
        "        inputs[\"attention_mask\"].append(attention_mask)\n",
        "        inputs[\"labels\"].append(labels)\n",
        "        inputs[\"advantages\"].append(advantages_seq)\n",
        "\n",
        "    # Convert to tensors\n",
        "    return {\n",
        "        k: torch.tensor(v, dtype=torch.long if k != \"advantages\" else torch.float, device=device)\n",
        "        for k, v in inputs.items()\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_token_log_probs(\n",
        "    model: Union[DeepSpeedEngine, PreTrainedModel],\n",
        "    inputs: Dict[str, torch.Tensor],\n",
        "    temperature: float,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute log probabilities for each token in the sequence, masked for valid labels only.\n",
        "\n",
        "    This function:\n",
        "    1. Runs the model forward pass\n",
        "    2. Applies temperature scaling to logits\n",
        "    3. Shifts the sequences for causal language modeling\n",
        "    4. Computes log probabilities for the actual tokens that appeared in the sequence\n",
        "    5. Masks the log probabilities to only include valid labels (non -100 positions)\n",
        "\n",
        "    Args:\n",
        "        model: The language model (either DeepSpeed-wrapped or regular HuggingFace model)\n",
        "        inputs: Dictionary containing:\n",
        "            - input_ids: Tensor of token ids [batch_size, seq_len]\n",
        "            - attention_mask: Tensor of attention mask [batch_size, seq_len]\n",
        "            - labels: Tensor of target labels [batch_size, seq_len] with -100 for ignored positions\n",
        "        temperature: Temperature for scaling the logits before softmax\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Log probabilities tensor of shape [batch_size, seq_len-1], where:\n",
        "            - Each value is the log probability of the actual token that appeared\n",
        "            - Values are masked to 0.0 for positions where labels were -100\n",
        "            - The sequence length is reduced by 1 due to the causal shift\n",
        "\n",
        "    Example:\n",
        "        >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "        >>> inputs = {\n",
        "        ...     \"input_ids\": torch.tensor([[1, 2, 3]]),\n",
        "        ...     \"attention_mask\": torch.tensor([[1, 1, 1]]),\n",
        "        ...     \"labels\": torch.tensor([[-100, 2, 3]])\n",
        "        ... }\n",
        "        >>> log_probs = compute_token_log_probs(model, inputs, temperature=1.0)\n",
        "        >>> log_probs.shape\n",
        "        torch.Size([1, 2])  # batch_size=1, seq_len-1=2\n",
        "        >>> # First position is 0 (masked), second position has actual log prob\n",
        "    \"\"\"\n",
        "    outputs = model(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        return_dict=True,\n",
        "        use_cache=False,\n",
        "    )\n",
        "\n",
        "    logits = outputs.logits.float() / temperature  # Shape: [batch_size, seq_len, vocab_size]\n",
        "    shift_logits = logits[..., :-1, :].contiguous()  # Shape: [batch_size, seq_len-1, vocab_size]\n",
        "    shift_labels = inputs[\"labels\"][..., 1:].contiguous()  # Shape: [batch_size, seq_len-1]\n",
        "\n",
        "    # Create mask for valid labels\n",
        "    label_mask = (shift_labels != -100).float()  # Shape: [batch_size, seq_len-1]\n",
        "    shift_labels[shift_labels == -100] = 0  # Shape: [batch_size, seq_len-1]\n",
        "\n",
        "    # Calculate log probabilities\n",
        "    log_probs = torch.log_softmax(shift_logits, dim=-1)  # Shape: [batch_size, seq_len-1, vocab_size]\n",
        "    log_probs = torch.gather(log_probs, dim=2, index=shift_labels.unsqueeze(2))  # Shape: [batch_size, seq_len-1, 1]\n",
        "    log_probs = log_probs.squeeze(2)  # Shape: [batch_size, seq_len-1]\n",
        "    log_probs = log_probs * label_mask  # Shape: [batch_size, seq_len-1]\n",
        "\n",
        "    return log_probs\n",
        "\n",
        "\n",
        "def find_free_port():\n",
        "    \"\"\"Find a free port on localhost.\"\"\"\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.bind((\"\", 0))\n",
        "        s.listen(1)\n",
        "        port = s.getsockname()[1]\n",
        "    return port\n",
        "\n",
        "\n",
        "def evaluate_on_test_set(\n",
        "    inference_engine: LLM,\n",
        "    test_dataset: Dataset,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    eos_token: str,\n",
        "    eval_sampling_params: SamplingParams,\n",
        "    reward_func: Callable[[str, Dict[str, Any]], Tuple[float, Dict[str, float]]],\n",
        ") -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Evaluate the model on a test dataset by generating responses and computing rewards.\n",
        "\n",
        "    Args:\n",
        "        inference_engine: The sglang Engine instance used for text generation\n",
        "        test_dataset: Dataset containing test samples\n",
        "        tokenizer: Tokenizer for decoding generated token IDs\n",
        "        eos_token: End of sequence token string\n",
        "        eval_sampling_params: Dictionary of parameters for controlling the generation process\n",
        "        reward_func: Function that computes rewards for generated responses. Takes a response\n",
        "            string and sample dict as input, returns a tuple of (overall_reward, reward_components)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing evaluation statistics:\n",
        "            - response_lengths: List of token counts for each generated response\n",
        "            - rewards: List of overall reward values for each response\n",
        "            - non_stop_rate: List of booleans indicating if generation ended for non-stop reason\n",
        "            - reward_metrics/*: Lists of individual reward component values, prefixed with\n",
        "              \"reward_metrics/\"\n",
        "        episodes: Dictionary containing:\n",
        "            - all_query_token_ids: List of query token IDs for each episode\n",
        "            - all_response_token_ids: List of response token IDs for each episode\n",
        "\n",
        "    Example:\n",
        "        >>> episodes, episodes_stats = evaluate_on_test_set(\n",
        "        ...     inference_engine=engine,\n",
        "        ...     test_dataset=dataset,\n",
        "        ...     tokenizer=tokenizer,\n",
        "        ...     eos_token=\"</s>\",\n",
        "        ...     eval_sampling_params={\"temperature\": 0.7, \"max_tokens\": 100},\n",
        "        ...     reward_func=compute_rewards\n",
        "        ... )\n",
        "        >>> print(f\"Average reward: {episodes_stats['rewards']:.3f}\")\n",
        "    \"\"\"\n",
        "    generations = inference_engine.generate(\n",
        "        prompt_token_ids=test_dataset[\"input_ids\"], sampling_params=eval_sampling_params\n",
        "    )\n",
        "\n",
        "    metrics = {\n",
        "        \"response_lengths\": [],\n",
        "        \"rewards\": [],\n",
        "        \"non_stop_rate\": [],\n",
        "    }\n",
        "\n",
        "    all_query_token_ids = []\n",
        "    all_responses_token_ids = []\n",
        "\n",
        "    for i, sample in enumerate(test_dataset):\n",
        "        query_token_ids = sample[\"input_ids\"]\n",
        "        response_token_ids = generations[i].outputs[0].token_ids\n",
        "        finish_reason = generations[i].outputs[0].finish_reason\n",
        "\n",
        "        response = tokenizer.decode(response_token_ids, skip_special_tokens=False)\n",
        "        reward, reward_components = reward_func(response, sample)\n",
        "\n",
        "        all_query_token_ids.append(query_token_ids)\n",
        "        all_responses_token_ids.append(response_token_ids)\n",
        "\n",
        "        metrics[\"rewards\"].append(reward)\n",
        "        metrics[\"non_stop_rate\"].append(finish_reason != \"stop\")\n",
        "        metrics[\"response_lengths\"].append(len(response_token_ids))\n",
        "        for k, v in reward_components.items():\n",
        "            metrics.setdefault(f\"reward_metrics/{k}\", []).append(v)\n",
        "\n",
        "    episodes = {\n",
        "        \"all_query_token_ids\": all_query_token_ids,\n",
        "        \"all_response_token_ids\": all_responses_token_ids,\n",
        "    }\n",
        "\n",
        "    return episodes, metrics\n",
        "\n",
        "\n",
        "def dump_episodes(\n",
        "    episodes: Dict[str, Any],\n",
        "    episodes_stats: Dict[str, Any],\n",
        "    exp_dir: Path,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    iteration: int,\n",
        "    is_eval: bool = False,\n",
        ") -> wandb.Table:\n",
        "    query_token_ids = episodes[\"all_query_token_ids\"]\n",
        "    response_token_ids = episodes[\"all_response_token_ids\"]\n",
        "    rewards = episodes_stats[\"rewards\"]\n",
        "    response_lengths = episodes_stats[\"response_lengths\"]\n",
        "\n",
        "    query_texts = tokenizer.batch_decode(\n",
        "        query_token_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
        "    )\n",
        "    response_texts = tokenizer.batch_decode(\n",
        "        response_token_ids,\n",
        "        skip_special_tokens=False,\n",
        "        clean_up_tokenization_spaces=False,\n",
        "    )\n",
        "\n",
        "    if not is_eval:\n",
        "        print(f\"########## Example 1 (Reward: {rewards[0]}, Response Length: {response_lengths[0]})\")\n",
        "        print(f\"#### Query:\\n`{query_texts[0]}`\")\n",
        "        print(f\"#### Response:\\n`{response_texts[0]}`\\n\\n\")\n",
        "\n",
        "        print(f\"########## Example 2 (Reward: {rewards[1]}, Response Length: {response_lengths[1]})\")\n",
        "        print(f\"#### Query:\\n`{query_texts[1]}`\")\n",
        "        print(f\"#### Response:\\n`{response_texts[1]}`\\n\\n\")\n",
        "\n",
        "    if is_eval:\n",
        "        episodes_dir = exp_dir / \"eval_episodes\"\n",
        "    else:\n",
        "        episodes_dir = exp_dir / \"episodes\"\n",
        "    episodes_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with open(episodes_dir / f\"eps_{iteration:06d}.json\", \"w\") as f:\n",
        "        json.dump(\n",
        "            [\n",
        "                {\n",
        "                    \"query\": query_texts[i],\n",
        "                    \"response\": response_texts[i],\n",
        "                    \"reward\": rewards[i],\n",
        "                }\n",
        "                for i in range(len(query_texts))\n",
        "            ],\n",
        "            f,\n",
        "        )\n",
        "\n",
        "    # Create wandb table\n",
        "    table = wandb.Table(columns=[\"query\", \"response\", \"reward\", \"response_length\"])\n",
        "    for i in range(len(query_texts)):\n",
        "        table.add_data(query_texts[i], response_texts[i], rewards[i], response_lengths[i])\n",
        "\n",
        "    return table\n",
        "\n",
        "\n",
        "def find_last_checkpoint(exp_dir: Path) -> Tuple[Optional[Path], Optional[int]]:\n",
        "    checkpoint_dir = exp_dir / \"checkpoints\"\n",
        "    checkpoints = list(checkpoint_dir.glob(\"ckpt_*\"))\n",
        "    # Filter out directories that don't have a deepspeed subdirectory\n",
        "    checkpoints = [ckpt for ckpt in checkpoints if (ckpt / \"deepspeed\").exists()]\n",
        "    if not checkpoints:\n",
        "        return None, None\n",
        "    ckpt_path = max(checkpoints, key=lambda x: int(x.stem.split(\"_\")[-1]))\n",
        "    ckpt_iter = int(ckpt_path.stem.split(\"_\")[-1])\n",
        "    return ckpt_path, ckpt_iter\n",
        "\n",
        "\n",
        "def load_model_into_vllm(model: Union[DeepSpeedEngine, PreTrainedModel], llm: LLM) -> None:\n",
        "    \"\"\"\n",
        "    Load weights from a HuggingFace model (either wrapped in DeepSpeed or not) into a vLLM inference engine.\n",
        "\n",
        "    This function transfers the weights from a training model to a vLLM inference engine,\n",
        "    allowing for efficient inference using the updated model weights.\n",
        "\n",
        "    Args:\n",
        "        model (Union[DeepSpeedEngine, PreTrainedModel]): The source model to copy weights from.\n",
        "            Can be either a DeepSpeed-wrapped model or a regular HuggingFace PreTrainedModel.\n",
        "        vllm (LLM): The target vLLM inference engine to load the weights into.\n",
        "            Must be already initialized and ready to accept new weights.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    state_dict = model.module.state_dict() if isinstance(model, DeepSpeedEngine) else model.state_dict()\n",
        "    llm.llm_engine.model_executor.driver_worker.model_runner.model.load_weights(state_dict.items())"
      ],
      "metadata": {
        "id": "JLUNRsMSDk50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fQoy_w5L__s"
      },
      "source": [
        "### Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74P5oTB5L__s"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import re\n",
        "import time\n",
        "from typing import Any, Dict, List, Tuple, Union\n",
        "\n",
        "import deepspeed\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from deepspeed import DeepSpeedEngine\n",
        "from tqdm import trange\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "import wandb\n",
        "\n",
        "# Needed to stop DeepSpeed from complaining\n",
        "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "os.environ[\"MASTER_PORT\"] = str(find_free_port())\n",
        "os.environ[\"RANK\"] = \"0\"\n",
        "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
        "os.environ[\"WORLD_SIZE\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usFviK1ML__s"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2cldJSLL__s"
      },
      "source": [
        "我们要开始定义训练的**超参数**了。这些参数大部分都参考了 [Mini-R1](https://www.philschmid.de/mini-deepseek-r1) 的实现。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGQ-vi2gL__s"
      },
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "#MODEL_NAME = \"Qwen/Qwen2.5-3B\"\n",
        "#MODEL_NAME = \"Qwen/Qwen2.5-1.5B\"\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
        "\n",
        "MODEL_CHAT_NAME = MODEL_NAME + \"-Instruct\"\n",
        "\n",
        "# Dataset configuration\n",
        "DATASET_NAME = \"Jiayi-Pan/Countdown-Tasks-3to4\"\n",
        "\n",
        "# Total number of training iterations\n",
        "NUM_ITERATIONS = 1000\n",
        "# Number of episodes to collect per iteration for training\n",
        "EPISODES_PER_ITERATION = 64\n",
        "# Number of responses to generate for each input prompt (i.e. group size in GRPO)\n",
        "GENERATIONS_PER_SAMPLE = 4\n",
        "# Controls how much the policy can deviate from the reference model\n",
        "KL_COEFFICIENT = 0.001\n",
        "\n",
        "# Training hyperparameters\n",
        "# Batch size for each GPU device during training\n",
        "PER_DEVICE_BATCH_SIZE = 4\n",
        "# Learning rate for model updates\n",
        "LEARNING_RATE = 1e-6\n",
        "\n",
        "# Sampling parameters\n",
        "# Maximum number of tokens to generate in each response\n",
        "MAX_RESPONSE_TOKENS = 1024\n",
        "# Controls randomness in generation (higher = more random)\n",
        "TEMPERATURE = 1.0\n",
        "# Nucleus sampling parameter (1.0 = disabled)\n",
        "TOP_P = 1.0\n",
        "# Top-k sampling parameter (-1 = disabled)\n",
        "TOP_K = -1  # no top k\n",
        "\n",
        "# DeepSpeed configuration\n",
        "# DeepSpeed config for the policy model\n",
        "deepspeed_config = {\n",
        "    \"bf16\": {\"enabled\": True},\n",
        "    \"zero_optimization\": {\"stage\": 2, \"overlap_comm\": False},\n",
        "    \"train_batch_size\": EPISODES_PER_ITERATION,\n",
        "    \"train_micro_batch_size_per_gpu\": PER_DEVICE_BATCH_SIZE,\n",
        "    \"gradient_accumulation_steps\": EPISODES_PER_ITERATION // PER_DEVICE_BATCH_SIZE,\n",
        "    \"gradient_clipping\": 1.0,\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "            \"lr\": LEARNING_RATE,\n",
        "            \"betas\": (0.9, 0.999),\n",
        "            \"eps\": 1e-8,\n",
        "            \"weight_decay\": 0.0,\n",
        "            \"torch_adam\": True,\n",
        "        },\n",
        "    },\n",
        "}\n",
        "# DeepSpeed config for the reference model\n",
        "ref_deepspeed_config = {\n",
        "    \"bf16\": {\"enabled\": True},\n",
        "    # Note that we don't train the reference model\n",
        "    # These are just for compatibility with DeepSpeed.\n",
        "    \"train_batch_size\": EPISODES_PER_ITERATION,\n",
        "    \"train_micro_batch_size_per_gpu\": PER_DEVICE_BATCH_SIZE,\n",
        "    \"gradient_accumulation_steps\": EPISODES_PER_ITERATION // PER_DEVICE_BATCH_SIZE,\n",
        "}\n",
        "\n",
        "RUN_NAME = \"r1-zero\"\n",
        "EXP_DIR = f\"{SCRATCH}/deepseek_r1z_hackathon/{RUN_NAME}\"\n",
        "os.makedirs(EXP_DIR, exist_ok=True)\n",
        "print(f\"Logs and Checkpoints will be saved to: {EXP_DIR}\")\n",
        "print(Path(EXP_DIR))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hRsQlw7L__t"
      },
      "source": [
        "## Generating the training prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xCQtP9qL__t"
      },
      "source": [
        "我们将在训练中使用 [Countdown-Tasks-3to4](https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4) 数据集。这个数据集提供了问题陈述及其最终答案（但不包含推理步骤）。\n",
        "\n",
        "### 倒计时任务\n",
        "\n",
        "倒计时游戏是一个数字谜题，玩家必须使用一组随机选择的数字和基本的算术运算：加法、减法、乘法和除法，来达到一个目标数字。每个数字必须且只能使用一次。\n",
        "\n",
        "示例：\n",
        "\n",
        "```yaml\n",
        "目标：622\n",
        "可用数字：[25, 3, 6, 100]\n",
        "\n",
        "# 数据集中不提供此内容\n",
        "解决方案：(100 × 6) + (25 − 3) = 622\n",
        "```\n",
        "\n",
        "这项任务非常适合训练大型语言模型练习推理、搜索和自我验证能力。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIuozqN_L__t"
      },
      "source": [
        "由于我们使用的是模型的**基础版本**，它只通过原始互联网数据进行了预训练，因此对系统提示或聊天格式没有先验理解。然而，我们仍然会采用**聊天格式**，以确保最终的模型能与期望这种格式的下游工具和框架兼容。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bzox0PDL__t"
      },
      "outputs": [],
      "source": [
        "SYSTEM_MESSAGE = (\n",
        "    \"You are a helpful assistant. You first think about the reasoning process in the mind \"\n",
        "    \"and then provide the user with the answer.\"\n",
        ")\n",
        "PROMPT_TEMPLATE = (\n",
        "    \"Using the numbers {numbers}, create an equation that equals {target}. \"\n",
        "    \"You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. \"\n",
        "    \"Show your work in <think> </think> tags. And return the final equation and answer in \"\n",
        "    \"<answer> </answer> tags, for example <answer>(1 + 2) / (3 * 5)</answer>.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FyUghB1L__t"
      },
      "source": [
        "有了系统消息和提示模板，我们就可以生成训练提示了。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBmv9ys0L__t"
      },
      "outputs": [],
      "source": [
        "# Load and process dataset\n",
        "def preprocess_example(example: Dict[str, Any]):\n",
        "    numbers: List[int] = example[\"nums\"]\n",
        "    target: int = example[\"target\"]\n",
        "\n",
        "    prefix = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
        "        {\"role\": \"user\", \"content\": PROMPT_TEMPLATE.format(numbers=numbers, target=target)},\n",
        "        {\"role\": \"assistant\", \"content\": \"Let me solve this step by step.\\n<think>\"},\n",
        "    ]\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        prefix, tokenize=True, continue_final_message=True\n",
        "    )\n",
        "    prompt = tokenizer.decode(\n",
        "        input_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
        "    )\n",
        "    return {\"prompt\": prompt, \"input_ids\": input_ids}\n",
        "\n",
        "# Note that the base model and \"instruct\" model have different eos token.\n",
        "# Here we make sure to use the correct one.\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHAT_NAME)\n",
        "EOS_TOKEN_ID = AutoTokenizer.from_pretrained(MODEL_NAME).eos_token_id\n",
        "EOS_TOKEN = tokenizer.convert_ids_to_tokens(EOS_TOKEN_ID)\n",
        "\n",
        "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
        "dataset = dataset.map(preprocess_example, num_proc=6)\n",
        "\n",
        "# Split dataset\n",
        "train_test_split = dataset.train_test_split(test_size=500, seed=42)\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "test_dataset = train_test_split[\"test\"]\n",
        "\n",
        "len(train_dataset), len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FY0bk04L__u"
      },
      "outputs": [],
      "source": [
        "print(\"Target: \", train_dataset[0][\"target\"])\n",
        "print(\"Available Numbers: \", train_dataset[0][\"nums\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94a5uiZuL__u"
      },
      "outputs": [],
      "source": [
        "print(train_dataset[0][\"prompt\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuiBd1eoL__u"
      },
      "source": [
        "在每个提示前添加了 `<assistant>` 标签和短语**“让我一步一步地解决这个问题。”**这有助于引导模型进入**回答模式**。如果没有这个引导，基础模型可能只会继续提示，而不是尝试解决任务，因为它本身没有理解指令的能力。\n",
        "\n",
        "此外，我们对每个提示进行分词，并将结果存储为 `input_ids`，这将在稍后的训练中使用。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FroDnBhLL__u"
      },
      "outputs": [],
      "source": [
        "print(train_dataset[0][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaZnqOCwL__u"
      },
      "source": [
        "## Reward Function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTqfJp8jL__u"
      },
      "source": [
        "DeepSeek R1 论文引入了**基于规则的奖励**来评估模型生成的解决方案是否正确。我们将采用类似的方法，定义两个自定义奖励函数：\n",
        "\n",
        "---\n",
        "\n",
        "## 奖励函数详解\n",
        "\n",
        "1.  **格式奖励（Format Reward）**：\n",
        "    * 检查输出是否遵循所需的格式：\n",
        "        `<think> [思考过程] </think><answer> [答案] </answer>`\n",
        "    * 强制执行这种格式主要是为了方便答案提取。虽然它并非答案正确性本身所必需，但在训练期间能大大简化解析过程。\n",
        "\n",
        "2.  **等式奖励（Equation Reward）**：\n",
        "    * 从 `<answer>` 标签中提取等式。\n",
        "    * 验证该等式计算结果是否与目标结果匹配。\n",
        "    * 确保所有提供的可用数字在等式中恰好使用一次。\n",
        "\n",
        "---\n",
        "\n",
        "## 最终奖励计算\n",
        "\n",
        "分配给一个回合/轨迹（即提示 + 响应）的最终奖励是这两个组件的简单总和。值得注意的是，奖励只在输出的**最后一个 token** 处计算。从强化学习的角度来看，这意味着所有中间动作都获得零奖励。此外，我们这里也没有应用任何折扣（即 $\\gamma = 1$）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST3PU8vvL__v"
      },
      "outputs": [],
      "source": [
        "def format_reward_func(completion: str) -> float:\n",
        "    \"\"\"\n",
        "    Format: <think>...</think>\\n</answer>...</answer>\n",
        "\n",
        "    Also checks that the content within <answer>...</answer> conforms to a\n",
        "    specified pattern (only digits, + - * / ( ) . and whitespace).\n",
        "\n",
        "    Args:\n",
        "        completion (str): Generated output\n",
        "\n",
        "    Returns:\n",
        "        float: Reward score\n",
        "    \"\"\"\n",
        "    # Define the allowed pattern (only numbers, +, -, *, /, (, ), ., and whitespace)\n",
        "    allowed_pattern = r\"^[\\d+\\-*/().\\s]+$\"\n",
        "\n",
        "    try:\n",
        "        # add synthetic <think> as its already part of the prompt and prefilled\n",
        "        # for the assistant to more easily match the regex\n",
        "        completion = \"<think>\" + completion\n",
        "\n",
        "        # Strip EOS token if present\n",
        "        if completion.endswith(EOS_TOKEN):\n",
        "            completion = completion[:-len(EOS_TOKEN)]\n",
        "\n",
        "        # Check if the format is correct\n",
        "        # Pattern means:\n",
        "        # 1) <think>...contents not including other <think> tags...</think>\n",
        "        # 2) \\n\n",
        "        # 3) <answer>...anything...</answer>\n",
        "        regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\n<answer>([\\s\\S]*?)<\\/answer>$\"\n",
        "        match = re.search(regex, completion, re.DOTALL)\n",
        "\n",
        "        if match is None or len(match.groups()) != 2:\n",
        "            # Format is incorrect\n",
        "            return 0.0\n",
        "        else:\n",
        "            # Extract the content inside <answer>...</answer>\n",
        "            answer_content = match.group(2).strip()\n",
        "\n",
        "            # Check if answer content matches the allowed pattern\n",
        "            if not re.match(allowed_pattern, answer_content):\n",
        "                # If it doesn't match, reward is 0.5\n",
        "                return 0.5\n",
        "            else:\n",
        "                # If both format and pattern are correct, reward is 1\n",
        "                return 1.0\n",
        "    except Exception:\n",
        "        # Any error leads to 0 reward\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def equation_reward_func(completion: str, nums: List[int], target: int) -> float:\n",
        "    \"\"\"\n",
        "    Evaluates completion based on mathematical correctness of the answer\n",
        "\n",
        "    Args:\n",
        "        completion (str): Generated output\n",
        "        target (str): Expected answer\n",
        "        nums (list): Available numbers to use in the equation\n",
        "\n",
        "    Returns:\n",
        "        float: Reward score\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if the format is correct\n",
        "        match = re.search(r\"<answer>(.*?)<\\/answer>\", completion)\n",
        "        if match is None:\n",
        "            return 0.0\n",
        "        # Extract the \"answer\" part from the completion\n",
        "        equation = match.group(1).strip()\n",
        "        # Extract all numbers from the equation\n",
        "        used_numbers = [int(n) for n in re.findall(r\"\\d+\", equation)]\n",
        "\n",
        "        # Check if all numbers are used exactly once\n",
        "        if sorted(used_numbers) != sorted(nums):\n",
        "            return 0.0\n",
        "        # Define a regex pattern that only allows numbers, operators, parentheses, and whitespace\n",
        "        allowed_pattern = r\"^[\\d+\\-*/().\\s]+$\"\n",
        "        if not re.match(allowed_pattern, equation):\n",
        "            return 0.0\n",
        "\n",
        "        # Evaluate the equation with restricted globals and locals\n",
        "        result = eval(equation, {\"__builtins__\": None}, {})\n",
        "        # Check if the equation is correct and matches the ground truth\n",
        "        if abs(float(result) - float(target)) < 1e-5:\n",
        "            return 1.0\n",
        "        else:\n",
        "            return 0.0\n",
        "    except Exception:\n",
        "        # If evaluation fails, reward is 0\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def compute_reward(completion: str, sample: Dict[str, Any]) -> Tuple[float, Dict[str, float]]:\n",
        "    nums = sample[\"nums\"]\n",
        "    target = sample[\"target\"]\n",
        "\n",
        "    format_reward = format_reward_func(completion)\n",
        "    equation_reward = equation_reward_func(\n",
        "        completion=completion, nums=nums, target=target\n",
        "    )\n",
        "\n",
        "    reward = format_reward + equation_reward\n",
        "\n",
        "    metrics = {\n",
        "        \"format_reward\": format_reward,\n",
        "        \"equation_reward\": equation_reward,\n",
        "    }\n",
        "\n",
        "    return reward, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUQJyWtfL__v"
      },
      "outputs": [],
      "source": [
        "# <think> is prefilled in the prompt. So, repeating it in the completion would be incorret.\n",
        "format_reward_func(\"<think>I think the answer is </think>\\n<answer>1+2</answer>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCEzbbTvL__v"
      },
      "outputs": [],
      "source": [
        "format_reward_func(\"I think the answer is </think>\\n<answer>1+2</answer>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLtWBs_4L__v"
      },
      "outputs": [],
      "source": [
        "format_reward_func(\"<think>I think the<think>and even more</think> answer is </think>\\n<answer>1+2</answer>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUXJfKgRL__v"
      },
      "outputs": [],
      "source": [
        "equation_reward_func(\"I think the answer is </think>\\n<answer>1+2+2</answer>\", [1,2], 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHPLk7NWL__v"
      },
      "source": [
        "## Episode Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV-2Oh3cL__w"
      },
      "source": [
        "情节生成（Episode generation）的目标是创建一个查询-响应对的集合，用于策略训练。从强化学习（RL）的角度来看，**查询（query）**充当初始状态，而**响应（response）**中生成的 token 则代表策略采取的行动。\n",
        "\n",
        "`create_training_episodes` 函数接收一个提示（初始状态）列表以及我们使用模型生成的相应补全。在 GRPO 中，我们总是为每个提示生成多个响应——具体来说，`GENERATIONS_PER_SAMPLE` 会大于 1。这意味着，在情节生成之后，每次 RL 迭代我们都会得到 `batch_size × GENERATIONS_PER_SAMPLE` 个情节。\n",
        "\n",
        "---\n",
        "\n",
        "### 优势计算 (Advantage Computation)\n",
        "\n",
        "除了生成情节，`create_training_episodes` 函数还负责计算每个响应 token 的**优势（advantage）**。\n",
        "\n",
        "在 RL 术语中，一个 token 的优势代表了该 token 的行动与该特定状态（提示 + 前缀）下平均生成的 token 相比，好或坏的程度。理想情况下，我们会为每个 token 单独计算优势，以捕捉每一步对整体奖励的贡献。\n",
        "\n",
        "然而，在 GRPO 中，没有按 token 计算的优势。相反，我们为每个响应计算一个单一的优势值。这个值反映了整个响应相对于为相同提示生成的其他响应的好坏程度。然后，我们将这个单一优势值均匀地分配给该响应中的所有 token。\n",
        "\n",
        "GRPO 使用一个简单的公式来实现这一点：\n",
        "\n",
        "1.  对于每个提示 $x$，以及其生成的一组响应 $y_1, y_2, \\ldots, y_G \\sim \\pi(\\cdot|x)$，计算它们的奖励 $R_1, R_2, \\ldots, R_G$。\n",
        "2.  计算该组的平均值和标准差：\n",
        "\n",
        "    $$\\mu = \\text{mean}(R_1, R_2, \\ldots, R_G)$$\n",
        "\n",
        "    $$\\sigma = \\text{std}(R_1, R_2, \\ldots, R_G)$$\n",
        "    \n",
        "3.  计算每个响应的**相对分数（relative score）**：\n",
        "\n",
        "    $$R^*_i = \\frac{R_i - \\mu}{\\sigma}$$\n",
        "4.  将这个相对分数 $R^*_i$ 作为优势分配给第 $i$ 个响应的所有 token：\n",
        "\n",
        "    $$A_t^{(i)} = R^*_i$$\n",
        "\n",
        "这种**按组归一化（per-group normalization）**的方法鼓励优于平均水平的响应，并惩罚那些表现较差的响应。\n",
        "\n",
        "---\n",
        "\n",
        "### 优势的实际应用示例\n",
        "\n",
        "考虑一个二元奖励场景，其中每个响应要么是正确的 (1)，要么是错误的 (0)：\n",
        "\n",
        "```python\n",
        ">>> rewards = np.array([1, 1, 0, 0, 0])\n",
        ">>> (rewards - rewards.mean()) / (rewards.std())\n",
        "array([ 1.22474487,  1.22474487, -0.81649658, -0.81649658, -0.81649658])\n",
        "```\n",
        "\n",
        "在这里，正确的响应获得了更高的优势分数，从而在未来的更新中得到推广。\n",
        "\n",
        "如果只有一个响应是正确的：\n",
        "\n",
        "```python\n",
        ">>> rewards = np.array([1, 0, 0, 0, 0])\n",
        ">>> (rewards - rewards.mean()) / (rewards.std())\n",
        "array([ 2. , -0.5, -0.5, -0.5, -0.5])\n",
        "```\n",
        "\n",
        "这类似于提示中的问题太难，模型平均而言无法生成正确响应的情况。然而，如果其中一个响应是正确的，它将被赋予更高的优势分数，所有不正确的响应将被赋予负的相对分数。\n",
        "\n",
        "如果所有响应都不正确：\n",
        "\n",
        "```python\n",
        ">>> rewards = np.array([0, 0, 0, 0, 0])\n",
        ">>> (rewards - rewards.mean()) / (rewards.std() + 1e-6)\n",
        "array([0., 0., 0., 0., 0.])\n",
        "```\n",
        "\n",
        "由于没有比平均更好的响应，模型收不到学习信号。\n",
        "\n",
        "如果所有响应都正确：\n",
        "\n",
        "```python\n",
        ">>> rewards = np.array([1, 1, 1, 1, 1])\n",
        ">>> (rewards - rewards.mean()) / (rewards.std() + 1e-6)\n",
        "array([0., 0., 0., 0., 0.])\n",
        "```\n",
        "\n",
        "同样，没有提供学习信号，因为没有什么可以改进的。\n",
        "\n",
        "在一个更复杂的情况下：\n",
        "\n",
        "```python\n",
        ">>> rewards = np.array([1, 1, 1, 1, 0])\n",
        ">>> (rewards - rewards.mean()) / (rewards.std() + 1e-6)\n",
        "array([0.5, 0.5, 0.5, 0.5, -2.])\n",
        "```\n",
        "\n",
        "这代表了对模型来说一个相对容易的问题。大多数响应都是正确的，但偶尔的错误响应会受到严厉的惩罚。\n",
        "\n",
        "---\n",
        "\n",
        "理解 GRPO 的优势计算如何鼓励模型学习更好的响应，即使是在不提供逐 token 奖励的情况下，这一点很重要。这能让模型在没有人类反馈的情况下，自主地进行自我改进。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CR27orbbL__w"
      },
      "outputs": [],
      "source": [
        "def create_training_episodes(\n",
        "    samples: List[Dict[str, Any]],\n",
        "    all_generations: List[List[int]],\n",
        "    all_finish_reasons: List[str],\n",
        ") -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Process model generations and calculate rewards for training episodes.\n",
        "\n",
        "    This function processes generated responses and calculates rewards for training episodes by:\n",
        "    1. Grouping generations by sample (GENERATIONS_PER_SAMPLE responses per input)\n",
        "    2. Computing rewards and advantages for each response\n",
        "    3. Processing response tokens\n",
        "\n",
        "    Args:\n",
        "        samples: List of input samples, each containing:\n",
        "            - input_ids: List[int], tokenized input prompt\n",
        "            - nums: List[int], numbers to use in equation\n",
        "            - target: int, target value for equation\n",
        "        all_generations: List of token ID sequences for each generated response\n",
        "        all_finish_reasons: List of finish reasons for each generation (\"stop\" or other)\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing:\n",
        "        1. Dictionary with processed data for training:\n",
        "            - all_query_token_ids: List[List[int]], input token IDs repeated for each generation\n",
        "            - all_response_token_ids: List[List[int]], response token IDs with EOS tokens added\n",
        "            - all_advantages: List[List[float]], advantage values repeated for each token\n",
        "        2. Dictionary with generation statistics:\n",
        "            - response_lengths: List[int], lengths of generated responses\n",
        "            - rewards: List[float], raw reward values\n",
        "            - non_stop_rate: List[bool], whether each generation ended naturally\n",
        "            - reward_metrics/*: Various reward component metrics\n",
        "\n",
        "    Example:\n",
        "        >>> samples = [{\"input_ids\": [1,2,3], \"nums\": [1,2,3], \"target\": 6}]\n",
        "        >>> generations = [[4,5, EOS_TOKEN_ID], [6,7], [8,9, EOS_TOKEN_ID]]  # 3 generations per sample\n",
        "        >>> finish_reasons = [\"stop\", \"length\", \"stop\"]\n",
        "        >>> episodes, stats = create_training_episodes(samples, generations, finish_reasons)\n",
        "        >>> episodes\n",
        "        {\n",
        "            'all_query_token_ids': [[1,2,3], [1,2,3], [1,2,3]],\n",
        "            'all_response_token_ids': [[4,5,EOS_TOKEN_ID], [6,7], [8,9,EOS_TOKEN_ID]],\n",
        "            'all_advantages': [[0.5,0.5,0.5], [-1.0,-1.0], [0.5,0.5,0.5]]\n",
        "        }\n",
        "    \"\"\"\n",
        "    assert len(all_generations) == len(all_finish_reasons)\n",
        "    assert len(all_generations) == len(samples) * GENERATIONS_PER_SAMPLE\n",
        "\n",
        "    # Process responses and calculate rewards\n",
        "    groups = [\n",
        "        list(range(i, i + GENERATIONS_PER_SAMPLE))\n",
        "        for i in range(0, len(all_generations), GENERATIONS_PER_SAMPLE)\n",
        "    ]  # example: [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n",
        "\n",
        "    all_query_token_ids, all_responses_token_ids, all_advantages = [], [], []\n",
        "\n",
        "    stats = {\n",
        "        \"response_lengths\": [],\n",
        "        \"rewards\": [],\n",
        "        \"non_stop_rate\": [],\n",
        "    }\n",
        "\n",
        "    for sample, group_indices in zip(samples, groups):\n",
        "        finish_reasons = [all_finish_reasons[i] for i in group_indices]\n",
        "        response_token_ids = [all_generations[i] for i in group_indices]\n",
        "        responses = tokenizer.batch_decode(response_token_ids, skip_special_tokens=False)\n",
        "\n",
        "        rewards_and_metrics = [compute_reward(resp, sample) for resp in responses]\n",
        "        rewards, reward_metrics = zip(*rewards_and_metrics)\n",
        "\n",
        "        rewards = np.array(rewards) # [group_size]\n",
        "        response_advantages = (rewards - rewards.mean()) / (rewards.std() + 1e-4)\n",
        "\n",
        "        advantages = [\n",
        "            [resp_adv] * len(resp)\n",
        "            for resp_adv, resp in zip(response_advantages, response_token_ids)\n",
        "        ]\n",
        "\n",
        "        all_query_token_ids.extend([sample[\"input_ids\"]] * GENERATIONS_PER_SAMPLE)\n",
        "        all_responses_token_ids.extend(response_token_ids)\n",
        "        all_advantages.extend(advantages)\n",
        "\n",
        "        stats[\"rewards\"].extend(rewards)\n",
        "        stats[\"non_stop_rate\"].extend([fr != \"stop\" for fr in finish_reasons])\n",
        "        stats[\"response_lengths\"].extend([len(ids) for ids in response_token_ids])\n",
        "        for rm in reward_metrics:\n",
        "            for k, v in rm.items():\n",
        "                stats.setdefault(f\"reward_metrics/{k}\", []).append(v)\n",
        "\n",
        "    episodes = {\n",
        "        \"all_query_token_ids\": all_query_token_ids,\n",
        "        \"all_response_token_ids\": all_responses_token_ids,\n",
        "        \"all_advantages\": all_advantages,\n",
        "    }\n",
        "\n",
        "    return episodes, stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0VMQ4BOL__w"
      },
      "outputs": [],
      "source": [
        "case_0 = {\n",
        "    \"sample\": {\"input_ids\": [1,2,3], \"nums\": [1,2,3], \"target\": 6},\n",
        "    \"generations\": [[4,5, 22, 33], [6,7], [8,9, 11], [10,11]],\n",
        "    \"finish_reasons\": [\"stop\", \"length\", \"stop\", \"stop\"]\n",
        "}\n",
        "\n",
        "case = case_0\n",
        "episodes, stats = create_training_episodes([case[\"sample\"]], case[\"generations\"], case[\"finish_reasons\"])\n",
        "episodes,stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFEbyM9sL__w"
      },
      "outputs": [],
      "source": [
        "case_1 = {\n",
        "    \"sample\": {\"input_ids\": [33, 44], \"nums\": [11, 7, 8], \"target\": 26},\n",
        "    \"generations\": [[1,2], [3,4], [5,6], [7,8]],\n",
        "    \"finish_reasons\": [\"stop\", \"stop\", \"length\", \"stop\"]\n",
        "}\n",
        "case = case_1\n",
        "episodes, stats = create_training_episodes([case[\"sample\"]], case[\"generations\"], case[\"finish_reasons\"])\n",
        "episodes,stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuaiptMjL__x"
      },
      "outputs": [],
      "source": [
        "case_2 = {\n",
        "    \"sample\": {\"input_ids\": [9, 8, 7, 6, 5, 4], \"nums\": [1,2,3,4], \"target\": 10},\n",
        "    \"generations\": [[9,10], [11,12], [13,14], [15,16]],\n",
        "    \"finish_reasons\": [\"length\", \"length\", \"stop\", \"stop\"]\n",
        "}\n",
        "case = case_2\n",
        "episodes, stats = create_training_episodes([case[\"sample\"]], case[\"generations\"], case[\"finish_reasons\"])\n",
        "episodes,stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGTgI-SZL__x"
      },
      "source": [
        "没错，正如您所见，在这个单一示例中，生成的**所有回合（episodes）的 `input_ids` 都是重复的**。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQF-suscL__x"
      },
      "source": [
        "## Policy Gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvbJoJGXL__x"
      },
      "source": [
        "\n",
        "现在我们有了一批带有相应优势值的情节，我们可以计算**策略梯度损失**来更新模型。\n",
        "\n",
        "GRPO 使用与 PPO 相同的损失公式，但关键区别在于优势的计算方式。为了理解 `compute_pg_loss` 中的实现，我们首先回顾一下原始 PPO 目标：\n",
        "\n",
        "$$\n",
        "\\mathcal{l}_{\\text{PPO}} = \\mathbb{E}\\left[\\min\\left(\n",
        "\\frac{\\pi_\\theta(y_t \\mid y_{<t}, x)}{\\pi_{\\theta_{\\text{old}}}(y_t \\mid y_{<t}, x)} A_t, \\;\n",
        "\\text{clip}\\left(\n",
        "\\frac{\\pi_\\theta(y_t \\mid y_{<t}, x)}{\\pi_{\\theta_{\\text{old}}}(y_t \\mid y_{<t}, x)}, \\;\n",
        "1 - \\epsilon, \\; 1 + \\epsilon\n",
        "\\right) A_t \\right)\\right]\n",
        "$$其中：\n",
        "\n",
        "- $\\pi_{\\theta}$ 是当前策略，\n",
        "- $\\pi_{\\theta_{\\text{old}}}$ 是来自上一次迭代的策略（我们从中采样情节的策略），\n",
        "- $A_t$ 是优势（advantage）。\n",
        "\n",
        "这个目标函数试图根据优势 $A\\_t$ 增加或减少 token 的概率，但仅限于新旧策略概率之比在由裁剪阈值 $\\\\epsilon$ 控制的小范围内。这种裁剪机制可以防止训练过程中出现大的、不稳定的更新。\n",
        "\n",
        "### 完全在线设置：简化目标函数\n",
        "\n",
        "通常 PPO 中，可以使用同一批情节进行多次梯度更新。然而，在我们的例子中，我们每迭代只使用新鲜采样的情节进行**一次梯度更新**。这意味着：\n",
        "\n",
        "- $\\pi_{\\theta} = \\pi_{\\theta_{\\text{old}}}$\n",
        "- 因此，\n",
        "$$\\frac{\\pi_\\theta(y_t \\mid y_{<t}, x)}{\\pi_{\\theta_{\\text{old}}}(y_t \\mid y_{<t}, x)} = 1 $$\n",
        "由于比率恰好为 1：\n",
        "\n",
        "  - 裁剪函数变得不活跃。\n",
        "  - $\\min(\\cdot,\\cdot)$ 运算符只返回未裁剪项。\n",
        "\n",
        "所以，目标函数**简化为**：\n",
        "\n",
        "$$\\mathcal{l}_{\\text{PPO}} = \\mathbb{E}\\left[ \\frac{\\pi_\\theta(y_t \\mid y_{<t}, x)}{\\pi_{\\theta_{\\text{old}}}(y_t \\mid y_{<t}, x)} A_t \\right]$$\n",
        "\n",
        "对这个损失函数关于 $\\theta$ 求梯度，我们得到：\n",
        "\n",
        "$$\\vec{g}_{\\text{PPO}} = \\nabla_\\theta \\mathcal{l}_{\\text{PPO}} = 2 \\underbrace{\\mathbb{E}\\left[ \\nabla_\\theta \\log \\pi_\\theta(y_t \\mid y_{<t}, x) \\cdot A_t \\right]}_{\\text{带有优势的原始策略梯度}}$$\n",
        "\n",
        "这是**标准策略梯度**公式，其中对数概率由优势加权。实际上，我们恢复了香草 REINFORCE 风格的学习。\n",
        "\n",
        "> 注意：常数乘数（如 2）不影响梯度的方向，可以安全地忽略。\n",
        "> - 在强化学习的背景下，“Vanilla Policy Gradient”（VPG）是指最简单的策略梯度算法，也称为 REINFORCE 。 这是一种基本方法，通过更新参数直接优化策略，以增加导致更高奖励的行为的概率并降低导致较低奖励的行为的概率\n",
        "> - https://spinningup.openai.com/en/latest/algorithms/vpg.html\n",
        "\n",
        "事实上，这种行为并非 GRPO 独有。在 PPO、TRPO 等所有方法中，收集新数据后的第一个梯度步骤总是会简化为相同的形式。只有在优化步骤之后，裁剪或信任区域约束才开始生效。\n",
        "\n",
        "### KL 惩罚\n",
        "\n",
        "最终损失还包含一个 **KL 惩罚**项，以确保新策略不会偏离参考策略太远：\n",
        "\n",
        "\n",
        "$$ \\mathcal{l} = \\mathcal{l}_{\\text{PPO}} - \\beta \\cdot \\text{KL}(\\pi_\\theta \\parallel \\pi_{\\theta_{\\text{ref}}})$$\n",
        "\n",
        "我们使用 [Schulman 的这篇博客文章](http://joschu.net/blog/kl-approx.html) 中的 **k3 估计器**来估计 KL 散度：\n",
        "\n",
        "$$\\text{KL}(\\pi_\\theta \\parallel \\pi_{\\theta_{\\text{ref}}}) = \\mathbb{E}\\left[\\frac{\\pi_{\\theta_{\\text{ref}}}(y_t \\mid y_{<t}, x)}{\\pi_\\theta(y_t \\mid y_{<t}, x)} - \\log\\left(\\frac{\\pi_{\\theta_{\\text{ref}}}(y_t \\mid y_{<t}, x)}{\\pi_\\theta(y_t \\mid y_{<t}, x)}\\right) - 1\\right]$$\n",
        "\n",
        "这种正则化项柔和地约束了更新后的模型，使其保持与参考策略的接近。\n",
        "\n",
        "### GRPO 与 PPO/VinePPO 的主要区别\n",
        "\n",
        "**GRPO** 与 **PPO/VinePPO** 等方法之间的主要区别在于**优势的计算和应用方式**：\n",
        "\n",
        "  - 在 **PPO/VinePPO** 中，每个 token/步骤的优势是单独计算的。这允许在序列中进行细粒度的信用分配（fine-grained credit assignment）。\n",
        "  - 在 **GRPO** 中，为整个响应计算一个**单一的标量优势（scalar advantage）**，并**均匀地应用于该响应中的所有 token**。\n",
        "\n",
        "这种区别如下所示：\n",
        "\n",
        "#### GRPO 中的成功响应：\n",
        "\n",
        "<img src=\"https://github.com/McGill-NLP/nano-aha-moment/blob/main/assets/grpo_successful.png?raw=true\" alt=\"GRPO vs PPO/VinePPO: successful response\" width=\"500\">\n",
        "\n",
        "#### GRPO 中的失败响应：\n",
        "<img src=\"https://github.com/McGill-NLP/nano-aha-moment/blob/main/assets/grpo_unsuccessful.png?raw=true\" alt=\"GRPO vs PPO/VinePPO: failed response\" width=\"500\">\n",
        "\n",
        "在 GRPO 中，响应中的所有 token 都以相同的幅度进行更新。相比之下，PPO/VinePPO 以不同的优势值更新每个 token/步骤：\n",
        "\n",
        "<img src=\"https://github.com/McGill-NLP/nano-aha-moment/blob/main/assets/ppo_and_vineppo.png?raw=true\" alt=\"GRPO vs PPO/VinePPO: PPO and VinePPO\" width=\"500\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCOdLcBML__y"
      },
      "outputs": [],
      "source": [
        "def compute_pg_loss(\n",
        "    policy_model: Union[DeepSpeedEngine, PreTrainedModel],\n",
        "    reference_model: Union[DeepSpeedEngine, PreTrainedModel],\n",
        "    batch: Dict[str, torch.Tensor],\n",
        "    total_response_len: int,\n",
        ") -> Tuple[torch.Tensor, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Compute the policy gradient loss with KL penalty between policy and reference models.\n",
        "\n",
        "    This function:\n",
        "    1. Computes log probabilities for both policy and reference models\n",
        "    2. Calculates KL divergence penalty between the models\n",
        "    3. Computes policy gradient loss using advantages\n",
        "    4. Combines the losses with KL coefficient\n",
        "\n",
        "    Args:\n",
        "        policy_model: The model being trained\n",
        "        reference_model: The reference model for KL penalty calculation\n",
        "        batch: Dictionary containing:\n",
        "            - input_ids: Tensor of shape [batch_size, seq_len]\n",
        "            - attention_mask: Tensor of shape [batch_size, seq_len]\n",
        "            - labels: Tensor of shape [batch_size, seq_len] with -100 for ignored positions\n",
        "            - advantages: Tensor of shape [batch_size, seq_len]\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing:\n",
        "            - loss: Combined policy gradient and KL penalty loss (scalar tensor)\n",
        "            - metrics: Dictionary with detailed loss components:\n",
        "                - policy_loss: Pure policy gradient loss\n",
        "                - kl_penalty: KL divergence penalty\n",
        "                - entropy: Policy entropy\n",
        "    \"\"\"\n",
        "    input_ids = batch[\"input_ids\"]  # [batch_size, seq_len]\n",
        "    attention_mask = batch[\"attention_mask\"]  # [batch_size, seq_len]\n",
        "    labels = batch[\"labels\"]  # [batch_size, seq_len]\n",
        "    advantages = batch[\"advantages\"]  # [batch_size, seq_len]\n",
        "\n",
        "    model_inputs = {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "\n",
        "    labels_mask = (labels[..., 1:] != -100).float()  # [batch_size, seq_len-1]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ref_logps = compute_token_log_probs(\n",
        "            reference_model, model_inputs, TEMPERATURE\n",
        "        )  # [batch_size, seq_len-1]\n",
        "\n",
        "    logps = compute_token_log_probs(policy_model, model_inputs, TEMPERATURE)  # [batch_size, seq_len-1]\n",
        "\n",
        "    kl_penalty = torch.exp(ref_logps - logps) - (ref_logps - logps) - 1  # [batch_size, seq_len-1]\n",
        "    kl_penalty = kl_penalty * labels_mask  # [batch_size, seq_len-1]\n",
        "\n",
        "    entropy = -logps.sum() / labels_mask.sum()  # scalar\n",
        "\n",
        "    policy_loss = -logps * advantages[..., 1:]  # [batch_size, seq_len-1]\n",
        "    policy_loss = policy_loss * labels_mask  # [batch_size, seq_len-1]\n",
        "\n",
        "    loss = (policy_loss + KL_COEFFICIENT * kl_penalty).sum() / total_response_len  # scalar\n",
        "\n",
        "    metrics = {\n",
        "        \"policy_loss\": policy_loss.sum().item() / total_response_len,\n",
        "        \"kl_penalty\": kl_penalty.sum().item() / total_response_len,\n",
        "        \"entropy\": entropy.item() / total_response_len,\n",
        "    }\n",
        "\n",
        "    return loss, metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFCHCx5bL__y"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKN1-6wdL__y"
      },
      "source": [
        "在开始强化学习（RL）循环之前，我们需要设置所有必要的组件：\n",
        "\n",
        "* **策略模型（Policy Model）**：将使用策略梯度进行训练的主要模型。\n",
        "* **参考模型（Reference Model）**：一个冻结的基础模型副本，用于 KL 正则化。\n",
        "* **DeepSpeed**：两个模型都用 DeepSpeed 进行初始化。\n",
        "* **vLLM 推理引擎（vLLM Inference Engine）**：用于在情节生成期间进行快速批处理推理。\n",
        "* **WandB 日志记录（WandB Logging）**：我们初始化 WandB 来跟踪训练指标、超参数和检查点。\n",
        "\n",
        "最后，如果检测到现有检查点，我们会自动从上次中断的地方恢复训练。\n",
        "\n",
        "几点说明：\n",
        "* 我们将参考模型移到 CPU，并且只在策略梯度计算期间将其重新移回 GPU。由于模型相对较小，这种在 GPU 和 CPU 之间的来回移动速度非常快。\n",
        "* 尽管整个训练都在单个 GPU 上运行，但我们仍然使用 DeepSpeed Zero stage 2。这是因为 stage 2 附带了一些优化，可以避免内存碎片，从而充分利用 GPU 内存。\n",
        "* 在我们的设置中，Flash Attention 是必需的，因为它将 Transformer 的内存需求从 $\\mathcal{O}(n^2)$ 降低到 $\\mathcal{O}(n)$，其中 $n$ 是序列长度。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing\n",
        "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n"
      ],
      "metadata": {
        "id": "XvHaN2G0U7mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"WANDB_API_KEY\"]=userdata.get('WANDB_API_KEY')"
      ],
      "metadata": {
        "id": "8XYiTWSSaYMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Y0hKDpbL__y"
      },
      "outputs": [],
      "source": [
        "# Initialize main and reference models\n",
        "policy_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=0,\n",
        ")\n",
        "reference_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=0,\n",
        ")\n",
        "policy_model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
        "\n",
        "\n",
        "# Initialize DeepSpeed engines\n",
        "policy_model, *_ = deepspeed.initialize(\n",
        "    model=policy_model,\n",
        "    config=deepspeed_config,\n",
        "    model_parameters=policy_model.parameters(),\n",
        ")\n",
        "reference_model, *_ = deepspeed.initialize(\n",
        "    model=reference_model,\n",
        "    config=ref_deepspeed_config,\n",
        ")\n",
        "\n",
        "reference_model.module.cpu()\n",
        "\n",
        "############################################\n",
        "# Initialize vLLM (Inference) engine\n",
        "############################################\n",
        "\n",
        "inference_engine = LLM(\n",
        "    model=MODEL_NAME,\n",
        "    skip_tokenizer_init=False,\n",
        "    gpu_memory_utilization=0.2,\n",
        "    enable_prefix_caching=True,\n",
        "    swap_space=1,\n",
        "    scheduling_policy=\"fcfs\",\n",
        "    dtype=torch.bfloat16,\n",
        "    max_model_len=2048,\n",
        "    enable_sleep_mode=True,\n",
        ")\n",
        "\n",
        "# Wandb for logging\n",
        "wandb.init(\n",
        "    project=\"r1-aha-moment\",\n",
        "    name=RUN_NAME,\n",
        "    config={\n",
        "        \"model_name\": MODEL_NAME,\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"num_iterations\": NUM_ITERATIONS,\n",
        "        \"episodes_per_iteration\": EPISODES_PER_ITERATION,\n",
        "        \"rollouts_per_episode\": GENERATIONS_PER_SAMPLE,\n",
        "        \"kl_coefficient\": KL_COEFFICIENT,\n",
        "        \"temperature\": TEMPERATURE,\n",
        "    },\n",
        ")\n",
        "\n",
        "# Load checkpoint if it exists\n",
        "begin_iter = 0\n",
        "ckpt_path, ckpt_iter = find_last_checkpoint(Path(EXP_DIR))\n",
        "if ckpt_path is not None:\n",
        "    print(f\"Resuming from checkpoint {ckpt_path} at iteration {ckpt_iter}\")\n",
        "    out = policy_model.load_checkpoint(ckpt_path / \"deepspeed\")\n",
        "    if out is None:\n",
        "        raise RuntimeError(f\"Failed to load checkpoint {ckpt_path}\")\n",
        "    begin_iter = ckpt_iter + 1\n",
        "    load_model_into_vllm(policy_model, inference_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRZo8M-8L__y"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9po1Jr5mL__y"
      },
      "source": [
        "环境搭建完毕，我们现在可以启动主要的训练循环了。循环的每次迭代都会执行以下步骤：\n",
        "\n",
        "---\n",
        "\n",
        "## 训练循环步骤\n",
        "\n",
        "1.  **评估**（可选）：\n",
        "    每隔几次迭代，模型会在测试集上进行评估，以监控训练进展。\n",
        "\n",
        "2.  **情节生成**：\n",
        "    从数据集中抽样一批提示，并使用推理引擎为每个提示生成多个响应。然后，我们会让推理引擎进入“休眠”状态。\n",
        "\n",
        "3.  **奖励计算**：\n",
        "    计算每个生成情节的奖励和优势。\n",
        "\n",
        "4.  **策略梯度训练**：\n",
        "    利用计算出的优势，我们计算策略梯度损失并更新模型参数。训练通过**梯度累积**来处理大批量数据。请注意，我们每次迭代只应用**一次梯度更新**。\n",
        "\n",
        "5.  **推理引擎更新**：\n",
        "    推理引擎被“唤醒”，并用最新的模型权重进行更新。\n",
        "\n",
        "6.  **日志记录**：\n",
        "    使用 WandB 记录训练和评估指标。\n",
        "\n",
        "7.  **检查点**：\n",
        "    每 50 次迭代，模型的权重和优化器状态会被保存下来。\n",
        "\n",
        "此循环会持续运行，直到完成指定数量的迭代。\n",
        "\n",
        "---\n",
        "\n",
        "## vLLM 的“休眠”机制\n",
        "\n",
        "在训练开始之前，我们会让 vLLM 进入“休眠”模式，以释放其 KV 缓存和模型权重，确保有足够的 GPU 内存可用于策略训练。训练步骤完成后，vLLM 会被“唤醒”，重新初始化其 KV 缓存，并准备好使用更新后的模型参数进行下一轮的采样。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HF_tFQ7pL__z"
      },
      "outputs": [],
      "source": [
        "for iteration in trange(NUM_ITERATIONS):\n",
        "    print(f\"Iteration {iteration}/{NUM_ITERATIONS}\")\n",
        "\n",
        "    metrics = {}\n",
        "\n",
        "    #########################################################\n",
        "    # Evaluation\n",
        "    #########################################################\n",
        "\n",
        "    eval_stats = None\n",
        "    if iteration % 25 == 0:\n",
        "        print(\"Evaluating on eval set...\")\n",
        "        eval_episodes, eval_stats = evaluate_on_test_set(\n",
        "            inference_engine=inference_engine,\n",
        "            test_dataset=test_dataset,\n",
        "            tokenizer=tokenizer,\n",
        "            eos_token=EOS_TOKEN,\n",
        "            eval_sampling_params=SamplingParams(\n",
        "                temperature=0.3,\n",
        "                max_tokens=1024,\n",
        "                n=1,\n",
        "                detokenize=False,\n",
        "                stop_token_ids=[EOS_TOKEN_ID],\n",
        "            ),\n",
        "            reward_func=lambda completion, sample: compute_reward(\n",
        "                completion, sample\n",
        "            ),\n",
        "        )\n",
        "        eval_episode_table = dump_episodes(\n",
        "            episodes=eval_episodes,\n",
        "            episodes_stats=eval_stats,\n",
        "            exp_dir=Path(EXP_DIR),\n",
        "            tokenizer=tokenizer,\n",
        "            iteration=iteration,\n",
        "            is_eval=True,\n",
        "        )\n",
        "        wandb.log({\"eval/episodes\": eval_episode_table, \"iteration\": iteration})\n",
        "\n",
        "\n",
        "    #########################################################\n",
        "    # Generate Episodes\n",
        "    #########################################################\n",
        "\n",
        "    # Sample training batch\n",
        "    num_samples = EPISODES_PER_ITERATION // GENERATIONS_PER_SAMPLE\n",
        "    indices = np.random.choice(\n",
        "        len(train_dataset), size=num_samples, replace=False\n",
        "    )\n",
        "    samples = train_dataset.select(indices)\n",
        "\n",
        "    # Sample responses\n",
        "    outputs = inference_engine.generate(\n",
        "        prompt_token_ids=samples[\"input_ids\"],\n",
        "        sampling_params=SamplingParams(\n",
        "            n=GENERATIONS_PER_SAMPLE,\n",
        "            temperature=TEMPERATURE,\n",
        "            top_p=TOP_P,\n",
        "            top_k=TOP_K,\n",
        "            max_tokens=MAX_RESPONSE_TOKENS,\n",
        "            detokenize=False,\n",
        "            stop_token_ids=[EOS_TOKEN_ID],\n",
        "        )\n",
        "    )\n",
        "    all_generations = [list(g.token_ids) for out in outputs for g in out.outputs]\n",
        "    all_finish_reasons = [g.finish_reason for out in outputs for g in out.outputs]\n",
        "    inference_engine.sleep(1)\n",
        "\n",
        "    print(f\"Generated {len(all_generations)} responses\")\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    time.sleep(1)\n",
        "\n",
        "    # Process responses and calculate rewards\n",
        "    episodes, episodes_stats = create_training_episodes(\n",
        "        samples,\n",
        "        all_generations,\n",
        "        all_finish_reasons,\n",
        "    )\n",
        "    for k, v in episodes_stats.items():\n",
        "        metrics.setdefault(k, []).extend(v)\n",
        "\n",
        "    episode_table = dump_episodes(\n",
        "        episodes=episodes,\n",
        "        episodes_stats=episodes_stats,\n",
        "        exp_dir=Path(EXP_DIR),\n",
        "        tokenizer=tokenizer,\n",
        "        iteration=iteration,\n",
        "    )\n",
        "\n",
        "    #########################################################\n",
        "    # Training\n",
        "    #########################################################\n",
        "\n",
        "    # Prepare training batch\n",
        "    model_inputs = prepare_model_inputs(\n",
        "        query_token_ids=episodes[\"all_query_token_ids\"],\n",
        "        response_token_ids=episodes[\"all_response_token_ids\"],\n",
        "        advantages=episodes[\"all_advantages\"],\n",
        "        device=\"cuda\"\n",
        "    )\n",
        "\n",
        "    # Calculate losses and update model\n",
        "    policy_model.train()\n",
        "    reference_model.module.cuda()\n",
        "    reference_model.eval()\n",
        "\n",
        "    total_response_len = (model_inputs[\"labels\"] != -100).sum().item()\n",
        "\n",
        "    for i in trange(0, EPISODES_PER_ITERATION, PER_DEVICE_BATCH_SIZE, desc=\"Gradient Accumulation\"):\n",
        "        batch = {\n",
        "            k: v[i : i + PER_DEVICE_BATCH_SIZE]\n",
        "            for k, v in model_inputs.items()\n",
        "        }\n",
        "\n",
        "        # Compute policy gradient loss\n",
        "        loss, loss_metrics = compute_pg_loss(\n",
        "            policy_model=policy_model,\n",
        "            reference_model=reference_model,\n",
        "            batch=batch,\n",
        "            total_response_len=total_response_len,\n",
        "        )\n",
        "\n",
        "        # Track metrics\n",
        "        metrics.setdefault(\"loss\", []).append(loss.item())\n",
        "        grad_norm = policy_model.get_global_grad_norm()\n",
        "        if grad_norm is not None:\n",
        "            grad_norm = grad_norm.item()\n",
        "        metrics.setdefault(\"grad_norm\", []).append(grad_norm)\n",
        "        for k, v in loss_metrics.items():\n",
        "            metrics.setdefault(k, []).append(v.item() if isinstance(v, torch.Tensor) else v)\n",
        "\n",
        "        # Backpropagation and optimization step\n",
        "        policy_model.backward(loss, scale_wrt_gas=False)\n",
        "\n",
        "        # Free memory\n",
        "        del loss, loss_metrics\n",
        "        if policy_model.is_gradient_accumulation_boundary():\n",
        "            reference_model.module.cpu()\n",
        "\n",
        "        policy_model.step()\n",
        "\n",
        "    #########################################################\n",
        "    # Update inference engine weights\n",
        "    #########################################################\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    time.sleep(1)\n",
        "\n",
        "    inference_engine.wake_up()\n",
        "    load_model_into_vllm(policy_model, inference_engine)\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    time.sleep(1)\n",
        "\n",
        "\n",
        "    #########################################################\n",
        "    # Log metrics\n",
        "    #########################################################\n",
        "\n",
        "    train_metrics = {\n",
        "        k: np.mean(v) for k, v in metrics.items() if None not in v\n",
        "    }\n",
        "    train_metrics[\"learning_rate\"] = policy_model.get_lr()[0]\n",
        "    logs = {\n",
        "        \"iteration\": iteration,\n",
        "        f\"episodes/iter_{iteration:06d}\": episode_table,\n",
        "        **{f\"train/{k}\": v for k, v in train_metrics.items()},\n",
        "    }\n",
        "    if eval_stats is not None:\n",
        "        eval_metrics = {k: np.mean(v) for k, v in eval_stats.items() if None not in v}\n",
        "        logs.update({f\"eval/{k}\": v for k, v in eval_metrics.items()})\n",
        "    wandb.log(logs)\n",
        "\n",
        "    selected_keys = [\n",
        "        \"train/kl_penalty\",\n",
        "        \"train/rewards\",\n",
        "        \"train/reward_metrics/format_reward\",\n",
        "        \"train/reward_metrics/equation_reward\",\n",
        "        \"eval/rewards\",\n",
        "        \"eval/reward_metrics/format_reward\",\n",
        "        \"eval/reward_metrics/equation_reward\",\n",
        "    ]\n",
        "    selected_metrics = {k: logs[k] for k in selected_keys if k in logs}\n",
        "    print(f\"KEY METRICS: {selected_metrics}\")\n",
        "\n",
        "    if iteration % 50 == 0 and iteration != 0:\n",
        "        policy_model.module.save_pretrained(\n",
        "            str(Path(EXP_DIR) / \"checkpoints\" / f\"ckpt_{iteration:06d}\" / \"hf_model\")\n",
        "        )\n",
        "        policy_model.save_checkpoint(\n",
        "            str(Path(EXP_DIR) / \"checkpoints\" / f\"ckpt_{iteration:06d}\" / \"deepspeed\")\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piXciuf5L__z"
      },
      "source": [
        "## Citation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SBXWIglL__z"
      },
      "source": [
        "如果您在研究中使用此代码库，请引用我们，引用方式如下：\n",
        "\n",
        "```bibtex\n",
        "@misc{Kazemnejad2025:NanoAhaMoment,\n",
        "  author       = {Amirhossein Kazemnejad and Milad Aghajohari and Alessandro Sordoni and Aaron Courville and Siva Reddy},\n",
        "  title        = {Nano Aha! Moment: Lunch Break Reproduction of DeepSeek R1-Zero from Scratch},\n",
        "  year         = {2025},\n",
        "  howpublished = {\\url{https://github.com/McGill-NLP/nano-aha-moment}},\n",
        "  note         = {GitHub repository}\n",
        "}\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 fixed modules",
      "language": "python",
      "name": "python3_fixed"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}