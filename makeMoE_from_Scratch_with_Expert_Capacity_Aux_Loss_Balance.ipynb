{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weedge/doraemon-nb/blob/main/makeMoE_from_Scratch_with_Expert_Capacity_Aux_Loss_Balance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "8e9b80fc-12cf-41a9-a0de-354f678b412b",
          "showTitle": false,
          "title": ""
        },
        "id": "90vgVgmDkRJQ"
      },
      "source": [
        "#### 从头开始的稀疏专家混合语言模型，灵感来源于（并在很大程度上基于）[Andrej Karpathy的makemore项目](https://github.com/karpathy/makemore) :)\n",
        "\n",
        "这是一个从头开始实现的稀疏专家混合语言模型。这受到了Andrej Karpathy项目'makemore'的启发，并且大部分重用的组件都来自于该实现。就像makemore一样，makeMoE也是一个自回归的字符级语言模型，但是使用了上述稀疏专家的架构。\n",
        "\n",
        "与makemore体系结构相比，有显着的变化\n",
        "\n",
        "- 稀疏专家混合而不是孤立的前馈神经网络。\n",
        "- 使用了top-k门控和嘈杂的top-k门控实现。\n",
        "- 初始化 - 这里使用了Kaiming He初始化，但这个笔记本的重点是可hack性，所以你可以替换为Xavier Glorot等，并进行尝试。\n",
        "\n",
        "与makemore不变的部分\n",
        "\n",
        "- Andrej最初选择的数据集、预处理（tokenizer）和语言建模任务 - 生成类似莎士比亚的文本\n",
        "- 自注意力因果实现\n",
        "- 训练循环\n",
        "- 推断逻辑\n",
        "\n",
        "在此实现中大量引用的论文：\n",
        "\n",
        "- Mixtral of Experts：https://arxiv.org/pdf/2401.04088.pdf\n",
        "- Outrageosly Large Neural Networks: The Sparsely-Gated Mixture-Of-Experts layer：https://arxiv.org/pdf/1701.06538.pdf\n",
        "- Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity: https://arxiv.org/pdf/2101.03961.pdf\n",
        "\n",
        "这个笔记本演示了整个模型架构的直觉以及所有内容是如何相互关联的。\n",
        "\n",
        "\n",
        "请注意，该实现强调易读性和可hack性而不是性能，因此有许多方法可以改进此实现。请尝试并告诉我。\n",
        "\n",
        "如果在colab中运行，选择t4 GPU即可。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2f4a58a8-bd4c-40de-a4a9-95457842db0b",
          "showTitle": false,
          "title": ""
        },
        "id": "hywLNfb0kRJT"
      },
      "source": [
        "![mixture of experts overview](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/moe.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5e1a3e38-8717-42ec-9bbc-71d3712c1c68",
          "showTitle": false,
          "title": ""
        },
        "id": "V521QQ_qkRJV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f3a5f9a-5022-44e9-eac8-a599f09f9a28"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x79f8342f66f0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#Import the necessary packages and set seed for reproducibility. For this notebook, pytorch is all you need\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "faf99ef2-39bb-46fc-b772-05d6d0482bbc",
          "showTitle": false,
          "title": ""
        },
        "id": "-4r_QNRRkRJV"
      },
      "source": [
        "接下来的几个部分，下载数据、预处理数据和自注意力直接来自makemore。我稍微详细说明了自注意力，并添加了一些可视化辅助，以便更好地理解这个过程。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "45143d84-28c7-463d-9fb5-e21122842600",
          "showTitle": false,
          "title": ""
        },
        "id": "2GhDw0yWkRJV",
        "outputId": "a9958e9a-eabe-44c7-c951-fa010bdf214b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-09 15:09:09--  https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.005s  \n",
            "\n",
            "2024-04-09 15:09:09 (207 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Downloading the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "192e830a-762d-4573-9484-70a58deb1fec",
          "showTitle": false,
          "title": ""
        },
        "id": "3sPAL1AKkRJW"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2d7181b7-f5e5-4ab5-bdd8-74c507c798ad",
          "showTitle": false,
          "title": ""
        },
        "id": "wNkF3RYLkRJX",
        "outputId": "3ae5dd2f-d23b-49ac-9580-855f805bba1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "68032e07-8625-4750-a340-bc8f4eed2458",
          "showTitle": false,
          "title": ""
        },
        "id": "AHIwr-yxkRJX",
        "outputId": "b3b8a965-eeae-4ed7-f40f-3a5e8f0d64ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b6995ad6-c9ac-4a21-9da0-ebbd3273c991",
          "showTitle": false,
          "title": ""
        },
        "id": "DHGayz7mkRJY",
        "outputId": "f51888d4-206b-4520-ae6d-d73ccb9ec916",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ],
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "43002fa3-ffd3-416c-9aaf-0a03b19c7bc1",
          "showTitle": false,
          "title": ""
        },
        "id": "pzn11WJckRJY",
        "outputId": "a2a5d311-815b-419e-de6b-ed142780e57b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ],
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b4609fc4-09c7-4a39-8367-e9ee39d440ed",
          "showTitle": false,
          "title": ""
        },
        "id": "YbBGz0O2kRJY",
        "outputId": "ba44e142-e995-4a35-a385-53b17ef692cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ],
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "88f7cb0d-02ff-42b0-92a5-a505dc3f8f25",
          "showTitle": false,
          "title": ""
        },
        "id": "hoLIeA7YkRJZ"
      },
      "outputs": [],
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6b554ddf-50f4-441b-8acf-10b81a508b7e",
          "showTitle": false,
          "title": ""
        },
        "id": "VY55nr6EkRJZ",
        "outputId": "a6976194-20d9-4568-a7b1-5577128e45f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "22ba4512-309d-4895-a908-2ef3efa317bc",
          "showTitle": false,
          "title": ""
        },
        "id": "5YbgrB9HkRJZ",
        "outputId": "b0965755-90c2-4549-ce3c-74c7590eb3a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bf386bff-0f63-4358-82fc-6c7d02c37321",
          "showTitle": false,
          "title": ""
        },
        "id": "Oaxhage8kRJZ"
      },
      "outputs": [],
      "source": [
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "99acd85c-233f-4f2d-a062-028dbcde9960",
          "showTitle": false,
          "title": ""
        },
        "id": "HfpkIUNdkRJZ",
        "outputId": "3f54e380-63d7-4827-f8fa-04a20c846c0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([250930, 237205, 974116, 383898])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "ix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e46dc826-9f39-4aed-b2d2-f9ea401136de",
          "showTitle": false,
          "title": ""
        },
        "id": "faoGVPG3kRJa",
        "outputId": "809a7bfb-f119-4e63-b1d3-bcff2e0f3874",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[42,  1, 58, 46, 59, 57,  1, 21],\n",
              "        [54, 56, 47, 43, 57, 58, 11,  0],\n",
              "        [49, 47, 52, 45, 12,  1, 58, 46],\n",
              "        [58, 46, 53, 59, 58,  1, 56, 43]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2886aedf-200e-40bd-9a9d-4658cf6c509b",
          "showTitle": false,
          "title": ""
        },
        "id": "hkllYFCPkRJa",
        "outputId": "d751cade-6f65-4acd-ccb3-24a5f28286f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1, 58, 46, 59, 57,  1, 21,  1],\n",
              "        [56, 47, 43, 57, 58, 11,  0, 37],\n",
              "        [47, 52, 45, 12,  1, 58, 46, 53],\n",
              "        [46, 53, 59, 58,  1, 56, 43, 42]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "a486fc04-ed29-456f-918b-5f8395e455cb",
          "showTitle": false,
          "title": ""
        },
        "id": "tWrajECBkRJa"
      },
      "source": [
        "以下代码块清楚地展示了预测的自回归性质，以及上下文是对token（在本例中是字符）的一维排列的滚动窗口。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "49a86e10-ac37-4b92-8f18-775cd4853fdc",
          "showTitle": false,
          "title": ""
        },
        "id": "xjgtxxztkRJa",
        "outputId": "153e1480-caae-4c87-a547-dbda6cee02f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 6,  0, 14, 43, 44, 53, 56, 43],\n",
            "        [39,  1, 42, 59, 43,  1, 39, 52],\n",
            "        [47, 41, 43,  1, 39, 52, 42,  1],\n",
            "        [53, 44,  1, 50, 43, 58,  1, 58]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 0, 14, 43, 44, 53, 56, 43,  1],\n",
            "        [ 1, 42, 59, 43,  1, 39, 52, 42],\n",
            "        [41, 43,  1, 39, 52, 42,  1, 42],\n",
            "        [44,  1, 50, 43, 58,  1, 58, 46]])\n",
            "----\n",
            "when input is [6] the target: 0\n",
            "when input is [6, 0] the target: 14\n",
            "when input is [6, 0, 14] the target: 43\n",
            "when input is [6, 0, 14, 43] the target: 44\n",
            "when input is [6, 0, 14, 43, 44] the target: 53\n",
            "when input is [6, 0, 14, 43, 44, 53] the target: 56\n",
            "when input is [6, 0, 14, 43, 44, 53, 56] the target: 43\n",
            "when input is [6, 0, 14, 43, 44, 53, 56, 43] the target: 1\n",
            "when input is [39] the target: 1\n",
            "when input is [39, 1] the target: 42\n",
            "when input is [39, 1, 42] the target: 59\n",
            "when input is [39, 1, 42, 59] the target: 43\n",
            "when input is [39, 1, 42, 59, 43] the target: 1\n",
            "when input is [39, 1, 42, 59, 43, 1] the target: 39\n",
            "when input is [39, 1, 42, 59, 43, 1, 39] the target: 52\n",
            "when input is [39, 1, 42, 59, 43, 1, 39, 52] the target: 42\n",
            "when input is [47] the target: 41\n",
            "when input is [47, 41] the target: 43\n",
            "when input is [47, 41, 43] the target: 1\n",
            "when input is [47, 41, 43, 1] the target: 39\n",
            "when input is [47, 41, 43, 1, 39] the target: 52\n",
            "when input is [47, 41, 43, 1, 39, 52] the target: 42\n",
            "when input is [47, 41, 43, 1, 39, 52, 42] the target: 1\n",
            "when input is [47, 41, 43, 1, 39, 52, 42, 1] the target: 42\n",
            "when input is [53] the target: 44\n",
            "when input is [53, 44] the target: 1\n",
            "when input is [53, 44, 1] the target: 50\n",
            "when input is [53, 44, 1, 50] the target: 43\n",
            "when input is [53, 44, 1, 50, 43] the target: 58\n",
            "when input is [53, 44, 1, 50, 43, 58] the target: 1\n",
            "when input is [53, 44, 1, 50, 43, 58, 1] the target: 58\n",
            "when input is [53, 44, 1, 50, 43, 58, 1, 58] the target: 46\n"
          ]
        }
      ],
      "source": [
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "dde3273f-0519-4108-ba84-dfd99e020722",
          "showTitle": false,
          "title": ""
        },
        "id": "RlON_gNikRJa"
      },
      "source": [
        "### Understanding the intuition of Causal Scaled Dot Product Self Attention\n",
        "\n",
        "这段代码来自于Andrej Karpathy出色的makemore代码库，链接在仓库中。\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e435d0cf-1383-446a-9026-cd80b4266019",
          "showTitle": false,
          "title": ""
        },
        "id": "uBVWP40SkRJa"
      },
      "source": [
        "![scaled dot product self attention](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/self_attention.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "97660589-1719-48c0-ad6f-4f7c2888348a",
          "showTitle": false,
          "title": ""
        },
        "id": "i-jFgMntkRJa"
      },
      "source": [
        "提供的代码演示了自注意力的机制和基本概念，特别是关注经典的缩放点积自注意力。在这个变体中，查询、键和值矩阵都来自同一个输入序列。为了确保自回归语言生成过程的完整性，特别是在仅包含解码器的模型中，代码实现了掩码。这种掩码技术至关重要，因为它隐藏了当前标记位置后面的任何信息，从而将模型的注意力引导到序列的前面部分。这样的注意力机制称为因果自注意力。值得注意的是，稀疏专家混合模型并不局限于仅包含解码器的Transformer架构。事实上，在这个领域的许多重要工作，特别是由Shazeer等人完成的工作，都围绕着T5架构展开，该架构包含了Transformer模型中的编码器和解码器组件。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6f82ca41-a301-4a92-aed9-ba7ac3a2bf88",
          "showTitle": false,
          "title": ""
        },
        "id": "lxMSgZWGkRJb",
        "outputId": "590fcf82-bda5-409e-d100-cadc479dd10d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1) #B,T,T\n",
        "\n",
        "v = value(x) #B,T,H\n",
        "out = wei @ v # (B,T,T) @ (B,T,H) -> (B,T,H)\n",
        "#The output from this final matrix product is subsequently passsed through a linear layer as shown in the diagram above\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "49c278ec-19db-4c5d-b4a3-3bdc45c5a443",
          "showTitle": false,
          "title": ""
        },
        "id": "cA3iXggEkRJb"
      },
      "source": [
        "对因果自注意力和多头因果自注意力的代码进行泛化和模块化。多头自注意力将多个注意力头并行应用，每个注意力头专注于通道的不同部分（嵌入维度）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "608c6c9f-fb93-43ed-9580-5e782fd90d61",
          "showTitle": false,
          "title": ""
        },
        "id": "909nX3PHkRJb"
      },
      "outputs": [],
      "source": [
        "#Causal scaled dot product self-Attention Head\n",
        "\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "head_size = 16\n",
        "dropout = 0.1\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6e8b31af-f45a-4066-8288-fb0d9c8e2aff",
          "showTitle": false,
          "title": ""
        },
        "id": "T3MoVK_WkRJb"
      },
      "outputs": [],
      "source": [
        "#Multi-Headed Self Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "16267e9a-008b-46e3-82ce-2ae41396a1a1",
          "showTitle": false,
          "title": ""
        },
        "id": "T-w53_mSkRJb",
        "outputId": "71e72213-ce85-4151-9134-407ec3a2d31d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "#Confirming that what's output from multi head attention is the original embedding size\n",
        "B,T,C = 4,8,64 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "mha = MultiHeadAttention(4,16)\n",
        "mha(x).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "5f7ff128-7fe5-4a91-b9f2-208e2132e505",
          "showTitle": false,
          "title": ""
        },
        "id": "wNlJTtfhkRJb"
      },
      "source": [
        "### Creating an Expert module i.e. a simple Multi Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "f6e422a5-57c1-4b2f-b7b9-2757e109848a",
          "showTitle": false,
          "title": ""
        },
        "id": "zv3fGRpbkRJb"
      },
      "source": [
        "在稀疏专家（MoE）架构中，每个Transformer块内部的自注意力机制保持不变。然而，每个块的结构发生了显著变化：标准的前馈神经网络被替换为几个稀疏激活的前馈网络，称为专家。 \"稀疏激活\" 指的是序列中的每个标记仅被路由到总池中的有限数量的这些专家之一或两个 - 通常是一个或两个。这种修改允许对输入数据的不同部分进行专门处理，使模型能够有效地处理更广泛的复杂性。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "efe9fdcc-82eb-4047-9233-ad3cfe8759b1",
          "showTitle": false,
          "title": ""
        },
        "id": "7Kz0Y_P0kRJc"
      },
      "source": [
        "![experts](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/experts.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a2f0f382-ab4a-45e1-9dce-27ae0d3da641",
          "showTitle": false,
          "title": ""
        },
        "id": "a-9CYWXgkRJc"
      },
      "outputs": [],
      "source": [
        "#Expert module\n",
        "class Expert(nn.Module):\n",
        "    \"\"\" An MLP is a simple linear layer followed by a non-linearity i.e. each Expert \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "a7764385-26e9-4d75-9aa7-ce011023e24e",
          "showTitle": false,
          "title": ""
        },
        "id": "qderdEuykRJc"
      },
      "source": [
        "### Top-k Gating Intuition through an Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "d3fca4df-4c47-4e9a-98cd-08cf8ccf7726",
          "showTitle": false,
          "title": ""
        },
        "id": "VxJv5y44kRJc"
      },
      "source": [
        "![top k gating](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/topk.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "8e494b86-cdb2-4f2a-8824-5fa2ef4b2606",
          "showTitle": false,
          "title": ""
        },
        "id": "n6DuhY0DkRJc"
      },
      "source": [
        "门控网络，也称为路由器，确定每个token从多头注意力中由哪个专家网络接收输出。让我们考虑一个简单的例子：假设有4个专家，并且要将标记路由到前2个专家。最初，我们通过一个线性层将token输入到门控网络中。这个层将输入张量从形状为（2，4，32）——表示（批量大小，tokens，n_embed，其中n_embed是输入的通道维度）——投影到一个新形状为（2，4，4）的张量，对应于（批量大小，tokens，num_experts），其中num_experts是专家网络的数量。接下来，我们确定最后一维中前k=2个最高值及其相应的索引。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "621916ff-2290-4e2f-9fd7-5181ed98d540",
          "showTitle": false,
          "title": ""
        },
        "id": "pNAuFDDvkRJc",
        "outputId": "f7bbe07c-9c1d-43fe-bcdd-3612247e7ec9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 0.9558,  0.1610],\n",
              "          [ 0.8659, -0.1494],\n",
              "          [ 0.8765,  0.7202],\n",
              "          [ 0.9496, -0.6609]],\n",
              " \n",
              "         [[ 0.4419, -0.2500],\n",
              "          [ 1.2602,  0.8430],\n",
              "          [ 0.8570,  0.7822],\n",
              "          [ 0.7376,  0.2561]]], grad_fn=<TopkBackward0>),\n",
              " tensor([[[2, 0],\n",
              "          [3, 2],\n",
              "          [3, 0],\n",
              "          [1, 2]],\n",
              " \n",
              "         [[1, 3],\n",
              "          [1, 2],\n",
              "          [1, 2],\n",
              "          [0, 1]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "#Understanding how gating works\n",
        "num_experts = 4\n",
        "top_k=2\n",
        "n_embed=32\n",
        "\n",
        "\n",
        "#Example multi-head attention output for a simple illustrative example, consider n_embed=32, context_length=4 and batch_size=2\n",
        "mh_output = torch.randn(2, 4, n_embed)\n",
        "\n",
        "topkgate_linear = nn.Linear(n_embed, num_experts) # nn.Linear(32, 4)\n",
        "\n",
        "logits = topkgate_linear(mh_output)\n",
        "top_k_logits, top_k_indices = logits.topk(top_k, dim=-1)  # Get top-k experts\n",
        "top_k_logits, top_k_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "0f135ff7-0aa3-4b6d-ab5e-42399c48427b",
          "showTitle": false,
          "title": ""
        },
        "id": "EKwAyJxrkRJd"
      },
      "source": [
        "通过仅保留沿着最后一个维度的各自索引处的前k个值，获取稀疏门控输出。用'-inf'填充其余部分，并通过softmax激活函数传递。这将'-inf'值推向零，使前两个值更加突出，并且总和为1。这种总和为1有助于对专家输出进行加权。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "735e160a-ef1e-424d-b6d9-09f63ea99ec1",
          "showTitle": false,
          "title": ""
        },
        "id": "IiVejzOpkRJd",
        "outputId": "1066e0a6-79dc-452f-a514-ec0fc262d5f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1610,    -inf,  0.9558,    -inf],\n",
              "         [   -inf,    -inf, -0.1494,  0.8659],\n",
              "         [ 0.7202,    -inf,    -inf,  0.8765],\n",
              "         [   -inf,  0.9496, -0.6609,    -inf]],\n",
              "\n",
              "        [[   -inf,  0.4419,    -inf, -0.2500],\n",
              "         [   -inf,  1.2602,  0.8430,    -inf],\n",
              "         [   -inf,  0.8570,  0.7822,    -inf],\n",
              "         [ 0.7376,  0.2561,    -inf,    -inf]]], grad_fn=<ScatterBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "zeros = torch.full_like(logits, float('-inf')) #full_like clones a tensor and fills it with a specified value (like infinity) for masking or calculations.\n",
        "sparse_logits = zeros.scatter(-1, top_k_indices, top_k_logits)\n",
        "sparse_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "9146e6f9-4eee-4a8b-8338-55072719ed59",
          "showTitle": false,
          "title": ""
        },
        "id": "HFgRxDF4kRJh",
        "outputId": "fa7f562d-fb78-48d4-afc1-fd35a6c26ad0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.3111, 0.0000, 0.6889, 0.0000],\n",
              "         [0.0000, 0.0000, 0.2660, 0.7340],\n",
              "         [0.4610, 0.0000, 0.0000, 0.5390],\n",
              "         [0.0000, 0.8335, 0.1665, 0.0000]],\n",
              "\n",
              "        [[0.0000, 0.6664, 0.0000, 0.3336],\n",
              "         [0.0000, 0.6028, 0.3972, 0.0000],\n",
              "         [0.0000, 0.5187, 0.4813, 0.0000],\n",
              "         [0.6181, 0.3819, 0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "gating_output= F.softmax(sparse_logits, dim=-1)\n",
        "gating_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "fe558b12-e443-4b62-9a85-c59120456352",
          "showTitle": false,
          "title": ""
        },
        "id": "dGJrq2uqkRJh"
      },
      "source": [
        "### Generalizing and Modularizing above code and adding noisy top-k Gating for load balancing\n",
        "泛化和模块化上述代码，并添加嘈杂的top-k门控以实现负载平衡。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "45516b59-d814-4853-a34e-d36aae9f04eb",
          "showTitle": false,
          "title": ""
        },
        "id": "TKp4DqwYkRJh"
      },
      "outputs": [],
      "source": [
        "# First define the top k router module\n",
        "class TopkRouter(nn.Module):\n",
        "    def __init__(self, n_embed, num_experts, top_k):\n",
        "        super(TopkRouter, self).__init__()\n",
        "        self.top_k = top_k\n",
        "        self.linear =nn.Linear(n_embed, num_experts)\n",
        "\n",
        "    def forward(self, mh_ouput):\n",
        "        # mh_ouput is the output tensor from multihead self attention block\n",
        "        logits = self.linear(mh_output)\n",
        "        top_k_logits, indices = logits.topk(self.top_k, dim=-1)\n",
        "        zeros = torch.full_like(logits, float('-inf'))\n",
        "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
        "        router_output = F.softmax(sparse_logits, dim=-1)\n",
        "        return router_output, indices\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c500844f-0866-4bbf-acef-c0c1d4979721",
          "showTitle": false,
          "title": ""
        },
        "id": "KjkouzwkkRJh",
        "outputId": "fa50e509-eef9-4016-8000-9ad34717cd83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 4, 4]),\n",
              " tensor([[[0.0000, 0.4249, 0.5751, 0.0000],\n",
              "          [0.3467, 0.6533, 0.0000, 0.0000],\n",
              "          [0.3970, 0.0000, 0.6030, 0.0000],\n",
              "          [0.0000, 0.0000, 0.7713, 0.2287]],\n",
              " \n",
              "         [[0.4043, 0.5957, 0.0000, 0.0000],\n",
              "          [0.0000, 0.5281, 0.0000, 0.4719],\n",
              "          [0.7053, 0.0000, 0.2947, 0.0000],\n",
              "          [0.0000, 0.4602, 0.5398, 0.0000]]], grad_fn=<SoftmaxBackward0>),\n",
              " tensor([[[2, 1],\n",
              "          [1, 0],\n",
              "          [2, 0],\n",
              "          [2, 3]],\n",
              " \n",
              "         [[1, 0],\n",
              "          [1, 3],\n",
              "          [0, 2],\n",
              "          [2, 1]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "#Testing this out:\n",
        "num_experts = 4\n",
        "top_k = 2\n",
        "n_embd = 32\n",
        "\n",
        "mh_output = torch.randn(2, 4, n_embd)  # Example input\n",
        "top_k_gate = TopkRouter(n_embd, num_experts, top_k)\n",
        "gating_output, indices = top_k_gate(mh_output)\n",
        "gating_output.shape, gating_output, indices\n",
        "#And it works!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "9fa02d0c-3688-4d01-811d-d0b2b851ab33",
          "showTitle": false,
          "title": ""
        },
        "id": "tAouN-GwkRJi"
      },
      "source": [
        "虽然最近发布的Mixtral论文没有提到，但我认为嘈杂的top-k门控是训练MoE模型的重要工具。基本上，您不希望所有的token都被发送到同一组“偏爱”的专家中。您希望在开发和探索之间达到良好的平衡。为此，为门控线性层的logits添加标准正态噪声有助于负载平衡，使训练更加高效。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "e05b3306-b89f-4ebc-901b-f16398a925c2",
          "showTitle": false,
          "title": ""
        },
        "id": "aWeueE83kRJi"
      },
      "source": [
        "![noisy top-k gating](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/noisytopkgating.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "dda4d805-373c-48f7-9037-da08fbc06e64",
          "showTitle": false,
          "title": ""
        },
        "id": "ZBrN-w3JkRJi"
      },
      "outputs": [],
      "source": [
        "#Changing the above to accomodate noisy top-k gating\n",
        "class NoisyTopkRouter(nn.Module):\n",
        "    def __init__(self, n_embed, num_experts, top_k):\n",
        "        super(NoisyTopkRouter, self).__init__()\n",
        "        self.top_k = top_k\n",
        "        #layer for router logits\n",
        "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
        "        self.noise_linear =nn.Linear(n_embed, num_experts)\n",
        "\n",
        "\n",
        "    def forward(self, mh_output):\n",
        "        # mh_ouput is the output tensor from multihead self attention block\n",
        "        logits = self.topkroute_linear(mh_output)\n",
        "\n",
        "        #Noise logits\n",
        "        noise_logits = self.noise_linear(mh_output)\n",
        "\n",
        "        #Adding scaled unit gaussian noise to the logits\n",
        "        noise = torch.randn_like(logits)*F.softplus(noise_logits)\n",
        "        noisy_logits = logits + noise\n",
        "\n",
        "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
        "        zeros = torch.full_like(noisy_logits, float('-inf'))\n",
        "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
        "        router_output = F.softmax(sparse_logits, dim=-1)\n",
        "        return router_output, indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a01a9d6b-fedb-427d-b0da-c3b2a75a8643",
          "showTitle": false,
          "title": ""
        },
        "id": "7Q6KcH9AkRJi",
        "outputId": "143490d3-24c8-46a9-a412-0f51c337b6d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 4, 8]),\n",
              " tensor([[[0.0000, 0.7710, 0.0000, 0.0000, 0.0000, 0.2290, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.3704, 0.6296, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.5412, 0.0000, 0.4588, 0.0000],\n",
              "          [0.6004, 0.0000, 0.0000, 0.0000, 0.0000, 0.3996, 0.0000, 0.0000]],\n",
              " \n",
              "         [[0.4171, 0.0000, 0.0000, 0.0000, 0.0000, 0.5829, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.5208, 0.0000, 0.0000, 0.4792],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.7725, 0.0000, 0.0000, 0.2275],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.8713, 0.0000, 0.0000, 0.1287]]],\n",
              "        grad_fn=<SoftmaxBackward0>),\n",
              " tensor([[[1, 5],\n",
              "          [4, 3],\n",
              "          [4, 6],\n",
              "          [0, 5]],\n",
              " \n",
              "         [[5, 0],\n",
              "          [4, 7],\n",
              "          [4, 7],\n",
              "          [4, 7]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "#Testing this out, again:\n",
        "num_experts = 8\n",
        "top_k = 2\n",
        "n_embd = 16\n",
        "\n",
        "mh_output = torch.randn(2, 4, n_embd)  # Example input\n",
        "noisy_top_k_gate = NoisyTopkRouter(n_embd, num_experts, top_k)\n",
        "gating_output, indices = noisy_top_k_gate(mh_output)\n",
        "gating_output.shape, gating_output, indices\n",
        "#It works!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "076fa004-a165-42a7-b729-0bca8ad39418",
          "showTitle": false,
          "title": ""
        },
        "id": "XyKjpR-dkRJi"
      },
      "source": [
        "\n",
        "### Creating a sparse Mixture of Experts module\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "6747b8de-0086-4cb0-8fbd-46ee95457eb9",
          "showTitle": false,
          "title": ""
        },
        "id": "UsRCy7i3kRJi"
      },
      "source": [
        "这个过程的主要方面涉及门控网络的输出。在获得这些结果后，会选择性地将前k个值与相应的前k个专家的输出相乘，以获得给定token的结果。这种选择性的乘法形成了加权求和，构成了SparseMoe块的输出。这个过程中的关键和具有挑战性的部分是避免不必要的乘法。只对前k个专家进行前向传播，然后计算这个加权和是至关重要的。对每个专家都进行前向传播会违背使用稀疏MoE的初衷，因为它将不再是稀疏的。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d6809b3f-4be9-4859-b39e-24fcdd6c8d86",
          "showTitle": false,
          "title": ""
        },
        "id": "7dDUHU_IkRJi"
      },
      "outputs": [],
      "source": [
        "class SparseMoE(nn.Module):\n",
        "    def __init__(self, n_embed, num_experts, top_k):\n",
        "        super(SparseMoE, self).__init__()\n",
        "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
        "        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def forward(self, x):\n",
        "        gating_output, indices = self.router(x)\n",
        "        final_output = torch.zeros_like(x)\n",
        "\n",
        "        # Reshape inputs for batch processing\n",
        "        flat_x = x.view(-1, x.size(-1))\n",
        "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
        "\n",
        "        # Process each expert in parallel\n",
        "        for i, expert in enumerate(self.experts):\n",
        "            # Create a mask for the inputs where the current expert is in top-k\n",
        "            expert_mask = (indices == i).any(dim=-1)\n",
        "            flat_mask = expert_mask.view(-1)\n",
        "\n",
        "            if flat_mask.any():\n",
        "                expert_input = flat_x[flat_mask]\n",
        "                expert_output = expert(expert_input)\n",
        "\n",
        "                # Extract and apply gating scores\n",
        "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
        "                weighted_output = expert_output * gating_scores\n",
        "\n",
        "                # Update final output\n",
        "                # We need to scatter_add the weighted outputs to their original positions in the batch\n",
        "                final_output.masked_scatter_(expert_mask.unsqueeze(-1), weighted_output)\n",
        "\n",
        "        return final_output.view_as(x)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "06239630-0a1c-47c9-976c-7770f3d82e18",
          "showTitle": false,
          "title": ""
        },
        "id": "q8kDLI1ukRJj",
        "outputId": "2dd90c46-2366-43a9-abf6-6c61d1d95838",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the final output: torch.Size([4, 8, 16])\n",
            "tensor([[[-7.2254e-02,  2.5299e-01, -1.2675e-01,  1.3837e-01,  5.4608e-02,\n",
            "           0.0000e+00, -7.7531e-02, -3.5290e-02, -1.4103e-02, -7.2050e-02,\n",
            "          -3.9991e-02, -0.0000e+00,  7.5821e-02,  2.1454e-02,  1.2398e-01,\n",
            "           5.4608e-02],\n",
            "         [-2.3598e-02, -2.6230e-02, -5.5828e-02, -5.5061e-02, -1.0413e-01,\n",
            "           8.5451e-02, -7.5070e-02, -0.0000e+00, -0.0000e+00, -4.5918e-02,\n",
            "           9.0797e-02, -1.0355e-01, -3.5142e-03,  8.1981e-02,  1.5646e-02,\n",
            "           3.1881e-02],\n",
            "         [-0.0000e+00,  1.2757e-01,  4.8477e-01,  2.2833e-02, -7.3061e-02,\n",
            "           6.9625e-02,  1.0888e-01,  8.7231e-04,  3.8779e-01, -6.9699e-02,\n",
            "          -1.0221e-01,  7.9689e-03, -3.9457e-01, -4.9123e-02, -2.2001e-01,\n",
            "          -0.0000e+00],\n",
            "         [-7.1534e-02, -1.1351e-01, -1.2371e-01, -1.3362e-01, -1.5074e-01,\n",
            "           1.5168e-01, -5.7698e-02, -1.0665e-01, -4.2614e-01, -1.0385e-01,\n",
            "           2.8249e-01, -3.8319e-01,  2.2413e-01,  1.8188e-01, -7.9379e-03,\n",
            "           1.6972e-01],\n",
            "         [-8.5017e-02,  1.9789e-01,  2.2419e-02, -9.6312e-02,  8.4114e-02,\n",
            "           8.6022e-02, -6.4299e-02, -0.0000e+00, -1.6287e-01, -5.4725e-02,\n",
            "          -4.1535e-02,  7.6786e-02, -1.1117e-01,  1.0696e-02, -2.2312e-01,\n",
            "           4.3087e-01],\n",
            "         [ 6.7270e-02,  1.9040e-02, -2.0836e-02,  3.6515e-02, -0.0000e+00,\n",
            "           4.2801e-02, -9.1316e-02, -2.0775e-02, -0.0000e+00, -8.8279e-02,\n",
            "          -2.1346e-02, -7.0689e-02,  0.0000e+00,  3.3134e-02, -8.1464e-02,\n",
            "          -2.0062e-02],\n",
            "         [-1.2017e-01,  7.1747e-02, -9.1265e-02,  7.2103e-02, -5.1611e-02,\n",
            "          -8.8643e-03, -1.9791e-01, -3.0405e-02,  3.6690e-02,  0.0000e+00,\n",
            "           1.6196e-02, -4.3344e-02, -4.6347e-02,  4.4744e-02,  1.6162e-02,\n",
            "          -5.8193e-02],\n",
            "         [ 6.1205e-02, -0.0000e+00, -1.2888e-01,  2.3773e-01,  1.1169e-01,\n",
            "          -6.8608e-03, -1.0914e-01,  1.2461e-01, -1.9423e-02,  2.6779e-02,\n",
            "           1.0085e-01,  3.5747e-02, -4.6356e-02,  3.3008e-02,  1.7307e-01,\n",
            "           7.0341e-02]],\n",
            "\n",
            "        [[-2.2072e-01,  1.5014e-01,  0.0000e+00,  1.8089e-01, -1.4623e-01,\n",
            "           0.0000e+00,  3.9562e-02,  0.0000e+00, -1.4275e-01,  2.0661e-01,\n",
            "          -0.0000e+00,  0.0000e+00,  1.7242e-01, -1.6202e-01,  1.5920e-01,\n",
            "          -7.5738e-02],\n",
            "         [-1.8034e-02, -4.0166e-02, -2.4314e-02, -1.2438e-02, -3.0155e-02,\n",
            "           1.4490e-02,  7.7188e-02, -2.3432e-02,  6.5127e-02, -3.6726e-02,\n",
            "           2.3018e-02, -3.2729e-02, -3.8020e-02,  1.3063e-01,  0.0000e+00,\n",
            "          -1.4322e-01],\n",
            "         [-1.2268e-03, -3.8461e-02,  0.0000e+00,  1.2173e-01, -6.1155e-02,\n",
            "          -3.6920e-02, -0.0000e+00,  0.0000e+00,  0.0000e+00,  1.4362e-01,\n",
            "           2.4042e-02,  4.9268e-02, -5.8924e-02,  1.2180e-01, -4.1180e-02,\n",
            "          -1.1900e-01],\n",
            "         [ 2.8980e-01,  1.2094e-01,  7.4835e-03, -0.0000e+00, -8.0925e-02,\n",
            "          -2.2369e-01,  2.0895e-01, -2.9814e-03,  1.6673e-01, -7.0064e-02,\n",
            "          -1.8148e-01,  2.1523e-01, -2.7033e-02, -2.8037e-02,  3.1835e-01,\n",
            "          -4.1519e-01],\n",
            "         [ 1.1143e-01,  4.6873e-03,  1.2217e-02,  5.5604e-02, -3.1156e-03,\n",
            "           8.9352e-02, -9.4345e-02, -4.2256e-02,  0.0000e+00, -1.3325e-02,\n",
            "           2.9701e-02, -1.8646e-02,  5.9456e-03,  1.1981e-01, -1.2936e-01,\n",
            "          -9.7802e-02],\n",
            "         [ 1.1070e-01,  2.8252e-02,  1.1670e-02, -4.4569e-02, -5.2687e-02,\n",
            "           6.0709e-02, -1.4263e-02, -8.5667e-02, -2.6391e-02, -4.5153e-02,\n",
            "           1.6210e-02, -0.0000e+00, -8.1045e-03,  1.8825e-02, -7.1831e-02,\n",
            "          -1.0774e-01],\n",
            "         [ 9.5493e-03,  8.7731e-02,  2.2026e-01,  0.0000e+00, -6.5836e-02,\n",
            "           8.1913e-02,  2.6249e-01,  8.4140e-02,  2.4698e-01,  9.7860e-02,\n",
            "          -2.0754e-03,  5.9234e-02,  2.1257e-02,  1.2319e-01,  0.0000e+00,\n",
            "          -2.0183e-01],\n",
            "         [-1.0350e-01,  3.1976e-01, -0.0000e+00, -5.2035e-02, -9.3496e-02,\n",
            "          -3.7147e-03, -0.0000e+00, -1.3588e-01, -1.2909e-01, -1.1285e-01,\n",
            "           6.2896e-02, -0.0000e+00, -1.0194e-01,  0.0000e+00,  9.9133e-02,\n",
            "           2.1341e-01]],\n",
            "\n",
            "        [[ 8.5853e-02,  0.0000e+00,  2.0908e-01,  2.9029e-02,  3.1522e-02,\n",
            "           1.2252e-01, -2.3888e-01,  1.0177e-01,  2.3374e-02,  9.6050e-02,\n",
            "          -1.2931e-01,  5.2198e-02,  1.1031e-01,  2.5168e-02,  0.0000e+00,\n",
            "          -5.5292e-02],\n",
            "         [ 9.1719e-02,  6.6978e-02,  1.2514e-01, -2.4588e-01, -8.6732e-02,\n",
            "          -3.9352e-02, -1.3936e-01, -1.2892e-01, -0.0000e+00,  2.5207e-02,\n",
            "          -7.1028e-02,  1.0194e-01, -3.0185e-01, -2.1280e-02, -3.4810e-02,\n",
            "          -0.0000e+00],\n",
            "         [-7.4532e-02,  2.6053e-02, -5.0679e-02,  1.2069e-01, -0.0000e+00,\n",
            "          -5.1638e-02,  1.9835e-01,  6.1755e-02, -5.3726e-02, -1.0879e-01,\n",
            "           6.4912e-02, -1.3584e-01,  1.7602e-03, -0.0000e+00, -2.6012e-02,\n",
            "           1.1523e-01],\n",
            "         [-1.1687e-01,  0.0000e+00, -1.5683e-03,  8.4167e-02, -1.1444e-01,\n",
            "           1.3664e-01,  3.5675e-02, -1.2570e-01, -9.9478e-02, -8.3631e-02,\n",
            "           6.6776e-02, -1.2128e-02, -9.4283e-02,  3.3217e-02, -2.2122e-03,\n",
            "           1.1870e-01],\n",
            "         [ 8.6710e-02,  2.9624e-01,  2.3303e-01, -4.5661e-01,  4.0395e-01,\n",
            "          -2.6222e-01, -0.0000e+00, -3.9774e-01,  7.5072e-02, -0.0000e+00,\n",
            "           3.2789e-01,  4.7911e-03, -1.5937e-01,  1.1006e-01,  1.1681e-01,\n",
            "           1.9445e-01],\n",
            "         [-0.0000e+00,  1.5005e-01,  9.1860e-02,  6.0557e-02, -8.4211e-02,\n",
            "          -1.4597e-02, -2.1061e-02, -1.2925e-01,  2.0255e-01,  3.2319e-02,\n",
            "          -1.5214e-03, -1.0031e-03,  0.0000e+00, -1.9398e-02, -6.7063e-02,\n",
            "           0.0000e+00],\n",
            "         [ 0.0000e+00,  2.5152e-01,  3.6323e-02,  1.2560e-01,  1.9987e-02,\n",
            "          -2.6218e-01, -4.3272e-01,  1.7963e-01,  3.8330e-03,  1.0061e-02,\n",
            "           4.4919e-01, -9.9248e-02, -7.3683e-02,  1.5681e-01,  1.9763e-02,\n",
            "          -1.2950e-02],\n",
            "         [ 1.0938e-01,  2.0943e-02,  1.4187e-01,  1.7703e-01,  1.3331e-01,\n",
            "           4.7314e-02,  1.7895e-01,  1.3852e-01,  2.0557e-01,  4.8969e-02,\n",
            "           3.5683e-02, -7.9485e-02,  7.0411e-02,  4.6915e-02, -2.0676e-01,\n",
            "          -8.7945e-02]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.6407e-01,  7.0756e-03, -5.6912e-02, -3.5089e-02,\n",
            "          -2.8014e-03, -6.8886e-02, -7.6755e-02, -8.6484e-02, -1.0582e-01,\n",
            "          -8.8473e-02,  7.1095e-02,  9.7264e-04,  1.4415e-02, -5.1183e-02,\n",
            "          -6.1982e-02],\n",
            "         [-8.4555e-02,  2.0778e-01, -3.0550e-02, -7.5457e-02,  1.8322e-01,\n",
            "          -6.9650e-02, -5.0651e-02, -1.8187e-01,  8.1128e-02,  8.7934e-02,\n",
            "          -3.2624e-02, -4.3434e-02, -1.1912e-01,  7.6655e-02,  0.0000e+00,\n",
            "           1.6574e-01],\n",
            "         [ 3.2726e-02,  3.8343e-01,  7.6917e-03, -5.1060e-02,  8.7633e-02,\n",
            "          -1.0676e-02, -1.0997e-02,  2.8567e-01,  3.3682e-02,  2.0725e-01,\n",
            "          -1.7203e-01,  1.4722e-01,  8.3326e-02, -2.1138e-01, -1.7972e-01,\n",
            "           5.5533e-03],\n",
            "         [ 4.4156e-01,  6.1723e-02, -4.1239e-02,  0.0000e+00, -3.2148e-01,\n",
            "           2.8945e-01, -2.0789e-01,  2.5529e-01,  2.0292e-01, -1.0053e-01,\n",
            "          -2.7526e-01,  1.2771e-01,  1.5159e-01,  1.2918e-01,  4.2344e-01,\n",
            "          -1.9489e-01],\n",
            "         [-1.6963e-01,  1.9187e-01,  4.7502e-02,  6.4914e-02,  2.1529e-01,\n",
            "          -2.3957e-01, -0.0000e+00, -2.8339e-01,  2.2684e-01,  1.9707e-01,\n",
            "           1.1746e-01,  4.2987e-04,  7.0439e-02, -2.2493e-02,  1.7136e-01,\n",
            "           1.6583e-01],\n",
            "         [-0.0000e+00, -1.2184e-01,  9.3158e-02,  1.0737e-01, -1.6752e-01,\n",
            "          -2.5672e-01, -1.5003e-01, -1.1505e-03,  1.0666e-01,  0.0000e+00,\n",
            "           1.9440e-01,  5.1719e-02,  4.1660e-02,  7.7480e-02, -5.6354e-02,\n",
            "          -1.5449e-01],\n",
            "         [ 7.3280e-04, -3.7157e-02,  8.4850e-02, -1.4770e-01,  5.7505e-02,\n",
            "           8.7246e-02, -9.4376e-02,  6.5040e-02, -9.5540e-02, -0.0000e+00,\n",
            "          -1.9724e-02,  8.1915e-02, -5.5435e-02, -2.2812e-02, -1.1856e-01,\n",
            "           2.0728e-01],\n",
            "         [ 5.0384e-02,  2.1565e-01,  0.0000e+00,  0.0000e+00,  1.5790e-01,\n",
            "          -2.0157e-02,  6.8297e-01,  1.6673e-01,  5.3931e-01,  2.2785e-01,\n",
            "          -1.9977e-01, -9.3917e-02, -1.9042e-01, -7.1522e-02,  9.2548e-02,\n",
            "          -5.0016e-01]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#Let's test this out\n",
        "num_experts = 8\n",
        "top_k = 2\n",
        "n_embd = 16\n",
        "dropout=0.1\n",
        "\n",
        "mh_output = torch.randn(4, 8, n_embd)  # Example multi-head attention output\n",
        "sparse_moe = SparseMoE(n_embd, num_experts, top_k)\n",
        "final_output = sparse_moe(mh_output)\n",
        "print(\"Shape of the final output:\", final_output.shape)\n",
        "print(final_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "7476ca07-a315-4108-aa77-46173a703ca2",
          "showTitle": false,
          "title": ""
        },
        "id": "l7GoxCi2kRJj"
      },
      "source": [
        "强调一下，需要认识到路由器/门控网络输出的前k个专家的幅值，正如上面的代码所示，也是非常重要的。这些前k个索引确定了被激活的专家，而在这些前k个维度中数值的大小决定了它们各自的权重。这种加权求和的概念在下面的图示中进一步强调了。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "d99b5dce-301e-4380-8263-b5cfb4136ab2",
          "showTitle": false,
          "title": ""
        },
        "id": "e9a_oQ2akRJj"
      },
      "source": [
        "![sparse MoE](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/sparseMoEfinal.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 引入专家容量 （Expert Capacity factor）\n",
        "\n",
        "from: https://huggingface.co/blog/AviSoori1x/makemoe2\n",
        "\n",
        "\n",
        "\n",
        "在预训练混合专家语言模型或任何大型语言模型时，该过程通常跨越多个GPU，并且通常涉及许多机器。跨这些硬件资源并行训练的方式对于平衡计算负载至关重要。然而，如果某些专家或一组专家过度受到偏爱——反映出对开发的偏好超过探索——它不仅可能导致模型中的性能问题，还可能导致集群中的计算负载不平衡。\n",
        "\n",
        "[Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) 实现使用专家容量来规避这个问题。专家容量确定每个专家在训练或推理过程中负责处理多少个标记。它是基于批次中的标记数和可用专家的数量定义的，通常通过容量因子进行调整。该因子允许在分配中灵活性，提供缓冲区以考虑数据分布的变化，并确保没有单个专家由于过载而成为瓶颈。在训练这些大型模型时，硬件故障是很常见的，可能持续数周甚至数月，因此这一点非常重要。\n",
        "\n",
        "以下是专家容量通常计算的方式：\n",
        "\n",
        "专家容量 = （每批标记数 / 专家数量）× 容量因子 其中：\n",
        "```python\n",
        "expert_capacity = int((tokens_per_batch / self.num_experts) * self.capacity_factor)\n",
        "```\n",
        "\n",
        "- 每批标记数`tokens_per_batch`是需要处理的批次中存在的总标记数。\n",
        "- 专家数量`num_experts`是MoE层中可用于处理数据的专家总数。\n",
        "- 容量因子`capacity_factor`是用于调整基础容量（每批标记数除以专家数量）的乘数。大于1的容量因子允许每个专家处理超出均匀分配份额的缓冲区，适应标记分配的不平衡。该值的一般范围为1-1.25。\n",
        "\n",
        "以下代码块进行了轻微调整，以实现专家容量的简单版本。"
      ],
      "metadata": {
        "id": "Xzd2KhXDnHMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SparseMoE(nn.Module):\n",
        "    def __init__(self, n_embed, num_experts, top_k, capacity_factor=1.0):\n",
        "        super(SparseMoE, self).__init__()\n",
        "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
        "        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
        "        self.top_k = top_k\n",
        "        # add capacity_factor\n",
        "        self.capacity_factor = capacity_factor\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "    def forward(self, x):\n",
        "    # Assuming x has shape [batch_size, seq_len, n_embd]\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        gating_output, indices = self.router(x)\n",
        "        final_output = torch.zeros_like(x)\n",
        "\n",
        "        # Flatten the batch and sequence dimensions to treat each token independently\n",
        "        flat_x = x.view(-1, x.size(-1))  # Now shape [batch_size * seq_len, n_embd]\n",
        "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
        "\n",
        "        tokens_per_batch = batch_size * seq_len * self.top_k\n",
        "        expert_capacity = int((tokens_per_batch / self.num_experts) * self.capacity_factor)\n",
        "\n",
        "        updates = torch.zeros_like(flat_x)\n",
        "\n",
        "        for i, expert in enumerate(self.experts):\n",
        "            expert_mask = (indices == i).any(dim=-1)\n",
        "            flat_mask = expert_mask.view(-1)\n",
        "            selected_indices = torch.nonzero(flat_mask).squeeze(-1)\n",
        "\n",
        "            limited_indices = selected_indices[:expert_capacity] if selected_indices.numel() > expert_capacity else selected_indices\n",
        "            if limited_indices.numel() > 0:\n",
        "                expert_input = flat_x[limited_indices]\n",
        "                expert_output = expert(expert_input)\n",
        "\n",
        "                gating_scores = flat_gating_output[limited_indices, i].unsqueeze(1)\n",
        "                weighted_output = expert_output * gating_scores\n",
        "\n",
        "                updates.index_add_(0, limited_indices, weighted_output)\n",
        "\n",
        "        # Reshape updates to match the original dimensions of x\n",
        "        final_output += updates.view(batch_size, seq_len, -1)\n",
        "\n",
        "        return final_output\n"
      ],
      "metadata": {
        "id": "HjoVac7Rood3"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#Let's test this out\n",
        "num_experts = 8\n",
        "top_k = 2\n",
        "n_embd = 16\n",
        "dropout=0.1\n",
        "\n",
        "mh_output = torch.randn(4, 8, n_embd)  # Example multi-head attention output\n",
        "sparse_moe = SparseMoE(n_embd, num_experts, top_k)\n",
        "final_output = sparse_moe(mh_output)\n",
        "print(\"Shape of the final output:\", final_output.shape)\n",
        "print(final_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HelPhgxrosLK",
        "outputId": "5d6d60e9-6358-4cdc-9e30-fb18680a7038"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the final output: torch.Size([4, 8, 16])\n",
            "tensor([[[ 1.1761e-01, -2.3379e-02, -4.7678e-01,  9.5960e-03, -1.1987e-01,\n",
            "           2.1115e-01,  1.2021e-01, -4.8881e-02,  2.6757e-01, -5.7497e-02,\n",
            "          -1.7639e-01,  2.4494e-01, -2.7037e-02, -2.1677e-03,  8.5174e-02,\n",
            "          -3.0657e-02],\n",
            "         [ 6.2244e-02, -3.8118e-02,  4.6825e-02,  2.3763e-02, -1.3538e-01,\n",
            "          -7.2525e-03,  3.6083e-01, -2.0759e-01,  2.2128e-01,  8.7651e-02,\n",
            "           3.9775e-01,  1.0490e-01,  7.7816e-02, -1.6218e-01,  5.7070e-02,\n",
            "          -1.2370e-01],\n",
            "         [-5.1299e-02, -3.1918e-01, -3.8807e-02,  1.5957e-01, -2.4989e-02,\n",
            "           2.3948e-01,  2.1835e-01, -6.3932e-02,  7.1299e-02,  2.5659e-02,\n",
            "           1.7443e-01, -1.3179e-01,  2.7071e-01,  1.5567e-01, -7.9719e-02,\n",
            "           7.4685e-02],\n",
            "         [-8.4326e-02,  1.5591e-01,  3.9071e-02,  2.1825e-03,  3.6338e-02,\n",
            "          -2.0504e-01,  6.0617e-01, -1.1862e-02, -4.1676e-02,  1.8078e-02,\n",
            "           1.9232e-02,  3.8714e-02,  2.6343e-01,  2.4067e-01, -2.3611e-01,\n",
            "           2.8365e-02],\n",
            "         [-1.8592e-01,  1.6202e-02, -3.8354e-03,  1.0900e-01, -2.4915e-04,\n",
            "           1.4595e-02,  0.0000e+00,  6.9159e-03,  1.4033e-01,  1.4450e-01,\n",
            "           2.5751e-01,  3.0044e-01,  1.8446e-01, -1.2821e-01,  9.1175e-02,\n",
            "          -1.2829e-01],\n",
            "         [ 4.1009e-02, -1.9092e-01,  7.7848e-02, -3.2117e-01, -1.0134e-01,\n",
            "           3.7495e-01,  1.9789e-01, -1.2996e-01,  4.8769e-02,  3.1543e-01,\n",
            "           4.1946e-02, -1.2240e-01,  2.3935e-01,  1.5834e-01,  6.2341e-02,\n",
            "          -3.8264e-02],\n",
            "         [-2.2346e-01, -4.2907e-02,  3.1496e-01, -2.3977e-01, -4.0763e-01,\n",
            "          -6.6128e-02,  1.6275e-01,  1.9268e-01,  5.0292e-02,  1.3243e-02,\n",
            "           1.9650e-01, -2.8954e-01, -8.6903e-02, -4.8529e-02, -4.1507e-02,\n",
            "           5.0609e-03],\n",
            "         [-7.1760e-02, -2.0387e-01,  3.4761e-02,  2.9082e-02, -2.2842e-04,\n",
            "           1.2365e-01,  1.2361e-01, -3.5590e-02,  2.4801e-01,  1.5718e-01,\n",
            "          -1.8469e-01,  6.7464e-02,  1.1553e-01,  2.0436e-01, -2.5312e-01,\n",
            "          -1.0582e-01]],\n",
            "\n",
            "        [[ 3.3822e-01,  1.6532e-02, -5.4341e-02, -3.1529e-01, -7.8354e-02,\n",
            "          -1.8377e-01, -6.9017e-02,  4.1287e-02,  1.1591e-01, -5.1189e-02,\n",
            "          -1.6855e-02,  1.1527e-01, -8.9395e-02,  8.9080e-02,  6.3285e-02,\n",
            "           2.1975e-01],\n",
            "         [-1.3127e-01, -6.5716e-02,  3.2663e-01, -3.5196e-03, -2.4191e-01,\n",
            "           7.2840e-03,  3.0277e-01, -2.3432e-01, -9.8787e-02,  2.1329e-01,\n",
            "           3.4513e-01,  3.1732e-02, -1.4992e-01, -1.1782e-02, -9.1388e-02,\n",
            "           3.9915e-01],\n",
            "         [ 7.8416e-02,  2.1889e-01, -1.9672e-01, -2.1862e-02, -4.4463e-03,\n",
            "           2.6436e-02,  2.1803e-02,  5.9317e-02,  1.3144e-01,  1.1206e-04,\n",
            "           1.6404e-01,  6.0737e-02,  2.2866e-01,  8.5585e-02,  1.2897e-01,\n",
            "          -3.3291e-01],\n",
            "         [ 1.4453e-01, -2.1259e-01, -1.1342e-01,  3.1514e-01,  3.5428e-02,\n",
            "           2.7819e-01, -7.1978e-02,  7.4069e-02,  5.4777e-02, -1.9070e-01,\n",
            "          -1.6421e-01, -1.4093e-01,  3.0801e-01, -1.1902e-01, -2.2222e-02,\n",
            "           2.0489e-01],\n",
            "         [-1.8574e-01, -5.6381e-02,  2.3509e-01, -2.6579e-01, -1.9499e-01,\n",
            "           3.7036e-01,  1.2382e-01, -1.2194e-01,  9.5181e-02,  1.9336e-01,\n",
            "           2.3090e-01, -6.4524e-02,  1.6271e-01, -4.3546e-02,  4.0574e-02,\n",
            "           5.1990e-02],\n",
            "         [ 8.7560e-02, -2.7777e-01, -5.6276e-02, -8.6378e-02, -6.0059e-02,\n",
            "           8.8870e-02,  0.0000e+00, -6.6557e-02,  1.1397e-01,  1.7875e-01,\n",
            "           1.3476e-01, -3.0730e-01,  9.7283e-02,  7.8546e-02, -1.0441e-01,\n",
            "          -9.1931e-02],\n",
            "         [ 1.9233e-02, -1.9242e-01,  5.1955e-02,  7.5633e-02,  1.4351e-01,\n",
            "           5.1756e-02,  2.3278e-01, -1.3162e-01,  2.0695e-01,  2.4082e-01,\n",
            "           4.6419e-02,  1.9782e-02,  1.8992e-01, -1.9110e-01,  1.4447e-02,\n",
            "           2.2182e-01],\n",
            "         [ 2.2731e-01,  7.3531e-02, -2.2533e-01, -3.2102e-02, -3.1506e-01,\n",
            "           7.3505e-02, -1.9376e-01, -1.0772e-02,  8.7359e-02, -1.6601e-01,\n",
            "           1.4132e-01,  9.5060e-02,  5.9473e-02,  2.7055e-02,  1.2102e-01,\n",
            "          -2.0934e-01]],\n",
            "\n",
            "        [[-1.3821e-02, -9.7596e-02, -6.9507e-02, -2.6995e-01, -9.7148e-02,\n",
            "           2.0591e-01,  1.7139e-01,  1.4667e-02,  2.6529e-02,  3.1804e-02,\n",
            "           2.3803e-02, -4.6522e-02,  8.3080e-03, -1.2861e-01, -1.1460e-02,\n",
            "           1.9847e-02],\n",
            "         [ 0.0000e+00, -2.7760e-01, -1.7907e-01,  1.1972e-01, -1.0152e-01,\n",
            "           8.7225e-02,  1.5980e-01,  8.6637e-02, -9.9914e-02, -2.4633e-01,\n",
            "          -6.4497e-02,  5.7713e-02,  1.6717e-01, -1.1363e-01,  7.4950e-02,\n",
            "          -2.4699e-02],\n",
            "         [ 3.3662e-01, -1.4741e-02, -3.8489e-01, -8.6995e-02,  4.7806e-02,\n",
            "           3.5054e-01,  8.5324e-02,  2.2074e-01,  1.0368e-01, -1.1038e-01,\n",
            "          -2.8398e-01, -3.7639e-02,  5.8126e-02,  2.3372e-02,  1.9260e-01,\n",
            "          -1.1057e-01],\n",
            "         [ 1.4947e-01, -1.3302e-01,  1.8104e-02, -2.0141e-02,  7.4776e-03,\n",
            "           4.8223e-02,  5.9981e-02,  4.5567e-02,  4.2302e-02,  2.4201e-01,\n",
            "           5.0404e-02,  6.6189e-02,  9.1169e-02, -2.3519e-01,  6.1954e-02,\n",
            "           2.9133e-02],\n",
            "         [-2.9699e-01,  7.4035e-03,  1.3634e-01, -1.8202e-01, -1.7155e-01,\n",
            "          -2.7487e-01,  1.6197e-01,  2.7083e-01, -1.7481e-01,  2.3146e-01,\n",
            "           8.9681e-02, -1.1815e-01,  1.9526e-01, -1.1101e-01, -3.1857e-01,\n",
            "           8.7166e-03],\n",
            "         [-4.0073e-02,  2.2725e-02, -4.4884e-03,  4.2728e-02, -5.4467e-02,\n",
            "          -1.0658e-01, -5.8841e-02,  1.7451e-02,  1.4080e-01, -5.1448e-02,\n",
            "           4.4924e-02,  9.6691e-02,  2.9719e-01,  1.3724e-01, -8.0834e-02,\n",
            "          -5.7808e-02],\n",
            "         [ 1.7960e-01,  1.8818e-01, -1.8929e-01,  2.0939e-01,  1.8429e-01,\n",
            "           2.0752e-01, -2.9725e-01,  1.3234e-01, -5.3397e-02, -8.8064e-03,\n",
            "           1.9419e-01,  4.3757e-01,  6.1021e-02,  1.8394e-01,  3.0537e-01,\n",
            "           4.7634e-01],\n",
            "         [ 1.0981e-01, -7.9943e-02, -1.2812e-02, -1.0405e-01,  1.0035e-01,\n",
            "           1.8833e-01,  2.6659e-01,  2.9314e-02, -1.7033e-01,  2.9653e-02,\n",
            "           6.0323e-02,  5.1782e-02,  2.1919e-01,  2.1890e-01,  7.0791e-02,\n",
            "           2.5961e-02]],\n",
            "\n",
            "        [[ 9.9521e-02, -8.9136e-03,  1.5778e-01,  2.0251e-01, -5.1051e-01,\n",
            "          -3.4778e-02, -1.7048e-01,  2.3121e-02, -2.4522e-01,  1.9953e-01,\n",
            "           5.7285e-01, -1.7650e-01,  1.3932e-01,  1.7048e-01,  1.0961e-02,\n",
            "           0.0000e+00],\n",
            "         [ 2.5344e-02, -2.7021e-01, -6.7312e-02, -6.4380e-04,  6.7444e-02,\n",
            "           1.5685e-01,  4.6898e-03,  3.8547e-02, -7.1837e-03,  1.3584e-01,\n",
            "          -3.7428e-02, -3.5240e-04,  1.5985e-01,  8.1692e-02, -1.1948e-01,\n",
            "          -2.5269e-02],\n",
            "         [ 1.3112e-01,  2.1155e-02, -9.3519e-02,  5.1737e-02,  5.0906e-02,\n",
            "           1.4658e-02,  0.0000e+00, -6.6034e-02,  7.6606e-02, -1.1670e-01,\n",
            "           2.2315e-02, -8.4421e-02, -4.3804e-02,  1.3613e-02, -1.3047e-01,\n",
            "          -1.2462e-01],\n",
            "         [-8.2337e-04, -3.1360e-02, -4.8367e-02, -4.6569e-03, -9.1643e-03,\n",
            "           5.4172e-02, -1.3279e-02, -1.0570e-02,  1.1830e-02,  0.0000e+00,\n",
            "          -3.6132e-02,  0.0000e+00,  1.6597e-02, -5.1793e-02,  9.4714e-02,\n",
            "           1.7932e-02],\n",
            "         [-1.1636e-01,  9.8343e-02, -3.9029e-02,  1.9246e-01,  2.1551e-02,\n",
            "          -9.2511e-02, -1.6283e-01,  1.0626e-01,  7.8360e-02, -1.2399e-01,\n",
            "          -8.8596e-02,  8.0761e-02,  2.8103e-01,  3.6824e-02,  2.6715e-02,\n",
            "          -2.2127e-02],\n",
            "         [-9.6374e-02,  0.0000e+00, -5.6795e-03, -9.8310e-03, -1.5146e-01,\n",
            "           2.1690e-01, -2.3653e-01,  6.6600e-02, -7.2132e-02,  0.0000e+00,\n",
            "           7.7496e-02,  4.5338e-02,  6.7549e-02, -5.7905e-02,  4.1711e-02,\n",
            "           6.9307e-02],\n",
            "         [-6.0887e-02,  0.0000e+00, -1.8841e-01,  0.0000e+00, -2.5189e-01,\n",
            "           1.4969e-01,  3.0118e-02,  1.6705e-01,  8.8829e-02, -1.2315e-01,\n",
            "          -2.8862e-02, -1.4579e-01,  1.2690e-01,  0.0000e+00,  5.8267e-01,\n",
            "           8.3379e-02],\n",
            "         [-1.4600e-01,  1.9351e-03, -1.5381e-02,  6.0385e-02, -7.2253e-02,\n",
            "          -2.9463e-02, -1.1311e-01,  1.0061e-02, -9.0140e-02, -8.8708e-02,\n",
            "           1.1602e-01,  3.6369e-02, -1.3180e-01,  1.4110e-01,  1.7004e-02,\n",
            "           2.7972e-01]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Balancing Loss\n",
        "\n",
        "from:\n",
        "\n",
        "switch transformers: https://arxiv.org/pdf/2101.03961.pdf  \n",
        "\n",
        "A. Differentiable Load Balancing Loss"
      ],
      "metadata": {
        "id": "rJJ50AAa5E0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@torch.jit.script\n",
        "def compute_aux_loss(num_experts: int,\n",
        "                     top_k_gates: torch.Tensor,\n",
        "                     top_k_indices: torch.Tensor,\n",
        "                     logits: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Calculate and return the auxiliary loss based on the accumulated statistics.\n",
        "    switch transformers: https://arxiv.org/pdf/2101.03961.pdf\n",
        "    A. Differentiable Load Balancing Loss\n",
        "\n",
        "    Args:\n",
        "        num_experts (int): The number of experts.\n",
        "        top_k_gates (tensor): k个最大值的对应logits, 其每个元素表示对应logit概率值。\n",
        "        top_k_indices (tensor): k个最大值的对应logits索引, 其每个元素表示logit对应索引值。\n",
        "        logits (tensor): 其每个元素表示对应logit概率值。\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The calculated auxiliary loss.\n",
        "    \"\"\"\n",
        "    # 对logits进行softmax操作，得到每个类别的概率分布\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    zeros = torch.zeros_like(probs)\n",
        "    # Convert zeros to match top_k_gates dtype\n",
        "    zeros = zeros.to(top_k_gates.dtype)\n",
        "    gates = zeros.scatter(-1, top_k_indices, top_k_gates)\n",
        "\n",
        "    # 获取 logits 张量的批次大小，即样本数量\n",
        "    count = logits.size(0)\n",
        "    # 计算每个专家被选中的概率之和，即将概率沿着批次维度求和。\n",
        "    probs = probs.sum(0)\n",
        "    # 计算每个专家被选中的频率，即计算门控值大于0的次数（即专家被选中的次数），\n",
        "    # 然后将其沿着批次维度求和。\n",
        "    freq = (gates > 0).float().sum(0)\n",
        "    # 计算 logits 张量经过 softmax 处理后的平方和的对数。\n",
        "    # 这里首先使用 softmax 函数将 logits 转换为概率分布，\n",
        "    # 然后计算概率分布的每个样本的平方和，并取对数，最后将结果沿着批次维度求和。\n",
        "    lsesq = (torch.log(torch.exp(logits).sum(dim=-1)) ** 2).sum()\n",
        "\n",
        "    # 计算专家选择损失，其计算方式为对每个专家的概率和频率进行归一化，然后计算它们的点积，最后将结果乘以专家数量。\n",
        "    switchloss = num_experts * \\\n",
        "        (F.normalize(probs, p=1, dim=0) * F.normalize(freq, p=1, dim=0)).sum()\n",
        "    # 计算 z 损失，即 logits 的对数平方和除以样本数量\n",
        "    zloss = lsesq / count\n",
        "    # 将专家选择损失和 z 损失加权相加得到最终的辅助损失\n",
        "    loss = switchloss + 0.1 * zloss\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "PK1uuohR5ELc"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#noisy top-k gating\n",
        "class NoisyTopkRouter(nn.Module):\n",
        "    def __init__(self, n_embed, num_experts, top_k):\n",
        "        super(NoisyTopkRouter, self).__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.top_k = top_k\n",
        "        #layer for router logits\n",
        "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
        "        self.noise_linear =nn.Linear(n_embed, num_experts)\n",
        "        self.aux_loss = 0.0\n",
        "\n",
        "\n",
        "    def forward(self, mh_output):\n",
        "        # mh_ouput is the output tensor from multihead self attention block\n",
        "        logits = self.topkroute_linear(mh_output)\n",
        "\n",
        "        #Noise logits\n",
        "        noise_logits = self.noise_linear(mh_output)\n",
        "\n",
        "        #Adding scaled unit gaussian noise to the logits\n",
        "        noise = torch.randn_like(logits)*F.softplus(noise_logits)\n",
        "        noisy_logits = logits + noise\n",
        "\n",
        "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
        "        zeros = torch.full_like(noisy_logits, float('-inf'))\n",
        "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
        "        router_output = F.softmax(sparse_logits, dim=-1)\n",
        "        # 训练时才计算辅助loss值, 为了专家之间的负载平衡\n",
        "        if self.training:\n",
        "          self.aux_loss = compute_aux_loss(self.num_experts, router_output, indices, noisy_logits)\n",
        "\n",
        "        return router_output, indices\n"
      ],
      "metadata": {
        "id": "dnTTSz4dfJ09"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing this out, again:\n",
        "num_experts = 8\n",
        "top_k = 2\n",
        "n_embd = 16\n",
        "\n",
        "mh_output = torch.randn(2, 4, n_embd)  # Example input\n",
        "noisy_top_k_gate = NoisyTopkRouter(n_embd, num_experts, top_k)\n",
        "noisy_top_k_gate.training = True\n",
        "\n",
        "gating_output, indices = noisy_top_k_gate(mh_output)\n",
        "print(noisy_top_k_gate.aux_loss.shape, noisy_top_k_gate.aux_loss)\n",
        "gating_output.shape, gating_output, indices\n",
        "#It works!!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUiTr1pOfU_-",
        "outputId": "60cb6332-538c-4131-bba6-830b387248a1"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([]) tensor(10.0999, grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 4, 8]),\n",
              " tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.6761, 0.0000, 0.3239, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4239, 0.0000, 0.5761],\n",
              "          [0.0000, 0.0000, 0.9223, 0.0000, 0.0000, 0.0000, 0.0777, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.4978, 0.0000, 0.5022, 0.0000, 0.0000]],\n",
              " \n",
              "         [[0.6397, 0.0000, 0.0000, 0.0000, 0.3603, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.7183, 0.0000, 0.0000, 0.0000, 0.2817, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.8339, 0.1661, 0.0000, 0.0000],\n",
              "          [0.4386, 0.0000, 0.0000, 0.5614, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
              "        grad_fn=<SoftmaxBackward0>),\n",
              " tensor([[[4, 6],\n",
              "          [7, 5],\n",
              "          [2, 6],\n",
              "          [5, 3]],\n",
              " \n",
              "         [[0, 4],\n",
              "          [2, 6],\n",
              "          [4, 5],\n",
              "          [3, 0]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "da5f3be4-f155-4d6c-bcbd-2f88a088261f",
          "showTitle": false,
          "title": ""
        },
        "id": "vMZCda7dkRJj"
      },
      "source": [
        "## Putting it all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0eaf71cd-c77e-40c7-b5be-e364e91685cf",
          "showTitle": false,
          "title": ""
        },
        "id": "f8yczkFHkRJj"
      },
      "outputs": [],
      "source": [
        "#First defining hyperparameters and boiler plate code. Imports and data preparation code is repeated for convenience\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import init\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 400\n",
        "head_size = 16\n",
        "n_embed = 128\n",
        "n_head = 8\n",
        "n_layer = 8\n",
        "dropout = 0.1\n",
        "num_experts = 8\n",
        "top_k = 2\n",
        "aux_loss_coef=0.01\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ee1180f7-5004-4425-87fe-9a81a17b9024",
          "showTitle": false,
          "title": ""
        },
        "id": "QfxJ6B2fkRJj"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "#Multi-Headed Self Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embed, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "03611a92-aaa2-4e0d-9755-cba56f96c794",
          "showTitle": false,
          "title": ""
        },
        "id": "y35jVCZYkRJk"
      },
      "outputs": [],
      "source": [
        "#Expert module\n",
        "class Expert(nn.Module):\n",
        "    \"\"\" An MLP is a simple linear layer followed by a non-linearity i.e. each Expert \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "#noisy top-k gating\n",
        "class NoisyTopkRouter(nn.Module):\n",
        "    def __init__(self, n_embed, num_experts, top_k):\n",
        "        super(NoisyTopkRouter, self).__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.top_k = top_k\n",
        "        #layer for router logits\n",
        "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
        "        self.noise_linear =nn.Linear(n_embed, num_experts)\n",
        "        self.aux_loss = 0.0\n",
        "\n",
        "\n",
        "    def forward(self, mh_output):\n",
        "        # mh_ouput is the output tensor from multihead self attention block\n",
        "        logits = self.topkroute_linear(mh_output)\n",
        "\n",
        "        #Noise logits\n",
        "        noise_logits = self.noise_linear(mh_output)\n",
        "\n",
        "        #Adding scaled unit gaussian noise to the logits\n",
        "        noise = torch.randn_like(logits)*F.softplus(noise_logits)\n",
        "        noisy_logits = logits + noise\n",
        "\n",
        "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
        "        zeros = torch.full_like(noisy_logits, float('-inf'))\n",
        "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
        "        router_output = F.softmax(sparse_logits, dim=-1)\n",
        "        # 训练时才计算辅助loss值, 为了专家之间的负载平衡\n",
        "        if self.training:\n",
        "            self.aux_loss = compute_aux_loss(self.num_experts, router_output,\n",
        "                                             indices, noisy_logits)\n",
        "\n",
        "        return router_output, indices\n",
        "\n",
        "#Now create the sparse mixture of experts module\n",
        "class SparseMoE(nn.Module):\n",
        "    def __init__(self, n_embed, num_experts, top_k, capacity_factor=1.0):\n",
        "        super(SparseMoE, self).__init__()\n",
        "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
        "        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
        "        self.top_k = top_k\n",
        "        # add capacity_factor\n",
        "        self.capacity_factor = capacity_factor\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "    def forward(self, x):\n",
        "    # Assuming x has shape [batch_size, seq_len, n_embd]\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        gating_output, indices = self.router(x)\n",
        "        final_output = torch.zeros_like(x)\n",
        "\n",
        "        # Flatten the batch and sequence dimensions to treat each token independently\n",
        "        flat_x = x.view(-1, x.size(-1))  # Now shape [batch_size * seq_len, n_embd]\n",
        "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
        "\n",
        "        tokens_per_batch = batch_size * seq_len * self.top_k\n",
        "        expert_capacity = int((tokens_per_batch / self.num_experts) * self.capacity_factor)\n",
        "\n",
        "        updates = torch.zeros_like(flat_x)\n",
        "\n",
        "        for i, expert in enumerate(self.experts):\n",
        "            expert_mask = (indices == i).any(dim=-1)\n",
        "            flat_mask = expert_mask.view(-1)\n",
        "            selected_indices = torch.nonzero(flat_mask).squeeze(-1)\n",
        "\n",
        "            limited_indices = selected_indices[:expert_capacity] if selected_indices.numel() > expert_capacity else selected_indices\n",
        "            if limited_indices.numel() > 0:\n",
        "                expert_input = flat_x[limited_indices]\n",
        "                expert_output = expert(expert_input)\n",
        "\n",
        "                gating_scores = flat_gating_output[limited_indices, i].unsqueeze(1)\n",
        "                weighted_output = expert_output * gating_scores\n",
        "\n",
        "                updates.index_add_(0, limited_indices, weighted_output)\n",
        "\n",
        "        # Reshape updates to match the original dimensions of x\n",
        "        final_output += updates.view(batch_size, seq_len, -1)\n",
        "\n",
        "        return final_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bfdff2bb-092f-41c8-9a33-c84e6f8d6633",
          "showTitle": false,
          "title": ""
        },
        "id": "jGiuRsSgkRJk"
      },
      "outputs": [],
      "source": [
        "#First create a self attention + mixture of experts block, that may be repeated several number of times\n",
        "#Copy pasting key architecture variables for clarity\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Mixture of Experts Transformer block: communication followed by computation (multi-head self attention + SparseMoE) \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, n_head, num_experts, top_k):\n",
        "        # n_embed: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.smoe = SparseMoE(n_embed, num_experts, top_k)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.smoe(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2d32a276-d0cc-4808-90d7-62441771af44",
          "showTitle": false,
          "title": ""
        },
        "id": "RpyZBA71kRJk"
      },
      "outputs": [],
      "source": [
        "#Finally putting it all together to crease a sparse mixture of experts language model\n",
        "class SparseMoELanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.blocks = nn.ModuleList([Block(n_embed, n_head=n_head, num_experts=num_experts,top_k=top_k) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embed) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        # x = self.blocks(x)  # (B,T,C)\n",
        "        aux_loss = 0.0\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "            if self.training:\n",
        "              #print(block.smoe.router.aux_loss)\n",
        "              aux_loss += block.smoe.router.aux_loss\n",
        "\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        if targets is not None and self.training:\n",
        "            loss += aux_loss_coef*aux_loss.to(loss.device)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "622ba7ce-3f20-4820-8982-93f3d3b7be09",
          "showTitle": false,
          "title": ""
        },
        "id": "bBzQXmfykRJk"
      },
      "source": [
        "这里使用Kaiming He初始化，因为专家中存在ReLU激活函数。可以随意尝试使用更常用于Transformer的Glorot初始化。Jeremy Howard的Fastai第2部分有一个非常出色的讲座，从零开始实现了这些初始化方法：https://course.fast.ai/Lessons/lesson17.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a6d3c057-08ee-4c1b-8013-6a88b2eadac5",
          "showTitle": false,
          "title": ""
        },
        "id": "guGaJqHbkRJk"
      },
      "outputs": [],
      "source": [
        "def kaiming_init_weights(m):\n",
        "    if isinstance (m, (nn.Linear)):\n",
        "        init.kaiming_normal_(m.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5b4d9525-8405-4a51-adda-661aba004e57",
          "showTitle": false,
          "title": ""
        },
        "id": "nJGGkXz4kRJl",
        "outputId": "b90d0e92-3db8-43e5-c63a-b984f2befd80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SparseMoELanguageModel(\n",
              "  (token_embedding_table): Embedding(65, 128)\n",
              "  (position_embedding_table): Embedding(32, 128)\n",
              "  (blocks): ModuleList(\n",
              "    (0-7): 8 x Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopkRouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (3): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=128, out_features=65, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ],
      "source": [
        "model = SparseMoELanguageModel()\n",
        "model.apply(kaiming_init_weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "6360e1b7-94c4-4ef1-a850-9bc93f49a083",
          "showTitle": false,
          "title": ""
        },
        "id": "WP_2lcRUkRJl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65f0adbd-e2b6-43fc-8f47-426245cf2dae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.996545 M parameters\n",
            "step 99: train loss 2.8891, val loss 2.8999\n",
            "step 199: train loss 2.6157, val loss 2.6062\n",
            "step 299: train loss 2.5084, val loss 2.5145\n",
            "step 399: train loss 2.4519, val loss 2.4494\n",
            "step 499: train loss 2.3518, val loss 2.3574\n",
            "step 599: train loss 2.2881, val loss 2.3115\n",
            "step 699: train loss 2.2374, val loss 2.2419\n",
            "step 799: train loss 2.1859, val loss 2.2104\n",
            "step 899: train loss 2.1419, val loss 2.1815\n",
            "step 999: train loss 2.0992, val loss 2.1380\n",
            "step 1099: train loss 2.0773, val loss 2.1293\n",
            "step 1199: train loss 2.0361, val loss 2.1078\n",
            "step 1299: train loss 2.0126, val loss 2.0945\n",
            "step 1399: train loss 1.9767, val loss 2.0741\n",
            "step 1499: train loss 1.9619, val loss 2.0489\n",
            "step 1599: train loss 1.9214, val loss 2.0253\n",
            "step 1699: train loss 1.9138, val loss 2.0122\n",
            "step 1799: train loss 1.8816, val loss 1.9845\n",
            "step 1899: train loss 1.8613, val loss 1.9789\n",
            "step 1999: train loss 1.8488, val loss 1.9656\n",
            "step 2099: train loss 1.8336, val loss 1.9536\n",
            "step 2199: train loss 1.8253, val loss 1.9430\n",
            "step 2299: train loss 1.8034, val loss 1.9251\n",
            "step 2399: train loss 1.7866, val loss 1.9180\n",
            "step 2499: train loss 1.7774, val loss 1.9051\n",
            "step 2599: train loss 1.7697, val loss 1.8986\n",
            "step 2699: train loss 1.7538, val loss 1.8940\n",
            "step 2799: train loss 1.7335, val loss 1.8838\n",
            "step 2899: train loss 1.7333, val loss 1.8880\n",
            "step 2999: train loss 1.7265, val loss 1.8838\n",
            "step 3099: train loss 1.7129, val loss 1.8671\n",
            "step 3199: train loss 1.7148, val loss 1.8653\n",
            "step 3299: train loss 1.6915, val loss 1.8558\n",
            "step 3399: train loss 1.6932, val loss 1.8438\n",
            "step 3499: train loss 1.6838, val loss 1.8417\n",
            "step 3599: train loss 1.6741, val loss 1.8196\n",
            "step 3699: train loss 1.6745, val loss 1.8278\n",
            "step 3799: train loss 1.6571, val loss 1.8042\n",
            "step 3899: train loss 1.6582, val loss 1.8173\n",
            "step 3999: train loss 1.6500, val loss 1.8015\n",
            "step 4099: train loss 1.6492, val loss 1.8095\n",
            "step 4199: train loss 1.6448, val loss 1.8044\n",
            "step 4299: train loss 1.6355, val loss 1.8021\n",
            "step 4399: train loss 1.6305, val loss 1.7956\n",
            "step 4499: train loss 1.6398, val loss 1.7986\n",
            "step 4599: train loss 1.6223, val loss 1.7898\n",
            "step 4699: train loss 1.6196, val loss 1.7839\n",
            "step 4799: train loss 1.6061, val loss 1.7861\n",
            "step 4899: train loss 1.6090, val loss 1.7915\n",
            "step 4999: train loss 1.6173, val loss 1.7807\n"
          ]
        }
      ],
      "source": [
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "model.train()\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if (iter+1) % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "8aa6e4c4-c688-4985-a3b8-e2af1f771e54",
          "showTitle": false,
          "title": ""
        },
        "id": "W4yshpXMkRJl",
        "outputId": "5978bc96-6e98-4959-db22-2bd2c7312915",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "lopk yilging antoo lost if ther\n",
            "Ifhuf arn afroht.\n",
            "\n",
            "JEN,itio,y bield.\n",
            "\n",
            "  Ileaved my tigrol:\n",
            "My Iid kuantie.\n",
            "\n",
            "Hir!\n",
            "\n",
            "ou hash:t not cealfurnome dearth, he'sing,\n",
            "Els what : that hou you'll sints for inome\n",
            "Whix theing that swrveful of dihtumented case.\n",
            "\n",
            "RICLAREN:\n",
            ": thio, told should\n",
            "up the fould usalchs; when hand frames nimetield.\n",
            "\n",
            "ELERCZEO:\n",
            ",- bor\n",
            "while' thats, I chall mend live: they doubtSicenent\n",
            "Whither sew ere itholy haset, Wrieldy thoLed ip it\n",
            "Fits day ield ver torshiciin have! loves his anlith,\n",
            "Fiding wortyorl I that tone e tthem blood\n",
            "Sture when were to sure and gentlem dovene; mone ballincheed\n",
            "Agud poon thwont onvy thou perfell would it thr follace ghneyr spauH;\n",
            "That is byean men, eye facenthrom honce fless Mornemy ! Has hemnest dehs tsenate\n",
            "Warough wleen hichseral. Time.\n",
            "That, everth a of boreyen you agince!\n",
            "\n",
            "DOFMWARUS:\n",
            "Dchmest this monishing, I shall, that had fond firth:\n",
            "And yumer; but youtio, an this lot accestizfed your then me\n",
            "Wague frviston feep d of themn, ufir\n",
            "Ussters. yet you'groung thi sorr'd\n",
            "Of pere hance ownnest O' rurunting souds,\n",
            "Thu whato thou siling; father feed is\n",
            "I gaivitter fheerscniendly you, lybelt lork-fath myon so Kinf Banchidiful to plusing honcthem!\n",
            "Hhatnd, if this yied the men lits and trong's ant when war\n",
            "inds btonecher'd a deating, lett 'tness;\n",
            "Frews madang is alous slain gownrofinquaciond silvly\n",
            "she lWis litgomertyors to mak,\n",
            "The ingerr alleat the rast reighee\n",
            "Th consuer, talt they his hey doninch\n",
            "do resicions the freits cerly sbbungbs\n",
            "in the councearnigng; and wwill it the canest yoe blood\n",
            "And to to her heast with take come my mockion,\n",
            "Pray untedonges glivrnes thou histhert for mean;\n",
            "And inscondring ourds lady morry groudd.\n",
            "\n",
            "PrCYNAR:\n",
            "Ancause king them in that surcicq;\n",
            "Fry, so, bydnowg his scausancer ton be sourant! why saity  of day, sirge!\n",
            "\n",
            "DoldfERD CAMIOX!\n",
            "\n",
            "FRYREYI:$--MOHved CLArizlifn, ore DucH hads suciun the wife,''\n",
            "\n",
            "Pronce tind Hefinnds, dong teaam\n",
            "As a carriv, yean come heirthin?\n",
            "\n",
            "CiMENENOIO:\n",
            "Iure youuse that n thee steet?, \n"
          ]
        }
      ],
      "source": [
        "# generate from the model. Not great. Not too bad either\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "makeMoE_from_Scratch",
      "widgets": {}
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}