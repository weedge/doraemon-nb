{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weedge/doraemon-nb/blob/main/achatbot_vision_qwen_vl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CjzTHSHQwRK"
      },
      "source": [
        "# transformers qwen2-vl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6p2eLzV1O0o"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi && lscpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5hIFzPcCeu4"
      },
      "outputs": [],
      "source": [
        "!pip show torch torchvision torchaudio transformers autoawq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2xPMMcm-313"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U autoawq torch torchvision torchaudio git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCSBF0uY_uAZ"
      },
      "outputs": [],
      "source": [
        "!pip show torch torchvision torchaudio transformers autoawq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kstQzvzkr5Jb"
      },
      "outputs": [],
      "source": [
        "!pip install -q qwen-vl-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ydqSihTr38d"
      },
      "outputs": [],
      "source": [
        "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
        "from transformers import TextIteratorStreamer\n",
        "import torch\n",
        "\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from time import perf_counter\n",
        "\n",
        "model_ckpt_name = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
        "model_ckpt_name = \"Qwen/Qwen2-VL-7B-Instruct-Int8\"\n",
        "model_ckpt_name = \"Qwen/Qwen2-VL-7B-Instruct-Int4\"\n",
        "model_ckpt_name = \"Qwen/Qwen2-VL-7B-Instruct-AWQ\"\n",
        "\n",
        "model_ckpt_name = \"Qwen/Qwen2-VL-2B-Instruct-Int8\"\n",
        "model_ckpt_name = \"Qwen/Qwen2-VL-2B-Instruct-Int4\"\n",
        "model_ckpt_name = \"Qwen/Qwen2-VL-2B-Instruct-AWQ\"\n",
        "model_ckpt_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
        "\n",
        "\n",
        "# default: Load the model on the available device(s)\n",
        "#model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "#    model_ckpt_name, torch_dtype=torch.float16, device_map=\"cuda\"\n",
        "#)\n",
        "\n",
        "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
        "# see detail: https://huggingface.co/docs/transformers/perf_infer_gpu_one\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "     model_ckpt_name,\n",
        "     torch_dtype=\"auto\",\n",
        "     #attn_implementation=\"sdpa\", #sdpa or flash_attention_2, no eager\n",
        "     #attn_implementation=\"flash_attention_2\", #sdpa or flash_attention_2, no eager\n",
        "     device_map=\"cuda\",\n",
        ")\n",
        "\n",
        "# default processer\n",
        "#processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
        "\n",
        "# The default range for the number of visual tokens per image in the model is 4-16384.\n",
        "# You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
        "min_pixels = 256*28*28\n",
        "max_pixels = 1280*28*28\n",
        "processor = AutoProcessor.from_pretrained(model_ckpt_name, min_pixels=min_pixels, max_pixels=max_pixels)\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"image\",\n",
        "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
        "            },\n",
        "            {\"type\": \"text\", \"text\": \"描述下图片内容\"},\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "times=[]\n",
        "start_time = perf_counter()\n",
        "# Preparation for inference\n",
        "text = processor.apply_chat_template(\n",
        "    messages, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "image_inputs, video_inputs = process_vision_info(messages)\n",
        "inputs = processor(\n",
        "    text=[text],\n",
        "    images=image_inputs,\n",
        "    videos=video_inputs,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "inputs = inputs.to(model.device)\n",
        "\n",
        "\n",
        "# Inference: Generation of the output with TextIteratorStreamer\n",
        "streamer = TextIteratorStreamer(processor, skip_prompt=True, skip_special_tokens=True)\n",
        "# Use Thread to run generation in background\n",
        "# Otherwise, the process is blocked until generation is complete\n",
        "# and no streaming effect can be observed.\n",
        "from threading import Thread\n",
        "generation_kwargs = dict(**inputs, streamer=streamer, max_new_tokens=128)\n",
        "thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
        "thread.start()\n",
        "\n",
        "generated_text = \"\"\n",
        "for new_text in streamer:\n",
        "    #print(new_text)\n",
        "    times.append(perf_counter() - start_time)\n",
        "    generated_text += new_text\n",
        "    start_time = perf_counter()\n",
        "print(generated_text)\n",
        "\n",
        "print(\"times\",times)\n",
        "print(\"total cost:\",sum(times))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1V1ORJVRTWK"
      },
      "source": [
        "# achatbot vision lm qwen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyZ_1-dhRR-_"
      },
      "outputs": [],
      "source": [
        "!cd /content && rm -rf achatbot && git clone --recursive https://github.com/ai-bot-pro/achatbot.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFvgRFMqRjlY"
      },
      "outputs": [],
      "source": [
        "%cd /content/achatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEpbiR_SRnwl"
      },
      "outputs": [],
      "source": [
        "!bash scripts/pypi_achatbot.sh dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XV7eALxLR1kT"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"dist/achatbot-0.0.8.6-py3-none-any.whl[tensorrt,daily_room_audio_stream,livekit,livekit-api,sense_voice_asr,deepgram_asr_processor,llm_transformers_manual_vision,einops,llm_transformers_manual_vision_img_janus,tts_edge,openai,queue]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2U1dZCWDEHc"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli download FunAudioLLM/SenseVoiceSmall --quiet --local-dir ./models/FunAudioLLM/SenseVoiceSmall --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSneSkUoWF8Z"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli download Qwen/Qwen2-VL-7B-Instruct --quiet --local-dir ./models/Qwen/Qwen2-VL-7B-Instruct --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzNZFynaWVMy"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli download Qwen/Qwen2-VL-2B-Instruct --quiet --local-dir ./models/Qwen/Qwen2-VL-2B-Instruct --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lK8Af_Vs_Vq"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli download unsloth/Llama-3.2-11B-Vision-Instruct --quiet --local-dir ./models/unsloth/Llama-3.2-11B-Vision-Instruct --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CI3WeGKBaHKU"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli download allenai/Molmo-7B-D-0924 --quiet --local-dir ./models/allenai/Molmo-7B-D-0924 --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OT3Fd_HZTEku"
      },
      "outputs": [],
      "source": [
        "# cpu\n",
        "!LLM_MODEL_NAME_OR_PATH=\"./models/Qwen/Qwen2-VL-2B-Instruct\" \\\n",
        "    python -m unittest test.core.llm.test_transformers_v_qwen.TestTransformersVQwen.test_chat_completion_prompts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gpu\n",
        "!LLM_MODEL_NAME_OR_PATH=\"./models/Qwen/Qwen2-VL-2B-Instruct\" LLM_DEVICE=cuda \\\n",
        "    python -m unittest test.core.llm.test_transformers_v_qwen.TestTransformersVQwen.test_chat_completion_prompts"
      ],
      "metadata": {
        "id": "umI-tTt5sLLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVP11g3FzCiF"
      },
      "outputs": [],
      "source": [
        "# gpu\n",
        "!LLM_MODEL_NAME_OR_PATH=\"./models/Qwen/Qwen2-VL-2B-Instruct\" LLM_DEVICE=cuda \\\n",
        "    VIDEO_FILE=\"/content/capture3_2024-09-15_20-10-27.mp4\" \\\n",
        "    python -m unittest test.core.llm.test_transformers_v_qwen.TestTransformersVQwen.test_chat_completion_prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf4D2_cv7XK_"
      },
      "outputs": [],
      "source": [
        "# gpu\n",
        "!LLM_MODEL_NAME_OR_PATH=\"./models/Qwen/Qwen2-VL-2B-Instruct\" LLM_DEVICE=cuda LLM_CHAT_HISTORY_SIZE=0 \\\n",
        "    VIDEO_FILE=\"/content/capture3_2024-09-15_20-10-27.mp4\" \\\n",
        "    python -m unittest test.core.llm.test_transformers_v_qwen.TestTransformersVQwen.test_chat_completion_prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "98VEXQ8Adoqd"
      },
      "outputs": [],
      "source": [
        "!LLM_TAG=llm_transformers_manual_vision_molmo \\\n",
        "    LLM_MODEL_NAME_OR_PATH=allenai/Molmo-7B-D-0924 \\\n",
        "    python -m unittest test.core.llm.test_transformers_v_llama.TestTransformersVLlama.test_chat_completion_prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1-4DPXudpa4"
      },
      "outputs": [],
      "source": [
        "!LLM_TAG=llm_transformers_manual_vision_molmo \\\n",
        "    LLM_MODEL_NAME_OR_PATH=allenai/Molmo-7B-D-0924 \\\n",
        "    LLM_CHAT_HISTORY_SIZE=0 \\\n",
        "    python -m unittest test.integration.processors.test_vision_processor.TestVisionProcessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Fg2ApmcbZ3Vc"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "DAILY_API_KEY=userdata.get('DAILY_API_KEY')\n",
        "\n",
        "LIVEKIT_URL=userdata.get('LIVEKIT_URL')\n",
        "LIVEKIT_API_KEY=userdata.get('LIVEKIT_API_KEY')\n",
        "LIVEKIT_API_SECRET=userdata.get('LIVEKIT_API_SECRET')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhPOa8RBb0dS"
      },
      "source": [
        "run bot task woker with bot.json, e.g.: dummy_bot.json\n",
        "\n",
        "- use daily/livekit room stream, u can click bot joined the room url, to start chat with bot with audio and camera stream,\n",
        "  - [daily](https://www.daily.co/) need DAILY_API_KEY\n",
        "  - [livekit](https://livekit.io/) need project url LIVEKIT_URL, LIVEKIT_API_KEY, LIVEKIT_API_SECRET\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/daily_describe_transformers_vision_bot.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8r5g8-u0paks",
        "outputId": "10f6b71c-988d-4c2e-e0c1-7bb0dbda4ef6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"chat_bot_name\": \"DailyDescribeVisionBot\",\n",
            "  \"room_name\": \"chat-bot\",\n",
            "  \"room_url\": \"\",\n",
            "  \"token\": \"\",\n",
            "  \"services\": {\n",
            "    \"pipeline\": \"achatbot\",\n",
            "    \"vad\": \"silero\",\n",
            "    \"asr\": \"sense_voice\",\n",
            "    \"llm\": \"transformers_manual_vision_qwen\",\n",
            "    \"tts\": \"edge\"\n",
            "  },\n",
            "  \"config\": {\n",
            "    \"vad\": {\n",
            "      \"tag\": \"silero_vad_analyzer\",\n",
            "      \"args\": { \"stop_secs\": 0.7 }\n",
            "    },\n",
            "    \"asr\": {\n",
            "      \"tag\": \"sense_voice_asr\",\n",
            "      \"args\": {\n",
            "        \"language\": \"zn\",\n",
            "        \"model_name_or_path\": \"./models/FunAudioLLM/SenseVoiceSmall\"\n",
            "      }\n",
            "    },\n",
            "    \"llm\": {\n",
            "      \"tag\":\"llm_transformers_manual_vision_qwen\",\n",
            "      \"args\":{\n",
            "        \"lm_device\":\"cuda\",\n",
            "        \"lm_model_name_or_path\":\"./models/Qwen/Qwen2-VL-2B-Instruct\",\n",
            "        \"chat_history_size\": 0,\n",
            "        \"init_chat_prompt\":\"请用中文交流\",\n",
            "        \"model_type\":\"chat_completion\"\n",
            "      },\n",
            "      \"language\": \"zh\"\n",
            "    },\n",
            "    \"tts\": {\n",
            "      \"tag\": \"tts_edge\",\n",
            "      \"args\": {\n",
            "        \"voice_name\": \"zh-CN-YunjianNeural\",\n",
            "        \"language\": \"zh\",\n",
            "        \"gender\": \"Male\"\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"config_list\": []\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKB146E-b9s_"
      },
      "outputs": [],
      "source": [
        "!DAILY_API_KEY=$DAILY_API_KEY python -m src.cmd.bots.main -f /content/daily_describe_transformers_vision_bot.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/livekit_describe_transformers_vision_bot.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKA0TixSpdyn",
        "outputId": "780fa7e7-8c62-4b25-d231-4496af5aa373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"chat_bot_name\": \"LivekitDescribeVisionBot\",\n",
            "  \"room_name\": \"chat-bot\",\n",
            "  \"room_url\": \"\",\n",
            "  \"token\": \"\",\n",
            "  \"room_manager\": {\n",
            "    \"tag\": \"livekit_room\",\n",
            "    \"args\": {\n",
            "      \"bot_name\": \"LivekitDescribeVisionBot\",\n",
            "      \"is_common_session\": false\n",
            "    }\n",
            "  },\n",
            "  \"services\": {\n",
            "    \"pipeline\": \"achatbot\",\n",
            "    \"vad\": \"silero\",\n",
            "    \"asr\": \"sense_voice\",\n",
            "    \"llm\": \"transformers_manual_vision_qwen\",\n",
            "    \"tts\": \"edge\"\n",
            "  },\n",
            "  \"config\": {\n",
            "    \"vad\": {\n",
            "      \"tag\": \"silero_vad_analyzer\",\n",
            "      \"args\": { \"stop_secs\": 0.7 }\n",
            "    },\n",
            "    \"asr\": {\n",
            "      \"tag\": \"sense_voice_asr\",\n",
            "      \"args\": {\n",
            "        \"language\": \"zn\",\n",
            "        \"model_name_or_path\": \"./models/FunAudioLLM/SenseVoiceSmall\"\n",
            "      }\n",
            "    },\n",
            "    \"llm\": {\n",
            "      \"tag\": \"llm_transformers_manual_vision_qwen\",\n",
            "      \"args\": {\n",
            "        \"lm_device\": \"cuda\",\n",
            "        \"lm_model_name_or_path\": \"./models/Qwen/Qwen2-VL-2B-Instruct\",\n",
            "        \"chat_history_size\": 0,\n",
            "        \"init_chat_prompt\": \"请用中文交流\",\n",
            "        \"model_type\": \"chat_completion\"\n",
            "      },\n",
            "      \"language\": \"zh\"\n",
            "    },\n",
            "    \"tts\": {\n",
            "      \"tag\": \"tts_edge\",\n",
            "      \"args\": {\n",
            "        \"voice_name\": \"zh-CN-YunjianNeural\",\n",
            "        \"language\": \"zh\",\n",
            "        \"gender\": \"Male\"\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"config_list\": []\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!LIVEKIT_URL=$LIVEKIT_URL LIVEKIT_API_KEY=$LIVEKIT_API_KEY LIVEKIT_API_SECRET=$LIVEKIT_API_SECRET \\\n",
        "  python -m src.cmd.bots.main -f /content/livekit_describe_transformers_vision_bot.json"
      ],
      "metadata": {
        "id": "JPAK853Tpgqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6EiEKxtbqVT"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "REDIS_PASSWORD=userdata.get('REDIS_PASSWORD')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FzqIIMQ9boPX"
      },
      "outputs": [],
      "source": [
        "!LOG_LEVEL=debug REDIS_PASSWORD=$REDIS_PASSWORD python -m src.cmd.bots.main -f /content/task_bot.json"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "4CjzTHSHQwRK"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOrK9kXH5aVEHMHIyzIlJ1n",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}