{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMcv0VK4S3Itwe7wcjYoLCP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weedge/doraemon-nb/blob/main/deepspeed_MII.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "legacy\n",
        "- https://github.com/microsoft/DeepSpeed-MII/tree/main/mii/legacy\n",
        "- https://towardsdatascience.com/deepspeed-deep-dive-model-implementations-for-inference-mii-b02aa5d5e7f7\n",
        "- https://www.deepspeed.ai/2022/10/10/mii.html\n",
        "- [**DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale**](https://arxiv.org/abs/2207.00032)\n",
        "\n",
        "fastgen\n",
        "- https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-fastgen/README.md\n",
        "- https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-fastgen/chinese/README.md\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8SApMofRDid_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nki3tY8wCeSg",
        "outputId": "f6202767-2ecf-4564-ca2e-3bf54a2356f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepspeed-mii\n",
            "  Downloading deepspeed_mii-0.1.2-py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.6/107.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Flask-RESTful (from deepspeed-mii)\n",
            "  Downloading Flask_RESTful-0.3.10-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from deepspeed-mii) (9.4.0)\n",
            "Requirement already satisfied: Werkzeug in /usr/local/lib/python3.10/dist-packages (from deepspeed-mii) (3.0.1)\n",
            "Collecting asyncio (from deepspeed-mii)\n",
            "  Downloading asyncio-3.4.3-py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deepspeed-kernels (from deepspeed-mii)\n",
            "  Downloading deepspeed_kernels-0.0.1.dev1698255861-py3-none-manylinux1_x86_64.whl (44.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deepspeed>=0.12.4 (from deepspeed-mii)\n",
            "  Downloading deepspeed-0.12.4.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.10/dist-packages (from deepspeed-mii) (1.59.3)\n",
            "Collecting grpcio-tools (from deepspeed-mii)\n",
            "  Downloading grpcio_tools-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deepspeed-mii) (1.10.13)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from deepspeed-mii) (0.4.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed-mii) (2.1.0+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from deepspeed-mii) (4.35.2)\n",
            "Collecting ujson (from deepspeed-mii)\n",
            "  Downloading ujson-5.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zmq (from deepspeed-mii)\n",
            "  Downloading zmq-0.0.0.zip (2.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hjson (from deepspeed>=0.12.4->deepspeed-mii)\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja (from deepspeed>=0.12.4->deepspeed-mii)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.12.4->deepspeed-mii) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.12.4->deepspeed-mii) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.12.4->deepspeed-mii) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.12.4->deepspeed-mii) (9.0.0)\n",
            "Collecting pynvml (from deepspeed>=0.12.4->deepspeed-mii)\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.12.4->deepspeed-mii) (4.66.1)\n",
            "Requirement already satisfied: cmake>=3.24 in /usr/local/lib/python3.10/dist-packages (from deepspeed-kernels->deepspeed-mii) (3.27.9)\n",
            "Collecting aniso8601>=0.82 (from Flask-RESTful->deepspeed-mii)\n",
            "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from Flask-RESTful->deepspeed-mii) (2.2.5)\n",
            "Requirement already satisfied: six>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from Flask-RESTful->deepspeed-mii) (1.16.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from Flask-RESTful->deepspeed-mii) (2023.3.post1)\n",
            "Collecting protobuf<5.0dev,>=4.21.6 (from grpcio-tools->deepspeed-mii)\n",
            "  Downloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio (from deepspeed-mii)\n",
            "  Downloading grpcio-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m127.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools->deepspeed-mii) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed-mii) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed-mii) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed-mii) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed-mii) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed-mii) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed-mii) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed-mii) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers->deepspeed-mii) (0.19.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->deepspeed-mii) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->deepspeed-mii) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->deepspeed-mii) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->deepspeed-mii) (0.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from Werkzeug->deepspeed-mii) (2.1.3)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.10/dist-packages (from zmq->deepspeed-mii) (23.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->Flask-RESTful->deepspeed-mii) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->Flask-RESTful->deepspeed-mii) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->deepspeed-mii) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->deepspeed-mii) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->deepspeed-mii) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->deepspeed-mii) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed-mii) (1.3.0)\n",
            "Building wheels for collected packages: deepspeed, zmq\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.12.4-py3-none-any.whl size=1290644 sha256=5f01d0ad84aba0493b6c70f79c6323ff9a10033e083a1523f2b62501672962d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/0c/52/f464610477b069120f740202a9d84a27f9d7235cbf035c4b75\n",
            "  Building wheel for zmq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zmq: filename=zmq-0.0.0-py3-none-any.whl size=1264 sha256=7f905ffe0cd2a9cc29ecd3c3fd5b3da8f49c1f471e601d375f537d99b7a95ec1\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/c5/fe/d853f71843cae26c123d37a7a5934baac20fc66f35a913951d\n",
            "Successfully built deepspeed zmq\n",
            "Installing collected packages: ninja, hjson, asyncio, aniso8601, zmq, ujson, pynvml, protobuf, grpcio, deepspeed-kernels, grpcio-tools, Flask-RESTful, deepspeed, deepspeed-mii\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.59.3\n",
            "    Uninstalling grpcio-1.59.3:\n",
            "      Successfully uninstalled grpcio-1.59.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Flask-RESTful-0.3.10 aniso8601-9.0.1 asyncio-3.4.3 deepspeed-0.12.4 deepspeed-kernels-0.0.1.dev1698255861 deepspeed-mii-0.1.2 grpcio-1.60.0 grpcio-tools-1.60.0 hjson-3.1.0 ninja-1.11.1.1 protobuf-4.25.1 pynvml-11.5.0 ujson-5.9.0 zmq-0.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "asyncio"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install deepspeed-mii"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ds_report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLdzy63DJ2u-",
        "outputId": "e38ff93a-89af-4c58-f82e-8cca9b8d1245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-13 09:10:55,461] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "--------------------------------------------------\n",
            "DeepSpeed C++/CUDA extension op report\n",
            "--------------------------------------------------\n",
            "NOTE: Ops not installed will be just-in-time (JIT) compiled at\n",
            "      runtime if needed. Op compatibility means that your system\n",
            "      meet the required dependencies to JIT install the op.\n",
            "--------------------------------------------------\n",
            "JIT compiled ops requires ninja\n",
            "ninja .................. \u001b[92m[OKAY]\u001b[0m\n",
            "--------------------------------------------------\n",
            "op name ................ installed .. compatible\n",
            "--------------------------------------------------\n",
            "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
            "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
            "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
            "async_io ............... \u001b[93m[NO]\u001b[0m ....... \u001b[93m[NO]\u001b[0m\n",
            "fused_adam ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "cpu_adam ............... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "cpu_adagrad ............ \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "cpu_lion ............... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
            "evoformer_attn ......... \u001b[93m[NO]\u001b[0m ....... \u001b[93m[NO]\u001b[0m\n",
            "fused_lamb ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "fused_lion ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "inference_core_ops ..... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "cutlass_ops ............ \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "quantizer .............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "ragged_device_ops ...... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "ragged_ops ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "random_ltd ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
            "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n",
            "sparse_attn ............ \u001b[93m[NO]\u001b[0m ....... \u001b[93m[NO]\u001b[0m\n",
            "spatial_inference ...... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "transformer ............ \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "stochastic_transformer . \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "transformer_inference .. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "--------------------------------------------------\n",
            "DeepSpeed general environment info:\n",
            "torch install path ............... ['/usr/local/lib/python3.10/dist-packages/torch']\n",
            "torch version .................... 2.1.0+cu118\n",
            "deepspeed install path ........... ['/usr/local/lib/python3.10/dist-packages/deepspeed']\n",
            "deepspeed info ................... 0.12.4, unknown, unknown\n",
            "torch cuda version ............... 11.8\n",
            "torch hip version ................ None\n",
            "nvcc version ..................... 11.8\n",
            "deepspeed wheel compiled w. ...... torch 2.1, cuda 11.8\n",
            "shared memory (/dev/shm) size .... 24.62 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/microsoft/DeepSpeed-MII"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shaTH9qEYZsr",
        "outputId": "a8648ee6-2f84-4da3-ee00-e56bf953f244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DeepSpeed-MII'...\n",
            "remote: Enumerating objects: 3197, done.\u001b[K\n",
            "remote: Counting objects: 100% (1806/1806), done.\u001b[K\n",
            "remote: Compressing objects: 100% (677/677), done.\u001b[K\n",
            "remote: Total 3197 (delta 1384), reused 1326 (delta 1104), pack-reused 1391\u001b[K\n",
            "Receiving objects: 100% (3197/3197), 6.35 MiB | 1.30 MiB/s, done.\n",
            "Resolving deltas: 100% (2079/2079), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DeepSpeed-Legacy\n",
        "\n",
        "ds mii 以前的组件\n",
        "\n",
        "https://github.com/microsoft/DeepSpeed-MII/tree/main/mii/legacy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "深度学习 (DL) 开源社区在过去几个月中出现了巨大的增长。极其强大的文本生成模型（如 Bloom 176B）或图像生成模型（如稳定扩散）现在可供通过 Hugging Face 等平台访问少量甚至单个 GPU 的任何人使用。尽管开源使人工智能功能的访问变得民主化，但其应用仍然受到两个关键因素的限制：推理延迟和成本。\n",
        "\n",
        "深度学习模型推理的系统优化已经取得了重大进展，可以大大减少延迟和成本，但这些进展并不容易实现。这种可访问性有限的主要原因是深度学习模型推理环境多种多样，模型的大小、架构、系统性能特征、硬件要求等各不相同。确定适用于给定模型的适当系统优化集并正确应用它们是很重要的。通常超出了大多数数据科学家的能力范围，导致低延迟和低成本推理几乎无法实现。\n",
        "\n",
        "DeepSpeed-MII 是 DeepSpeed 的一个新的开源 Python 库，旨在使强大模型的低延迟、低成本推理不仅可行，而且易于访问。\n",
        "\n",
        "- MII 提供对数千种广泛使用的深度学习模型的高度优化实现。\n",
        "- 与原始实施相比，MII 支持的模型可显着降低延迟和成本。例如，MII 将 Big-Science Bloom 176B 模型的延迟降低了 5.7 倍，同时将成本降低了 40 倍以上。同样，它将部署稳定扩散的延迟和成本降低了 1.9 倍。查看更多详细信息，[了解MII 的详尽延迟和成本分析](https://github.com/microsoft/DeepSpeed-MII/tree/main/mii/legacy#quantifying-latency-and-cost-reduction)。\n",
        "- 为了实现低延迟/成本推理，MII 利用 DeepSpeed-Inference 的一系列优化，例如transformer的深度融合、用于多 GPU 推理的自动张量切片、使用 ZeroQuant 进行动态量化以及其他几个（请参阅我们的博客文章[了解更多详细信息](https://www.deepspeed.ai/2022/10/10/mii.html)）。\n",
        "- 凭借最先进的性能，MII 只需几行代码即可通过 AML 在本地和 Azure 上低成本部署这些模型。\n",
        "\n",
        "\n",
        "![](https://github.com/microsoft/DeepSpeed-MII/blob/main/mii/legacy/docs/images/mii-arch.png?raw=true)\n",
        "MII 架构，显示 MII 如何使用 DS-Inference 自动优化 OSS 模型，然后使用 GRPC 在本地部署，或使用 AML Inference 在 Microsoft Azure 上部署。\n",
        "\n",
        "MII 的底层由DeepSpeed-Inference提供支持。根据模型类型、模型大小、批量大小和可用硬件资源，MII 自动应用 DeepSpeed-Inference 中的一组适当的系统优化，以最小化延迟并最大化吞吐量。它通过使用许多预先指定的模型注入策略之一来实现这一点，该策略允许 MII 和 DeepSpeed-Inference 识别底层 PyTorch 模型架构并用优化的实现替换它（见图A）。在此过程中，MII 使其 DeepSpeed-Inference 中的广泛优化自动可用于其支持的数千种流行模型。\n",
        "\n",
        "\n",
        "MII 可以使用 DeepSpeed-Inference 的两种变体。\n",
        "- 第一个称为 ds-public，包含此处讨论的大部分 DeepSpeed-Inference 优化，也可以通过我们的开源 DeepSpeed 库获得。\n",
        "- 第二个称为 ds-azure，提供与 Azure 更紧密的集成，并可通过 MII 向所有 Microsoft Azure 客户提供。我们将运行两个 DeepSpeed-Inference 变体的 MII 分别称为 MII-Public 和 MII-Azure。\n",
        "\n",
        "虽然这两种变体都比开源 PyTorch 基线提供了显着的延迟和成本降低，但后者为基于生成的工作负载提供了额外的性能优势。此处提供了与 PyTorch 基线以及这两个版本之间的完整延迟和成本优势比较。\n",
        "\n",
        "云服务部署方式大同小异，aws部署方式差不多\n",
        "\n",
        "这里介绍第一种方式\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r4xJvVWnP_lz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 部署"
      ],
      "metadata": {
        "id": "nk-SjUbJV2cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MII-Public 可以部署在本地或任何云产品上，只需几行代码。MII 创建一个轻量级 GRPC 服务器来支持这种形式的部署，并为查询提供 GRPC 推理端点。\n",
        "\n",
        "# 作为示例，这里是Hugging Face 的bigscience/bloom-560m模型的部署：\n",
        "import mii\n",
        "mii_configs = {\"tensor_parallel\": 1, \"dtype\": \"fp16\"}\n",
        "mii.deploy(task=\"text-generation\",\n",
        "           model=\"bigscience/bloom-560m\",\n",
        "           deployment_name=\"bloom560m_deployment\",\n",
        "           mii_config=mii_configs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SguRnOWkVlEG",
        "outputId": "37411537-640f-4052-b633-0645ce9c00b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-13 09:11:19,436] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2023-12-13 09:12:42,722] [INFO] [deployment.py:75:deploy] ************* MII is using DeepSpeed Optimizations to accelerate your model *************\n",
            "[2023-12-13 09:12:42,722] [INFO] [deployment.py:75:deploy] ************* MII is using DeepSpeed Optimizations to accelerate your model *************\n",
            "[2023-12-13 09:12:42,727] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:12:42,728] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:12:42,735] [INFO] [server.py:38:__init__] Hostfile /job/hostfile not found, creating hostfile.\n",
            "[2023-12-13 09:12:42,735] [INFO] [server.py:38:__init__] Hostfile /job/hostfile not found, creating hostfile.\n",
            "[2023-12-13 09:12:42,738] [INFO] [server.py:103:_launch_server_process] MII server server launch: ['deepspeed', '-H', '/tmp/tmp30gbq06t', '-i', 'localhost:0', '--master_port', '29500', '--master_addr', 'localhost', '--no_ssh_check', '--no_local_rank', '--no_python', '/usr/bin/python3', '-m', 'mii.legacy.launch.multi_gpu_server', '--deployment-name', 'bloom560m_deployment', '--load-balancer-port', '50050', '--restful-gateway-port', '51080', '--server-port', '50051', '--model-config', 'eyJtb2RlbCI6ICJiaWdzY2llbmNlL2Jsb29tLTU2MG0iLCAidGFzayI6ICJ0ZXh0LWdlbmVyYXRpb24iLCAiZHR5cGUiOiAidG9yY2guZmxvYXQxNiIsICJtb2RlbF9wYXRoIjogIi90bXAvbWlpX21vZGVscyIsICJsb2FkX3dpdGhfc3lzX21lbSI6IGZhbHNlLCAibWV0YV90ZW5zb3IiOiBmYWxzZSwgImRlcGxveV9yYW5rIjogWzBdLCAidG9yY2hfZGlzdF9wb3J0IjogMjk1MDAsICJyZXBsaWNhX251bSI6IDEsICJyZXBsaWNhX2NvbmZpZ3MiOiBbeyJob3N0bmFtZSI6ICJsb2NhbGhvc3QiLCAidGVuc29yX3BhcmFsbGVsX3BvcnRzIjogWzUwMDUxXSwgInRvcmNoX2Rpc3RfcG9ydCI6IDI5NTAwLCAiZ3B1X2luZGljZXMiOiBbMF19XSwgInByb2ZpbGVfbW9kZWxfdGltZSI6IGZhbHNlLCAic2tpcF9tb2RlbF9jaGVjayI6IHRydWUsICJoZl9hdXRoX3Rva2VuIjogbnVsbCwgInRydXN0X3JlbW90ZV9jb2RlIjogZmFsc2UsICJwaXBlbGluZV9rd2FyZ3MiOiB7fSwgImVuYWJsZV9kZWVwc3BlZWQiOiB0cnVlLCAiZW5hYmxlX3plcm8iOiBmYWxzZSwgImRzX2NvbmZpZyI6IHt9LCAidGVuc29yX3BhcmFsbGVsIjogMSwgImVuYWJsZV9jdWRhX2dyYXBoIjogZmFsc2UsICJyZXBsYWNlX3dpdGhfa2VybmVsX2luamVjdCI6IHRydWUsICJjaGVja3BvaW50X2RpY3QiOiBudWxsLCAibWF4X3Rva2VucyI6IDEwMjR9']\n",
            "[2023-12-13 09:12:42,738] [INFO] [server.py:103:_launch_server_process] MII server server launch: ['deepspeed', '-H', '/tmp/tmp30gbq06t', '-i', 'localhost:0', '--master_port', '29500', '--master_addr', 'localhost', '--no_ssh_check', '--no_local_rank', '--no_python', '/usr/bin/python3', '-m', 'mii.legacy.launch.multi_gpu_server', '--deployment-name', 'bloom560m_deployment', '--load-balancer-port', '50050', '--restful-gateway-port', '51080', '--server-port', '50051', '--model-config', 'eyJtb2RlbCI6ICJiaWdzY2llbmNlL2Jsb29tLTU2MG0iLCAidGFzayI6ICJ0ZXh0LWdlbmVyYXRpb24iLCAiZHR5cGUiOiAidG9yY2guZmxvYXQxNiIsICJtb2RlbF9wYXRoIjogIi90bXAvbWlpX21vZGVscyIsICJsb2FkX3dpdGhfc3lzX21lbSI6IGZhbHNlLCAibWV0YV90ZW5zb3IiOiBmYWxzZSwgImRlcGxveV9yYW5rIjogWzBdLCAidG9yY2hfZGlzdF9wb3J0IjogMjk1MDAsICJyZXBsaWNhX251bSI6IDEsICJyZXBsaWNhX2NvbmZpZ3MiOiBbeyJob3N0bmFtZSI6ICJsb2NhbGhvc3QiLCAidGVuc29yX3BhcmFsbGVsX3BvcnRzIjogWzUwMDUxXSwgInRvcmNoX2Rpc3RfcG9ydCI6IDI5NTAwLCAiZ3B1X2luZGljZXMiOiBbMF19XSwgInByb2ZpbGVfbW9kZWxfdGltZSI6IGZhbHNlLCAic2tpcF9tb2RlbF9jaGVjayI6IHRydWUsICJoZl9hdXRoX3Rva2VuIjogbnVsbCwgInRydXN0X3JlbW90ZV9jb2RlIjogZmFsc2UsICJwaXBlbGluZV9rd2FyZ3MiOiB7fSwgImVuYWJsZV9kZWVwc3BlZWQiOiB0cnVlLCAiZW5hYmxlX3plcm8iOiBmYWxzZSwgImRzX2NvbmZpZyI6IHt9LCAidGVuc29yX3BhcmFsbGVsIjogMSwgImVuYWJsZV9jdWRhX2dyYXBoIjogZmFsc2UsICJyZXBsYWNlX3dpdGhfa2VybmVsX2luamVjdCI6IHRydWUsICJjaGVja3BvaW50X2RpY3QiOiBudWxsLCAibWF4X3Rva2VucyI6IDEwMjR9']\n",
            "[2023-12-13 09:12:42,741] [INFO] [server.py:103:_launch_server_process] load balancer server launch: ['/usr/bin/python3', '-m', 'mii.legacy.launch.multi_gpu_server', '--deployment-name', 'bloom560m_deployment', '--load-balancer-port', '50050', '--restful-gateway-port', '51080', '--load-balancer', '--model-config', 'eyJtb2RlbCI6ICJiaWdzY2llbmNlL2Jsb29tLTU2MG0iLCAidGFzayI6ICJ0ZXh0LWdlbmVyYXRpb24iLCAiZHR5cGUiOiAidG9yY2guZmxvYXQxNiIsICJtb2RlbF9wYXRoIjogIi90bXAvbWlpX21vZGVscyIsICJsb2FkX3dpdGhfc3lzX21lbSI6IGZhbHNlLCAibWV0YV90ZW5zb3IiOiBmYWxzZSwgImRlcGxveV9yYW5rIjogWzBdLCAidG9yY2hfZGlzdF9wb3J0IjogMjk1MDAsICJyZXBsaWNhX251bSI6IDEsICJyZXBsaWNhX2NvbmZpZ3MiOiBbeyJob3N0bmFtZSI6ICJsb2NhbGhvc3QiLCAidGVuc29yX3BhcmFsbGVsX3BvcnRzIjogWzUwMDUxXSwgInRvcmNoX2Rpc3RfcG9ydCI6IDI5NTAwLCAiZ3B1X2luZGljZXMiOiBbMF19XSwgInByb2ZpbGVfbW9kZWxfdGltZSI6IGZhbHNlLCAic2tpcF9tb2RlbF9jaGVjayI6IHRydWUsICJoZl9hdXRoX3Rva2VuIjogbnVsbCwgInRydXN0X3JlbW90ZV9jb2RlIjogZmFsc2UsICJwaXBlbGluZV9rd2FyZ3MiOiB7fSwgImVuYWJsZV9kZWVwc3BlZWQiOiB0cnVlLCAiZW5hYmxlX3plcm8iOiBmYWxzZSwgImRzX2NvbmZpZyI6IHt9LCAidGVuc29yX3BhcmFsbGVsIjogMSwgImVuYWJsZV9jdWRhX2dyYXBoIjogZmFsc2UsICJyZXBsYWNlX3dpdGhfa2VybmVsX2luamVjdCI6IHRydWUsICJjaGVja3BvaW50X2RpY3QiOiBudWxsLCAibWF4X3Rva2VucyI6IDEwMjR9']\n",
            "[2023-12-13 09:12:42,741] [INFO] [server.py:103:_launch_server_process] load balancer server launch: ['/usr/bin/python3', '-m', 'mii.legacy.launch.multi_gpu_server', '--deployment-name', 'bloom560m_deployment', '--load-balancer-port', '50050', '--restful-gateway-port', '51080', '--load-balancer', '--model-config', 'eyJtb2RlbCI6ICJiaWdzY2llbmNlL2Jsb29tLTU2MG0iLCAidGFzayI6ICJ0ZXh0LWdlbmVyYXRpb24iLCAiZHR5cGUiOiAidG9yY2guZmxvYXQxNiIsICJtb2RlbF9wYXRoIjogIi90bXAvbWlpX21vZGVscyIsICJsb2FkX3dpdGhfc3lzX21lbSI6IGZhbHNlLCAibWV0YV90ZW5zb3IiOiBmYWxzZSwgImRlcGxveV9yYW5rIjogWzBdLCAidG9yY2hfZGlzdF9wb3J0IjogMjk1MDAsICJyZXBsaWNhX251bSI6IDEsICJyZXBsaWNhX2NvbmZpZ3MiOiBbeyJob3N0bmFtZSI6ICJsb2NhbGhvc3QiLCAidGVuc29yX3BhcmFsbGVsX3BvcnRzIjogWzUwMDUxXSwgInRvcmNoX2Rpc3RfcG9ydCI6IDI5NTAwLCAiZ3B1X2luZGljZXMiOiBbMF19XSwgInByb2ZpbGVfbW9kZWxfdGltZSI6IGZhbHNlLCAic2tpcF9tb2RlbF9jaGVjayI6IHRydWUsICJoZl9hdXRoX3Rva2VuIjogbnVsbCwgInRydXN0X3JlbW90ZV9jb2RlIjogZmFsc2UsICJwaXBlbGluZV9rd2FyZ3MiOiB7fSwgImVuYWJsZV9kZWVwc3BlZWQiOiB0cnVlLCAiZW5hYmxlX3plcm8iOiBmYWxzZSwgImRzX2NvbmZpZyI6IHt9LCAidGVuc29yX3BhcmFsbGVsIjogMSwgImVuYWJsZV9jdWRhX2dyYXBoIjogZmFsc2UsICJyZXBsYWNlX3dpdGhfa2VybmVsX2luamVjdCI6IHRydWUsICJjaGVja3BvaW50X2RpY3QiOiBudWxsLCAibWF4X3Rva2VucyI6IDEwMjR9']\n",
            "[2023-12-13 09:12:47,749] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:12:47,749] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:12:52,756] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:12:52,756] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:12:57,764] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:12:57,764] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:02,768] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:02,768] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:07,775] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:07,775] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:12,782] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:12,782] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:17,789] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:17,789] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:22,796] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:22,796] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:27,803] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:27,803] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:32,809] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:32,809] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:37,815] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:37,815] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:42,822] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:42,822] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:47,830] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:47,830] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:52,838] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:52,838] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:57,845] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:57,845] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:02,852] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:02,852] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:07,859] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:07,859] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:12,865] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:12,865] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:17,873] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:17,873] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:22,879] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:22,879] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:27,887] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:27,887] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:32,896] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:32,896] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:37,904] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:37,904] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:42,913] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:42,913] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:47,921] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:47,921] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:52,928] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:52,928] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:57,935] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:57,935] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:15:02,942] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:15:02,942] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:15:07,950] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:15:07,950] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:15:07,952] [INFO] [server.py:64:_wait_until_server_is_live] server has started on ports [50051]\n",
            "[2023-12-13 09:15:07,952] [INFO] [server.py:64:_wait_until_server_is_live] server has started on ports [50051]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -i:50051"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeGFUem0dYtS",
        "outputId": "667ef9e5-58da-4431-fe54-a9b7ea60c080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\n",
            "python3 1480 root   40u  IPv4  62907      0t0  TCP *:50051 (LISTEN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ps -ef | grep 1480"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Taw703dDddCF",
        "outputId": "a7cee095-dd7f-4482-9903-5d33f678edff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        1480    1423 45 09:12 ?        00:01:21 /usr/bin/python3 -m mii.legacy.launch.multi_gpu_\n",
            "root        2486     392  0 09:15 ?        00:00:00 /bin/bash -c ps -ef | grep 1480\n",
            "root        2488    2486  0 09:15 ?        00:00:00 grep 1480\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 查询"
      ],
      "metadata": {
        "id": "K1qjbYn5V36j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 唯一需要的键是\"query\"，字典之外的所有其他项目都将作为generatekwargs 传递。对于 Hugging Face 提供的模型，您可以在其生成文档中找到所有可能的参数。\n",
        "#import nest_asyncio\n",
        "#nest_asyncio.apply()\n",
        "\n",
        "import mii\n",
        "generator = mii.mii_query_handle(\"bloom560m_deployment\")\n",
        "result = generator.query({\"query\": [\"DeepSpeed is\", \"Seattle is\"]}, do_sample=True, max_new_tokens=30)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPmaQQ5EV8iY",
        "outputId": "926459aa-a6d1-483b-f9d5-3d62d81e41ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-13 09:24:59,842] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:24:59,843] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "response: \"DeepSpeed is [ ] \"\n",
            "response: \"Seattle is The Crown:\\n The...\\n...\\n...\\n.........\\n...\\n...\\n...\\n...\\n...\\n...\\n......\\n...\\n......\\n...\\n......\\n...\\n...\\n...\\n...\\n......\\n\"\n",
            "time_taken: 0.654122353\n",
            "model_time_taken: -1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 负载均衡\n",
        "\n",
        "您可以启动负载平衡器和 MII 服务器的多个副本。当您指定 的值时replica_num，mii.deploy()启动负载均衡器服务器和replica_num副本数。请注意，每个副本均由部署在同一服务器上的tensor_parallel服务器进程组成。\n",
        "```\n",
        "mii_configs = {\n",
        "...\n",
        "    \"tensor_parallel\": tensor_parallel,\n",
        "    \"replica_num\": replica_num,\n",
        "    \"hostfile\": hostfile\n",
        "}\n",
        "mii.deploy(...\n",
        "           mii_config=mii_configs,\n",
        "           ...)\n",
        "```\n",
        "\n",
        "客户端将请求发送到负载均衡器，负载均衡器将请求转发到副本，而不是将请求发送到各个 MII 服务器。目前，负载均衡器实现了简单的循环算法。replica_num当设置为 1 时，负载均衡器充当简单代理。\n",
        "\n",
        "hostfile是 DeepSpeed 启动器使用的主机文件的路径。当未指定 hostfile 时，DeepSpeed-MII 使用/job/hostfile为 DeepSpeed 定义的默认路径。有关详细信息，请参阅DeepSpeed 的文档。\n",
        "\n",
        "\n",
        "https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node"
      ],
      "metadata": {
        "id": "uhNXb6HCdGhY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 关闭服务"
      ],
      "metadata": {
        "id": "C-QBdUiJWo2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mii\n",
        "mii.terminate(\"bloom560m_deployment\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftp-NJ4aWsHe",
        "outputId": "a3c8848e-4ddb-4769-e431-602908448ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-13 09:25:31,531] [INFO] [terminate.py:12:terminate] Terminating server for bloom560m_deployment\n",
            "[2023-12-13 09:25:31,531] [INFO] [terminate.py:12:terminate] Terminating server for bloom560m_deployment\n",
            "[2023-12-13 09:25:31,535] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:25:31,536] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:25:31,550] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:25:31,551] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## chat\n",
        "\n",
        "https://github.com/microsoft/DeepSpeed-MII/blob/main/mii/legacy/examples/local/chat/README.md\n",
        "\n",
        "\n",
        "**训练笔记：https://github.com/weedge/doraemon-nb/blob/main/ds_examples_chatbot.ipynb**\n",
        "\n",
        "\n",
        "使用[DeepSpeed-Chat](https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/README.md)通过RLHF训练的模型，已经上传至huggingface:\n",
        "\n",
        "https://huggingface.co/AdamG012/chat-opt-1.3b-rlhf-actor-deepspeed"
      ],
      "metadata": {
        "id": "1YFj5Ck3Zr2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# deploy start server\n",
        "!cd DeepSpeed-MII/mii/legacy/examples/local/chat && (nohup python chat-server-example.py &)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XY6YhwXJZMgl",
        "outputId": "f866cb4f-7eda-42da-b327-f78d51f45018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat DeepSpeed-MII/mii/legacy/examples/local/chat/nohup.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUTAidYMkMjl",
        "outputId": "1689415c-6b1d-4710-e052-b2a1a731fe0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-13 09:45:03,835] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "2023-12-13 09:45:05.726142: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-13 09:45:05.726202: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-13 09:45:05.726229: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-13 09:45:06.874407: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Deploying AdamG012/chat-opt-1.3b-rlhf-actor-deepspeed...\n",
            "[2023-12-13 09:45:44,816] [INFO] [deployment.py:75:deploy] ************* MII is using DeepSpeed Optimizations to accelerate your model *************\n",
            "[2023-12-13 09:45:44,816] [INFO] [deployment.py:75:deploy] ************* MII is using DeepSpeed Optimizations to accelerate your model *************\n",
            "[2023-12-13 09:45:44,818] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:45:44,818] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:45:44,824] [INFO] [server.py:38:__init__] Hostfile /job/hostfile not found, creating hostfile.\n",
            "[2023-12-13 09:45:44,824] [INFO] [server.py:38:__init__] Hostfile /job/hostfile not found, creating hostfile.\n",
            "[2023-12-13 09:45:44,825] [INFO] [server.py:103:_launch_server_process] MII server server launch: ['deepspeed', '-H', '/tmp/tmp0staojpq', '-i', 'localhost:0', '--master_port', '29500', '--master_addr', 'localhost', '--no_ssh_check', '--no_local_rank', '--no_python', '/usr/bin/python3', '-m', 'mii.legacy.launch.multi_gpu_server', '--deployment-name', 'chat_example_deployment', '--load-balancer-port', '50050', '--restful-gateway-port', '51080', '--server-port', '50051', '--model-config', 'eyJtb2RlbCI6ICJBZGFtRzAxMi9jaGF0LW9wdC0xLjNiLXJsaGYtYWN0b3ItZGVlcHNwZWVkIiwgInRhc2siOiAidGV4dC1nZW5lcmF0aW9uIiwgImR0eXBlIjogInRvcmNoLmZsb2F0MzIiLCAibW9kZWxfcGF0aCI6ICIvdG1wL21paV9tb2RlbHMiLCAibG9hZF93aXRoX3N5c19tZW0iOiBmYWxzZSwgIm1ldGFfdGVuc29yIjogZmFsc2UsICJkZXBsb3lfcmFuayI6IFswXSwgInRvcmNoX2Rpc3RfcG9ydCI6IDI5NTAwLCAicmVwbGljYV9udW0iOiAxLCAicmVwbGljYV9jb25maWdzIjogW3siaG9zdG5hbWUiOiAibG9jYWxob3N0IiwgInRlbnNvcl9wYXJhbGxlbF9wb3J0cyI6IFs1MDA1MV0sICJ0b3JjaF9kaXN0X3BvcnQiOiAyOTUwMCwgImdwdV9pbmRpY2VzIjogWzBdfV0sICJwcm9maWxlX21vZGVsX3RpbWUiOiBmYWxzZSwgInNraXBfbW9kZWxfY2hlY2siOiB0cnVlLCAiaGZfYXV0aF90b2tlbiI6IG51bGwsICJ0cnVzdF9yZW1vdGVfY29kZSI6IGZhbHNlLCAicGlwZWxpbmVfa3dhcmdzIjoge30sICJlbmFibGVfZGVlcHNwZWVkIjogdHJ1ZSwgImVuYWJsZV96ZXJvIjogZmFsc2UsICJkc19jb25maWciOiB7fSwgInRlbnNvcl9wYXJhbGxlbCI6IDEsICJlbmFibGVfY3VkYV9ncmFwaCI6IGZhbHNlLCAicmVwbGFjZV93aXRoX2tlcm5lbF9pbmplY3QiOiB0cnVlLCAiY2hlY2twb2ludF9kaWN0IjogbnVsbCwgIm1heF90b2tlbnMiOiAxMDI0fQ==']\n",
            "[2023-12-13 09:45:44,825] [INFO] [server.py:103:_launch_server_process] MII server server launch: ['deepspeed', '-H', '/tmp/tmp0staojpq', '-i', 'localhost:0', '--master_port', '29500', '--master_addr', 'localhost', '--no_ssh_check', '--no_local_rank', '--no_python', '/usr/bin/python3', '-m', 'mii.legacy.launch.multi_gpu_server', '--deployment-name', 'chat_example_deployment', '--load-balancer-port', '50050', '--restful-gateway-port', '51080', '--server-port', '50051', '--model-config', 'eyJtb2RlbCI6ICJBZGFtRzAxMi9jaGF0LW9wdC0xLjNiLXJsaGYtYWN0b3ItZGVlcHNwZWVkIiwgInRhc2siOiAidGV4dC1nZW5lcmF0aW9uIiwgImR0eXBlIjogInRvcmNoLmZsb2F0MzIiLCAibW9kZWxfcGF0aCI6ICIvdG1wL21paV9tb2RlbHMiLCAibG9hZF93aXRoX3N5c19tZW0iOiBmYWxzZSwgIm1ldGFfdGVuc29yIjogZmFsc2UsICJkZXBsb3lfcmFuayI6IFswXSwgInRvcmNoX2Rpc3RfcG9ydCI6IDI5NTAwLCAicmVwbGljYV9udW0iOiAxLCAicmVwbGljYV9jb25maWdzIjogW3siaG9zdG5hbWUiOiAibG9jYWxob3N0IiwgInRlbnNvcl9wYXJhbGxlbF9wb3J0cyI6IFs1MDA1MV0sICJ0b3JjaF9kaXN0X3BvcnQiOiAyOTUwMCwgImdwdV9pbmRpY2VzIjogWzBdfV0sICJwcm9maWxlX21vZGVsX3RpbWUiOiBmYWxzZSwgInNraXBfbW9kZWxfY2hlY2siOiB0cnVlLCAiaGZfYXV0aF90b2tlbiI6IG51bGwsICJ0cnVzdF9yZW1vdGVfY29kZSI6IGZhbHNlLCAicGlwZWxpbmVfa3dhcmdzIjoge30sICJlbmFibGVfZGVlcHNwZWVkIjogdHJ1ZSwgImVuYWJsZV96ZXJvIjogZmFsc2UsICJkc19jb25maWciOiB7fSwgInRlbnNvcl9wYXJhbGxlbCI6IDEsICJlbmFibGVfY3VkYV9ncmFwaCI6IGZhbHNlLCAicmVwbGFjZV93aXRoX2tlcm5lbF9pbmplY3QiOiB0cnVlLCAiY2hlY2twb2ludF9kaWN0IjogbnVsbCwgIm1heF90b2tlbnMiOiAxMDI0fQ==']\n",
            "[2023-12-13 09:45:44,827] [INFO] [server.py:103:_launch_server_process] load balancer server launch: ['/usr/bin/python3', '-m', 'mii.legacy.launch.multi_gpu_server', '--deployment-name', 'chat_example_deployment', '--load-balancer-port', '50050', '--restful-gateway-port', '51080', '--load-balancer', '--model-config', 'eyJtb2RlbCI6ICJBZGFtRzAxMi9jaGF0LW9wdC0xLjNiLXJsaGYtYWN0b3ItZGVlcHNwZWVkIiwgInRhc2siOiAidGV4dC1nZW5lcmF0aW9uIiwgImR0eXBlIjogInRvcmNoLmZsb2F0MzIiLCAibW9kZWxfcGF0aCI6ICIvdG1wL21paV9tb2RlbHMiLCAibG9hZF93aXRoX3N5c19tZW0iOiBmYWxzZSwgIm1ldGFfdGVuc29yIjogZmFsc2UsICJkZXBsb3lfcmFuayI6IFswXSwgInRvcmNoX2Rpc3RfcG9ydCI6IDI5NTAwLCAicmVwbGljYV9udW0iOiAxLCAicmVwbGljYV9jb25maWdzIjogW3siaG9zdG5hbWUiOiAibG9jYWxob3N0IiwgInRlbnNvcl9wYXJhbGxlbF9wb3J0cyI6IFs1MDA1MV0sICJ0b3JjaF9kaXN0X3BvcnQiOiAyOTUwMCwgImdwdV9pbmRpY2VzIjogWzBdfV0sICJwcm9maWxlX21vZGVsX3RpbWUiOiBmYWxzZSwgInNraXBfbW9kZWxfY2hlY2siOiB0cnVlLCAiaGZfYXV0aF90b2tlbiI6IG51bGwsICJ0cnVzdF9yZW1vdGVfY29kZSI6IGZhbHNlLCAicGlwZWxpbmVfa3dhcmdzIjoge30sICJlbmFibGVfZGVlcHNwZWVkIjogdHJ1ZSwgImVuYWJsZV96ZXJvIjogZmFsc2UsICJkc19jb25maWciOiB7fSwgInRlbnNvcl9wYXJhbGxlbCI6IDEsICJlbmFibGVfY3VkYV9ncmFwaCI6IGZhbHNlLCAicmVwbGFjZV93aXRoX2tlcm5lbF9pbmplY3QiOiB0cnVlLCAiY2hlY2twb2ludF9kaWN0IjogbnVsbCwgIm1heF90b2tlbnMiOiAxMDI0fQ==']\n",
            "[2023-12-13 09:45:44,827] [INFO] [server.py:103:_launch_server_process] load balancer server launch: ['/usr/bin/python3', '-m', 'mii.legacy.launch.multi_gpu_server', '--deployment-name', 'chat_example_deployment', '--load-balancer-port', '50050', '--restful-gateway-port', '51080', '--load-balancer', '--model-config', 'eyJtb2RlbCI6ICJBZGFtRzAxMi9jaGF0LW9wdC0xLjNiLXJsaGYtYWN0b3ItZGVlcHNwZWVkIiwgInRhc2siOiAidGV4dC1nZW5lcmF0aW9uIiwgImR0eXBlIjogInRvcmNoLmZsb2F0MzIiLCAibW9kZWxfcGF0aCI6ICIvdG1wL21paV9tb2RlbHMiLCAibG9hZF93aXRoX3N5c19tZW0iOiBmYWxzZSwgIm1ldGFfdGVuc29yIjogZmFsc2UsICJkZXBsb3lfcmFuayI6IFswXSwgInRvcmNoX2Rpc3RfcG9ydCI6IDI5NTAwLCAicmVwbGljYV9udW0iOiAxLCAicmVwbGljYV9jb25maWdzIjogW3siaG9zdG5hbWUiOiAibG9jYWxob3N0IiwgInRlbnNvcl9wYXJhbGxlbF9wb3J0cyI6IFs1MDA1MV0sICJ0b3JjaF9kaXN0X3BvcnQiOiAyOTUwMCwgImdwdV9pbmRpY2VzIjogWzBdfV0sICJwcm9maWxlX21vZGVsX3RpbWUiOiBmYWxzZSwgInNraXBfbW9kZWxfY2hlY2siOiB0cnVlLCAiaGZfYXV0aF90b2tlbiI6IG51bGwsICJ0cnVzdF9yZW1vdGVfY29kZSI6IGZhbHNlLCAicGlwZWxpbmVfa3dhcmdzIjoge30sICJlbmFibGVfZGVlcHNwZWVkIjogdHJ1ZSwgImVuYWJsZV96ZXJvIjogZmFsc2UsICJkc19jb25maWciOiB7fSwgInRlbnNvcl9wYXJhbGxlbCI6IDEsICJlbmFibGVfY3VkYV9ncmFwaCI6IGZhbHNlLCAicmVwbGFjZV93aXRoX2tlcm5lbF9pbmplY3QiOiB0cnVlLCAiY2hlY2twb2ludF9kaWN0IjogbnVsbCwgIm1heF90b2tlbnMiOiAxMDI0fQ==']\n",
            "[2023-12-13 09:45:47,160] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2023-12-13 09:45:47,233] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2023-12-13 09:45:48,991] [INFO] [runner.py:570:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=localhost --master_port=29500 --no_python --no_local_rank --enable_each_rank_log=None /usr/bin/python3 -m mii.legacy.launch.multi_gpu_server --deployment-name chat_example_deployment --load-balancer-port 50050 --restful-gateway-port 51080 --server-port 50051 --model-config eyJtb2RlbCI6ICJBZGFtRzAxMi9jaGF0LW9wdC0xLjNiLXJsaGYtYWN0b3ItZGVlcHNwZWVkIiwgInRhc2siOiAidGV4dC1nZW5lcmF0aW9uIiwgImR0eXBlIjogInRvcmNoLmZsb2F0MzIiLCAibW9kZWxfcGF0aCI6ICIvdG1wL21paV9tb2RlbHMiLCAibG9hZF93aXRoX3N5c19tZW0iOiBmYWxzZSwgIm1ldGFfdGVuc29yIjogZmFsc2UsICJkZXBsb3lfcmFuayI6IFswXSwgInRvcmNoX2Rpc3RfcG9ydCI6IDI5NTAwLCAicmVwbGljYV9udW0iOiAxLCAicmVwbGljYV9jb25maWdzIjogW3siaG9zdG5hbWUiOiAibG9jYWxob3N0IiwgInRlbnNvcl9wYXJhbGxlbF9wb3J0cyI6IFs1MDA1MV0sICJ0b3JjaF9kaXN0X3BvcnQiOiAyOTUwMCwgImdwdV9pbmRpY2VzIjogWzBdfV0sICJwcm9maWxlX21vZGVsX3RpbWUiOiBmYWxzZSwgInNraXBfbW9kZWxfY2hlY2siOiB0cnVlLCAiaGZfYXV0aF90b2tlbiI6IG51bGwsICJ0cnVzdF9yZW1vdGVfY29kZSI6IGZhbHNlLCAicGlwZWxpbmVfa3dhcmdzIjoge30sICJlbmFibGVfZGVlcHNwZWVkIjogdHJ1ZSwgImVuYWJsZV96ZXJvIjogZmFsc2UsICJkc19jb25maWciOiB7fSwgInRlbnNvcl9wYXJhbGxlbCI6IDEsICJlbmFibGVfY3VkYV9ncmFwaCI6IGZhbHNlLCAicmVwbGFjZV93aXRoX2tlcm5lbF9pbmplY3QiOiB0cnVlLCAiY2hlY2twb2ludF9kaWN0IjogbnVsbCwgIm1heF90b2tlbnMiOiAxMDI0fQ==\n",
            "2023-12-13 09:45:49.299986: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-13 09:45:49.300052: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-13 09:45:49.300079: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[2023-12-13 09:45:49,829] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:45:49,829] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "2023-12-13 09:45:50.496774: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2023-12-13 09:45:51,242] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2023-12-13 09:45:51,666] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:45:51,666] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:45:52,923] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.15.5-1+cuda11.8\n",
            "[2023-12-13 09:45:52,923] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.15.5-1\n",
            "[2023-12-13 09:45:52,923] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.15.5-1\n",
            "[2023-12-13 09:45:52,923] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
            "[2023-12-13 09:45:52,923] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.15.5-1+cuda11.8\n",
            "[2023-12-13 09:45:52,923] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
            "[2023-12-13 09:45:52,923] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.15.5-1\n",
            "[2023-12-13 09:45:52,923] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2023-12-13 09:45:52,923] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2023-12-13 09:45:52,923] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2023-12-13 09:45:52,923] [INFO] [launch.py:163:main] dist_world_size=1\n",
            "[2023-12-13 09:45:52,923] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "[2023-12-13 09:45:54,833] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:45:54,833] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:45:55,196] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "2023-12-13 09:45:57.061922: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-13 09:45:57.061972: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-13 09:45:57.062006: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-13 09:45:58.164593: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2023-12-13 09:45:59,267] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:45:59,267] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "\rconfig.json:   0%|          | 0.00/800 [00:00<?, ?B/s]\rconfig.json: 100%|██████████| 800/800 [00:00<00:00, 4.31MB/s]\n",
            "[2023-12-13 09:45:59,838] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:45:59,838] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:   0%|          | 0.00/3.24G [00:00<?, ?B/s]\rpytorch_model.bin:   0%|          | 10.5M/3.24G [00:00<04:18, 12.5MB/s]\rpytorch_model.bin:   1%|          | 21.0M/3.24G [00:01<02:36, 20.5MB/s]\rpytorch_model.bin:   1%|          | 31.5M/3.24G [00:01<02:16, 23.5MB/s]\rpytorch_model.bin:   1%|▏         | 41.9M/3.24G [00:01<01:55, 27.8MB/s]\rpytorch_model.bin:   2%|▏         | 52.4M/3.24G [00:02<01:43, 30.7MB/s]\rpytorch_model.bin:   2%|▏         | 62.9M/3.24G [00:02<01:45, 30.2MB/s]\rpytorch_model.bin:   2%|▏         | 73.4M/3.24G [00:02<01:38, 32.2MB/s]\rpytorch_model.bin:   3%|▎         | 83.9M/3.24G [00:02<01:35, 33.0MB/s]\rpytorch_model.bin:   3%|▎         | 94.4M/3.24G [00:03<01:31, 34.4MB/s]\rpytorch_model.bin:   3%|▎         | 105M/3.24G [00:03<01:28, 35.4MB/s] \rpytorch_model.bin:   4%|▎         | 115M/3.24G [00:03<01:26, 36.1MB/s][2023-12-13 09:46:04,843] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:46:04,843] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:   4%|▍         | 126M/3.24G [00:04<01:25, 36.4MB/s]\rpytorch_model.bin:   4%|▍         | 136M/3.24G [00:04<01:24, 36.7MB/s]\rpytorch_model.bin:   5%|▍         | 147M/3.24G [00:04<01:23, 36.8MB/s]\rpytorch_model.bin:   5%|▍         | 157M/3.24G [00:04<01:28, 34.8MB/s]\rpytorch_model.bin:   5%|▌         | 168M/3.24G [00:05<01:26, 35.7MB/s]\rpytorch_model.bin:   6%|▌         | 178M/3.24G [00:05<01:22, 37.2MB/s]\rpytorch_model.bin:   6%|▌         | 189M/3.24G [00:05<01:29, 34.2MB/s]\rpytorch_model.bin:   6%|▌         | 199M/3.24G [00:06<01:26, 35.2MB/s]\rpytorch_model.bin:   6%|▋         | 210M/3.24G [00:06<01:20, 37.5MB/s]\rpytorch_model.bin:   7%|▋         | 220M/3.24G [00:06<01:17, 38.7MB/s]\rpytorch_model.bin:   7%|▋         | 231M/3.24G [00:06<01:17, 39.0MB/s]\rpytorch_model.bin:   7%|▋         | 241M/3.24G [00:07<01:23, 36.0MB/s]\rpytorch_model.bin:   8%|▊         | 252M/3.24G [00:07<01:21, 36.5MB/s]\rpytorch_model.bin:   8%|▊         | 262M/3.24G [00:07<01:15, 39.4MB/s]\rpytorch_model.bin:   8%|▊         | 273M/3.24G [00:08<01:29, 33.3MB/s]\rpytorch_model.bin:   9%|▊         | 283M/3.24G [00:08<01:26, 34.3MB/s]\rpytorch_model.bin:   9%|▉         | 294M/3.24G [00:08<01:29, 32.7MB/s][2023-12-13 09:46:09,847] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:46:09,847] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:   9%|▉         | 304M/3.24G [00:09<01:26, 34.0MB/s]\rpytorch_model.bin:  10%|▉         | 315M/3.24G [00:09<01:30, 32.5MB/s]\rpytorch_model.bin:  10%|█         | 325M/3.24G [00:09<01:26, 33.7MB/s]\rpytorch_model.bin:  10%|█         | 336M/3.24G [00:10<01:23, 34.8MB/s]\rpytorch_model.bin:  11%|█         | 346M/3.24G [00:10<01:26, 33.3MB/s]\rpytorch_model.bin:  11%|█         | 357M/3.24G [00:10<01:24, 33.9MB/s]\rpytorch_model.bin:  11%|█▏        | 367M/3.24G [00:11<01:45, 27.1MB/s]\rpytorch_model.bin:  12%|█▏        | 377M/3.24G [00:11<01:41, 28.3MB/s]\rpytorch_model.bin:  12%|█▏        | 388M/3.24G [00:11<01:33, 30.6MB/s]\rpytorch_model.bin:  12%|█▏        | 398M/3.24G [00:12<01:27, 32.4MB/s]\rpytorch_model.bin:  13%|█▎        | 409M/3.24G [00:12<01:26, 32.6MB/s]\rpytorch_model.bin:  13%|█▎        | 419M/3.24G [00:12<01:26, 32.6MB/s]\rpytorch_model.bin:  13%|█▎        | 430M/3.24G [00:13<01:24, 33.1MB/s]\rpytorch_model.bin:  14%|█▎        | 440M/3.24G [00:13<01:23, 33.3MB/s]\rpytorch_model.bin:  14%|█▍        | 451M/3.24G [00:13<01:20, 34.6MB/s]\rpytorch_model.bin:  14%|█▍        | 461M/3.24G [00:13<01:18, 35.5MB/s][2023-12-13 09:46:14,852] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:46:14,852] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  15%|█▍        | 472M/3.24G [00:14<01:17, 35.6MB/s]\rpytorch_model.bin:  15%|█▍        | 482M/3.24G [00:14<01:20, 34.0MB/s]\rpytorch_model.bin:  15%|█▌        | 493M/3.24G [00:14<01:18, 35.1MB/s]\rpytorch_model.bin:  16%|█▌        | 503M/3.24G [00:15<01:16, 35.8MB/s]\rpytorch_model.bin:  16%|█▌        | 514M/3.24G [00:15<01:14, 36.4MB/s]\rpytorch_model.bin:  16%|█▌        | 524M/3.24G [00:15<01:20, 33.8MB/s]\rpytorch_model.bin:  17%|█▋        | 535M/3.24G [00:16<01:17, 34.9MB/s]\rpytorch_model.bin:  17%|█▋        | 545M/3.24G [00:16<01:15, 35.7MB/s]\rpytorch_model.bin:  17%|█▋        | 556M/3.24G [00:16<01:13, 36.3MB/s]\rpytorch_model.bin:  17%|█▋        | 566M/3.24G [00:16<01:14, 35.6MB/s]\rpytorch_model.bin:  18%|█▊        | 577M/3.24G [00:17<01:19, 33.5MB/s]\rpytorch_model.bin:  18%|█▊        | 587M/3.24G [00:17<01:16, 34.7MB/s]\rpytorch_model.bin:  18%|█▊        | 598M/3.24G [00:17<01:14, 35.6MB/s]\rpytorch_model.bin:  19%|█▉        | 608M/3.24G [00:18<01:13, 35.8MB/s]\rpytorch_model.bin:  19%|█▉        | 619M/3.24G [00:18<01:18, 33.3MB/s]\rpytorch_model.bin:  19%|█▉        | 629M/3.24G [00:18<01:21, 31.8MB/s][2023-12-13 09:46:19,856] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:46:19,856] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  20%|█▉        | 640M/3.24G [00:19<01:24, 30.8MB/s]\rpytorch_model.bin:  20%|██        | 650M/3.24G [00:19<01:25, 30.1MB/s]\rpytorch_model.bin:  20%|██        | 661M/3.24G [00:19<01:20, 31.9MB/s]\rpytorch_model.bin:  21%|██        | 671M/3.24G [00:20<01:22, 31.0MB/s]\rpytorch_model.bin:  21%|██        | 682M/3.24G [00:20<01:23, 30.4MB/s]\rpytorch_model.bin:  21%|██▏       | 692M/3.24G [00:20<01:19, 32.1MB/s]\rpytorch_model.bin:  22%|██▏       | 703M/3.24G [00:21<01:20, 31.3MB/s]\rpytorch_model.bin:  22%|██▏       | 713M/3.24G [00:21<01:17, 32.7MB/s]\rpytorch_model.bin:  22%|██▏       | 724M/3.24G [00:21<01:18, 31.9MB/s]\rpytorch_model.bin:  23%|██▎       | 734M/3.24G [00:22<01:15, 33.0MB/s]\rpytorch_model.bin:  23%|██▎       | 744M/3.24G [00:22<01:12, 34.2MB/s]\rpytorch_model.bin:  23%|██▎       | 755M/3.24G [00:22<01:14, 33.2MB/s]\rpytorch_model.bin:  24%|██▎       | 765M/3.24G [00:23<01:13, 33.7MB/s]\rpytorch_model.bin:  24%|██▍       | 776M/3.24G [00:23<01:10, 34.8MB/s]\rpytorch_model.bin:  24%|██▍       | 786M/3.24G [00:23<01:14, 32.7MB/s]\rpytorch_model.bin:  25%|██▍       | 797M/3.24G [00:23<01:11, 34.1MB/s][2023-12-13 09:46:24,861] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:46:24,861] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  25%|██▍       | 807M/3.24G [00:24<01:09, 35.1MB/s]\rpytorch_model.bin:  25%|██▌       | 818M/3.24G [00:24<01:07, 35.8MB/s]\rpytorch_model.bin:  26%|██▌       | 828M/3.24G [00:24<01:06, 36.3MB/s]\rpytorch_model.bin:  26%|██▌       | 839M/3.24G [00:25<01:11, 33.6MB/s]\rpytorch_model.bin:  26%|██▌       | 849M/3.24G [00:25<01:13, 32.5MB/s]\rpytorch_model.bin:  27%|██▋       | 860M/3.24G [00:25<01:10, 33.9MB/s]\rpytorch_model.bin:  27%|██▋       | 870M/3.24G [00:26<01:17, 30.4MB/s]\rpytorch_model.bin:  27%|██▋       | 881M/3.24G [00:26<01:13, 31.9MB/s]\rpytorch_model.bin:  28%|██▊       | 891M/3.24G [00:26<01:15, 31.2MB/s]\rpytorch_model.bin:  28%|██▊       | 902M/3.24G [00:27<01:17, 30.1MB/s]\rpytorch_model.bin:  28%|██▊       | 912M/3.24G [00:27<01:12, 32.1MB/s]\rpytorch_model.bin:  29%|██▊       | 923M/3.24G [00:28<01:32, 25.0MB/s]\rpytorch_model.bin:  29%|██▉       | 933M/3.24G [00:28<01:22, 27.9MB/s]\rpytorch_model.bin:  29%|██▉       | 944M/3.24G [00:28<01:20, 28.6MB/s][2023-12-13 09:46:29,866] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:46:29,866] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  29%|██▉       | 954M/3.24G [00:29<01:14, 30.5MB/s]\rpytorch_model.bin:  30%|██▉       | 965M/3.24G [00:29<01:10, 32.3MB/s]\rpytorch_model.bin:  30%|███       | 975M/3.24G [00:29<01:10, 32.3MB/s]\rpytorch_model.bin:  30%|███       | 986M/3.24G [00:29<01:07, 33.6MB/s]\rpytorch_model.bin:  31%|███       | 996M/3.24G [00:30<01:14, 30.2MB/s]\rpytorch_model.bin:  31%|███       | 1.01G/3.24G [00:30<01:09, 32.1MB/s]\rpytorch_model.bin:  31%|███▏      | 1.02G/3.24G [00:30<01:06, 33.5MB/s]\rpytorch_model.bin:  32%|███▏      | 1.03G/3.24G [00:31<01:09, 31.8MB/s]\rpytorch_model.bin:  32%|███▏      | 1.04G/3.24G [00:31<01:11, 30.8MB/s]\rpytorch_model.bin:  32%|███▏      | 1.05G/3.24G [00:32<01:12, 30.1MB/s]\rpytorch_model.bin:  33%|███▎      | 1.06G/3.24G [00:32<01:13, 29.7MB/s]\rpytorch_model.bin:  33%|███▎      | 1.07G/3.24G [00:32<01:13, 29.5MB/s]\rpytorch_model.bin:  33%|███▎      | 1.08G/3.24G [00:33<01:13, 29.4MB/s]\rpytorch_model.bin:  34%|███▎      | 1.09G/3.24G [00:33<01:08, 31.3MB/s]\rpytorch_model.bin:  34%|███▍      | 1.10G/3.24G [00:33<01:10, 30.1MB/s][2023-12-13 09:46:34,870] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:46:34,870] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  34%|███▍      | 1.11G/3.24G [00:34<01:09, 30.5MB/s]\rpytorch_model.bin:  35%|███▍      | 1.12G/3.24G [00:34<01:07, 31.5MB/s]\rpytorch_model.bin:  35%|███▍      | 1.13G/3.24G [00:34<01:10, 29.9MB/s]\rpytorch_model.bin:  35%|███▌      | 1.14G/3.24G [00:35<01:05, 31.8MB/s]\rpytorch_model.bin:  36%|███▌      | 1.15G/3.24G [00:35<01:02, 33.3MB/s]\rpytorch_model.bin:  36%|███▌      | 1.16G/3.24G [00:35<01:04, 32.3MB/s]\rpytorch_model.bin:  36%|███▋      | 1.17G/3.24G [00:36<01:01, 33.3MB/s]\rpytorch_model.bin:  37%|███▋      | 1.18G/3.24G [00:36<01:04, 32.0MB/s]\rpytorch_model.bin:  37%|███▋      | 1.20G/3.24G [00:36<01:00, 33.5MB/s]\rpytorch_model.bin:  37%|███▋      | 1.21G/3.24G [00:37<01:11, 28.4MB/s]\rpytorch_model.bin:  38%|███▊      | 1.22G/3.24G [00:37<01:05, 30.7MB/s]\rpytorch_model.bin:  38%|███▊      | 1.23G/3.24G [00:37<01:01, 32.5MB/s]\rpytorch_model.bin:  38%|███▊      | 1.24G/3.24G [00:38<00:59, 33.8MB/s]\rpytorch_model.bin:  39%|███▊      | 1.25G/3.24G [00:38<01:00, 32.7MB/s]\rpytorch_model.bin:  39%|███▉      | 1.26G/3.24G [00:38<01:06, 29.7MB/s][2023-12-13 09:46:39,875] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:46:39,875] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  39%|███▉      | 1.27G/3.24G [00:39<01:02, 31.7MB/s]\rpytorch_model.bin:  40%|███▉      | 1.28G/3.24G [00:39<00:58, 33.3MB/s]\rpytorch_model.bin:  40%|███▉      | 1.29G/3.24G [00:39<00:59, 32.5MB/s]\rpytorch_model.bin:  40%|████      | 1.30G/3.24G [00:39<00:57, 33.9MB/s]\rpytorch_model.bin:  41%|████      | 1.31G/3.24G [00:40<01:24, 22.7MB/s]\rpytorch_model.bin:  41%|████      | 1.32G/3.24G [00:41<01:18, 24.5MB/s]\rpytorch_model.bin:  41%|████      | 1.33G/3.24G [00:41<01:09, 27.4MB/s]\rpytorch_model.bin:  41%|████▏     | 1.34G/3.24G [00:41<01:03, 29.9MB/s]\rpytorch_model.bin:  42%|████▏     | 1.35G/3.24G [00:41<01:01, 30.5MB/s]\rpytorch_model.bin:  42%|████▏     | 1.36G/3.24G [00:42<01:04, 29.2MB/s]\rpytorch_model.bin:  42%|████▏     | 1.37G/3.24G [00:42<01:01, 30.4MB/s]\rpytorch_model.bin:  43%|████▎     | 1.38G/3.24G [00:42<00:57, 32.2MB/s]\rpytorch_model.bin:  43%|████▎     | 1.39G/3.24G [00:43<00:59, 30.9MB/s]\rpytorch_model.bin:  43%|████▎     | 1.41G/3.24G [00:43<00:55, 32.7MB/s]\rpytorch_model.bin:  44%|████▎     | 1.42G/3.24G [00:43<00:53, 34.1MB/s][2023-12-13 09:46:44,877] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:46:44,877] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  44%|████▍     | 1.43G/3.24G [00:44<00:51, 35.0MB/s]\rpytorch_model.bin:  44%|████▍     | 1.44G/3.24G [00:44<00:50, 35.8MB/s]\rpytorch_model.bin:  45%|████▍     | 1.45G/3.24G [00:44<00:49, 35.9MB/s]\rpytorch_model.bin:  45%|████▌     | 1.46G/3.24G [00:45<00:48, 36.4MB/s]\rpytorch_model.bin:  45%|████▌     | 1.47G/3.24G [00:45<00:59, 29.9MB/s]\rpytorch_model.bin:  46%|████▌     | 1.48G/3.24G [00:46<01:04, 27.2MB/s]\rpytorch_model.bin:  46%|████▌     | 1.49G/3.24G [00:46<01:07, 25.9MB/s]\rpytorch_model.bin:  46%|████▋     | 1.50G/3.24G [00:46<01:13, 23.5MB/s]\rpytorch_model.bin:  47%|████▋     | 1.51G/3.24G [00:47<01:14, 23.3MB/s]\rpytorch_model.bin:  47%|████▋     | 1.52G/3.24G [00:47<01:18, 22.0MB/s]\rpytorch_model.bin:  47%|████▋     | 1.53G/3.24G [00:48<01:16, 22.2MB/s]\rpytorch_model.bin:  48%|████▊     | 1.54G/3.24G [00:48<01:15, 22.4MB/s][2023-12-13 09:46:49,882] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:46:49,882] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  48%|████▊     | 1.55G/3.24G [00:49<01:14, 22.6MB/s]\rpytorch_model.bin:  48%|████▊     | 1.56G/3.24G [00:49<01:13, 22.7MB/s]\rpytorch_model.bin:  49%|████▊     | 1.57G/3.24G [00:50<01:12, 22.8MB/s]\rpytorch_model.bin:  49%|████▉     | 1.58G/3.24G [00:50<01:12, 22.9MB/s]\rpytorch_model.bin:  49%|████▉     | 1.59G/3.24G [00:51<01:11, 22.9MB/s]\rpytorch_model.bin:  50%|████▉     | 1.60G/3.24G [00:51<01:11, 23.0MB/s]\rpytorch_model.bin:  50%|████▉     | 1.61G/3.24G [00:52<01:10, 23.0MB/s]\rpytorch_model.bin:  50%|█████     | 1.63G/3.24G [00:52<01:09, 23.0MB/s]\rpytorch_model.bin:  51%|█████     | 1.64G/3.24G [00:53<01:09, 23.0MB/s]\rpytorch_model.bin:  51%|█████     | 1.65G/3.24G [00:53<01:09, 23.0MB/s]\rpytorch_model.bin:  51%|█████     | 1.66G/3.24G [00:53<01:08, 23.0MB/s][2023-12-13 09:46:54,886] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:46:54,886] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  52%|█████▏    | 1.67G/3.24G [00:54<01:07, 23.1MB/s]\rpytorch_model.bin:  52%|█████▏    | 1.68G/3.24G [00:54<01:07, 23.1MB/s]\rpytorch_model.bin:  52%|█████▏    | 1.69G/3.24G [00:55<01:07, 23.1MB/s]\rpytorch_model.bin:  52%|█████▏    | 1.70G/3.24G [00:55<01:03, 24.4MB/s]\rpytorch_model.bin:  53%|█████▎    | 1.71G/3.24G [00:56<01:11, 21.3MB/s]\rpytorch_model.bin:  53%|█████▎    | 1.72G/3.24G [00:56<01:13, 20.6MB/s]\rpytorch_model.bin:  53%|█████▎    | 1.73G/3.24G [00:57<01:14, 20.2MB/s]\rpytorch_model.bin:  54%|█████▍    | 1.74G/3.24G [00:57<01:15, 19.9MB/s]\rpytorch_model.bin:  54%|█████▍    | 1.75G/3.24G [00:58<01:15, 19.7MB/s]\rpytorch_model.bin:  54%|█████▍    | 1.76G/3.24G [00:58<01:11, 20.5MB/s][2023-12-13 09:46:59,891] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:46:59,891] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  55%|█████▍    | 1.77G/3.24G [00:59<01:12, 20.2MB/s]\rpytorch_model.bin:  55%|█████▌    | 1.78G/3.24G [00:59<01:09, 20.9MB/s]\rpytorch_model.bin:  55%|█████▌    | 1.79G/3.24G [01:00<01:07, 21.5MB/s]\rpytorch_model.bin:  56%|█████▌    | 1.80G/3.24G [01:00<01:08, 20.9MB/s]\rpytorch_model.bin:  56%|█████▌    | 1.81G/3.24G [01:01<01:06, 21.4MB/s]\rpytorch_model.bin:  56%|█████▋    | 1.82G/3.24G [01:01<01:04, 21.9MB/s]\rpytorch_model.bin:  57%|█████▋    | 1.84G/3.24G [01:02<01:03, 22.2MB/s]\rpytorch_model.bin:  57%|█████▋    | 1.85G/3.24G [01:02<01:01, 22.5MB/s]\rpytorch_model.bin:  57%|█████▋    | 1.86G/3.24G [01:03<01:00, 22.6MB/s]\rpytorch_model.bin:  58%|█████▊    | 1.87G/3.24G [01:03<01:00, 22.8MB/s][2023-12-13 09:47:04,896] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:47:04,896] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  58%|█████▊    | 1.88G/3.24G [01:04<00:59, 22.9MB/s]\rpytorch_model.bin:  58%|█████▊    | 1.89G/3.24G [01:04<00:55, 24.1MB/s]\rpytorch_model.bin:  59%|█████▊    | 1.90G/3.24G [01:04<00:56, 23.8MB/s]\rpytorch_model.bin:  59%|█████▉    | 1.91G/3.24G [01:05<00:56, 23.6MB/s]\rpytorch_model.bin:  59%|█████▉    | 1.92G/3.24G [01:05<00:56, 23.4MB/s]\rpytorch_model.bin:  60%|█████▉    | 1.93G/3.24G [01:06<00:55, 23.4MB/s]\rpytorch_model.bin:  60%|█████▉    | 1.94G/3.24G [01:06<00:55, 23.3MB/s]\rpytorch_model.bin:  60%|██████    | 1.95G/3.24G [01:07<00:55, 23.2MB/s]\rpytorch_model.bin:  61%|██████    | 1.96G/3.24G [01:07<00:54, 23.2MB/s]\rpytorch_model.bin:  61%|██████    | 1.97G/3.24G [01:08<00:54, 23.2MB/s]\rpytorch_model.bin:  61%|██████    | 1.98G/3.24G [01:08<00:54, 23.1MB/s]\rpytorch_model.bin:  62%|██████▏   | 1.99G/3.24G [01:08<00:51, 24.3MB/s][2023-12-13 09:47:09,900] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:47:09,900] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  62%|██████▏   | 2.00G/3.24G [01:09<00:51, 24.0MB/s]\rpytorch_model.bin:  62%|██████▏   | 2.01G/3.24G [01:09<00:51, 23.7MB/s]\rpytorch_model.bin:  63%|██████▎   | 2.02G/3.24G [01:10<00:51, 23.5MB/s]\rpytorch_model.bin:  63%|██████▎   | 2.03G/3.24G [01:10<00:51, 23.5MB/s]\rpytorch_model.bin:  63%|██████▎   | 2.04G/3.24G [01:11<00:50, 23.4MB/s]\rpytorch_model.bin:  64%|██████▎   | 2.06G/3.24G [01:11<00:48, 24.5MB/s]\rpytorch_model.bin:  64%|██████▍   | 2.07G/3.24G [01:12<00:48, 24.2MB/s]\rpytorch_model.bin:  64%|██████▍   | 2.08G/3.24G [01:12<00:48, 24.0MB/s]\rpytorch_model.bin:  64%|██████▍   | 2.09G/3.24G [01:12<00:46, 25.0MB/s]\rpytorch_model.bin:  65%|██████▍   | 2.10G/3.24G [01:13<00:48, 23.6MB/s]\rpytorch_model.bin:  65%|██████▌   | 2.11G/3.24G [01:13<00:49, 23.0MB/s][2023-12-13 09:47:14,905] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:47:14,905] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  65%|██████▌   | 2.12G/3.24G [01:14<00:45, 24.4MB/s]\rpytorch_model.bin:  66%|██████▌   | 2.13G/3.24G [01:14<00:46, 24.1MB/s]\rpytorch_model.bin:  66%|██████▌   | 2.14G/3.24G [01:15<00:43, 25.3MB/s]\rpytorch_model.bin:  66%|██████▋   | 2.15G/3.24G [01:15<00:49, 21.9MB/s]\rpytorch_model.bin:  67%|██████▋   | 2.16G/3.24G [01:16<00:47, 22.7MB/s]\rpytorch_model.bin:  67%|██████▋   | 2.17G/3.24G [01:16<00:44, 24.2MB/s]\rpytorch_model.bin:  67%|██████▋   | 2.18G/3.24G [01:16<00:41, 25.4MB/s]\rpytorch_model.bin:  68%|██████▊   | 2.19G/3.24G [01:17<00:37, 27.5MB/s]\rpytorch_model.bin:  68%|██████▊   | 2.20G/3.24G [01:17<00:37, 27.9MB/s]\rpytorch_model.bin:  68%|██████▊   | 2.21G/3.24G [01:17<00:36, 28.3MB/s]\rpytorch_model.bin:  69%|██████▊   | 2.22G/3.24G [01:18<00:33, 30.5MB/s]\rpytorch_model.bin:  69%|██████▉   | 2.23G/3.24G [01:18<00:33, 30.0MB/s]\rpytorch_model.bin:  69%|██████▉   | 2.24G/3.24G [01:18<00:32, 30.1MB/s][2023-12-13 09:47:19,910] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:47:19,910] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  70%|██████▉   | 2.25G/3.24G [01:19<00:31, 31.6MB/s]\rpytorch_model.bin:  70%|██████▉   | 2.26G/3.24G [01:19<00:29, 33.2MB/s]\rpytorch_model.bin:  70%|███████   | 2.28G/3.24G [01:19<00:27, 34.4MB/s]\rpytorch_model.bin:  71%|███████   | 2.29G/3.24G [01:20<00:30, 31.1MB/s]\rpytorch_model.bin:  71%|███████   | 2.30G/3.24G [01:20<00:28, 32.8MB/s]\rpytorch_model.bin:  71%|███████▏  | 2.31G/3.24G [01:20<00:27, 34.1MB/s]\rpytorch_model.bin:  72%|███████▏  | 2.32G/3.24G [01:20<00:26, 35.2MB/s]\rpytorch_model.bin:  72%|███████▏  | 2.33G/3.24G [01:21<00:25, 36.0MB/s]\rpytorch_model.bin:  72%|███████▏  | 2.34G/3.24G [01:21<00:37, 23.9MB/s]\rpytorch_model.bin:  73%|███████▎  | 2.35G/3.24G [01:22<00:32, 27.0MB/s]\rpytorch_model.bin:  73%|███████▎  | 2.36G/3.24G [01:22<00:31, 27.5MB/s]\rpytorch_model.bin:  73%|███████▎  | 2.37G/3.24G [01:22<00:29, 29.3MB/s]\rpytorch_model.bin:  74%|███████▎  | 2.38G/3.24G [01:23<00:28, 29.8MB/s]\rpytorch_model.bin:  74%|███████▍  | 2.39G/3.24G [01:23<00:32, 26.4MB/s]\rpytorch_model.bin:  74%|███████▍  | 2.40G/3.24G [01:24<00:28, 28.8MB/s][2023-12-13 09:47:24,914] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:47:24,914] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  75%|███████▍  | 2.41G/3.24G [01:24<00:28, 29.1MB/s]\rpytorch_model.bin:  75%|███████▍  | 2.42G/3.24G [01:24<00:26, 31.0MB/s]\rpytorch_model.bin:  75%|███████▌  | 2.43G/3.24G [01:25<00:28, 27.8MB/s]\rpytorch_model.bin:  76%|███████▌  | 2.44G/3.24G [01:25<00:25, 31.5MB/s]\rpytorch_model.bin:  76%|███████▌  | 2.45G/3.24G [01:25<00:23, 33.2MB/s]\rpytorch_model.bin:  76%|███████▌  | 2.46G/3.24G [01:25<00:22, 33.8MB/s]\rpytorch_model.bin:  76%|███████▋  | 2.47G/3.24G [01:26<00:22, 34.6MB/s]\rpytorch_model.bin:  77%|███████▋  | 2.49G/3.24G [01:26<00:22, 33.9MB/s]\rpytorch_model.bin:  77%|███████▋  | 2.50G/3.24G [01:27<00:37, 19.9MB/s]\rpytorch_model.bin:  77%|███████▋  | 2.51G/3.24G [01:27<00:31, 23.2MB/s]\rpytorch_model.bin:  78%|███████▊  | 2.52G/3.24G [01:28<00:27, 26.1MB/s]\rpytorch_model.bin:  78%|███████▊  | 2.53G/3.24G [01:28<00:24, 28.8MB/s]\rpytorch_model.bin:  78%|███████▊  | 2.54G/3.24G [01:28<00:22, 31.0MB/s]\rpytorch_model.bin:  79%|███████▊  | 2.55G/3.24G [01:29<00:21, 32.7MB/s][2023-12-13 09:47:29,919] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:47:29,919] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  79%|███████▉  | 2.56G/3.24G [01:29<00:19, 33.9MB/s]\rpytorch_model.bin:  79%|███████▉  | 2.57G/3.24G [01:29<00:19, 34.8MB/s]\rpytorch_model.bin:  80%|███████▉  | 2.58G/3.24G [01:30<00:24, 27.0MB/s]\rpytorch_model.bin:  80%|████████  | 2.59G/3.24G [01:30<00:22, 29.0MB/s]\rpytorch_model.bin:  80%|████████  | 2.60G/3.24G [01:30<00:23, 26.9MB/s]\rpytorch_model.bin:  81%|████████  | 2.61G/3.24G [01:31<00:19, 31.5MB/s]\rpytorch_model.bin:  81%|████████  | 2.62G/3.24G [01:31<00:20, 29.4MB/s]\rpytorch_model.bin:  81%|████████▏ | 2.63G/3.24G [01:31<00:20, 29.2MB/s]\rpytorch_model.bin:  82%|████████▏ | 2.64G/3.24G [01:32<00:20, 29.1MB/s]\rpytorch_model.bin:  82%|████████▏ | 2.65G/3.24G [01:32<00:19, 29.2MB/s]\rpytorch_model.bin:  82%|████████▏ | 2.66G/3.24G [01:32<00:18, 31.0MB/s]\rpytorch_model.bin:  83%|████████▎ | 2.67G/3.24G [01:33<00:18, 30.4MB/s]\rpytorch_model.bin:  83%|████████▎ | 2.68G/3.24G [01:33<00:18, 30.0MB/s]\rpytorch_model.bin:  83%|████████▎ | 2.69G/3.24G [01:33<00:16, 31.9MB/s][2023-12-13 09:47:34,923] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:47:34,923] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  84%|████████▎ | 2.71G/3.24G [01:34<00:17, 31.0MB/s]\rpytorch_model.bin:  84%|████████▍ | 2.72G/3.24G [01:34<00:16, 30.6MB/s]\rpytorch_model.bin:  84%|████████▍ | 2.73G/3.24G [01:34<00:15, 32.1MB/s]\rpytorch_model.bin:  85%|████████▍ | 2.74G/3.24G [01:35<00:15, 31.4MB/s]\rpytorch_model.bin:  85%|████████▍ | 2.75G/3.24G [01:35<00:14, 32.8MB/s]\rpytorch_model.bin:  85%|████████▌ | 2.76G/3.24G [01:35<00:15, 31.9MB/s]\rpytorch_model.bin:  86%|████████▌ | 2.77G/3.24G [01:36<00:14, 33.1MB/s]\rpytorch_model.bin:  86%|████████▌ | 2.78G/3.24G [01:36<00:15, 29.6MB/s]\rpytorch_model.bin:  86%|████████▌ | 2.79G/3.24G [01:36<00:15, 29.5MB/s]\rpytorch_model.bin:  87%|████████▋ | 2.80G/3.24G [01:37<00:14, 30.8MB/s]\rpytorch_model.bin:  87%|████████▋ | 2.81G/3.24G [01:37<00:13, 31.1MB/s]\rpytorch_model.bin:  87%|████████▋ | 2.82G/3.24G [01:37<00:13, 31.0MB/s]\rpytorch_model.bin:  87%|████████▋ | 2.83G/3.24G [01:38<00:14, 28.8MB/s]\rpytorch_model.bin:  88%|████████▊ | 2.84G/3.24G [01:38<00:13, 29.0MB/s][2023-12-13 09:47:39,928] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:47:39,928] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  88%|████████▊ | 2.85G/3.24G [01:39<00:14, 26.6MB/s]\rpytorch_model.bin:  88%|████████▊ | 2.86G/3.24G [01:39<00:15, 24.0MB/s]\rpytorch_model.bin:  89%|████████▉ | 2.87G/3.24G [01:40<00:17, 21.1MB/s]\rpytorch_model.bin:  89%|████████▉ | 2.88G/3.24G [01:40<00:17, 20.5MB/s]\rpytorch_model.bin:  89%|████████▉ | 2.89G/3.24G [01:41<00:17, 20.0MB/s]\rpytorch_model.bin:  90%|████████▉ | 2.90G/3.24G [01:42<00:18, 17.9MB/s]\rpytorch_model.bin:  90%|█████████ | 2.92G/3.24G [01:42<00:19, 16.7MB/s]\rpytorch_model.bin:  90%|█████████ | 2.93G/3.24G [01:43<00:19, 16.0MB/s][2023-12-13 09:47:44,932] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:47:44,932] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  91%|█████████ | 2.94G/3.24G [01:44<00:18, 16.1MB/s]\rpytorch_model.bin:  91%|█████████ | 2.95G/3.24G [01:45<00:18, 15.6MB/s]\rpytorch_model.bin:  91%|█████████▏| 2.96G/3.24G [01:45<00:17, 15.8MB/s]\rpytorch_model.bin:  92%|█████████▏| 2.97G/3.24G [01:46<00:16, 16.0MB/s]\rpytorch_model.bin:  92%|█████████▏| 2.98G/3.24G [01:47<00:17, 14.9MB/s]\rpytorch_model.bin:  92%|█████████▏| 2.99G/3.24G [01:47<00:17, 14.2MB/s]\rpytorch_model.bin:  93%|█████████▎| 3.00G/3.24G [01:48<00:17, 13.7MB/s][2023-12-13 09:47:49,937] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:47:49,937] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  93%|█████████▎| 3.01G/3.24G [01:49<00:16, 13.5MB/s]\rpytorch_model.bin:  93%|█████████▎| 3.02G/3.24G [01:50<00:16, 13.3MB/s]\rpytorch_model.bin:  94%|█████████▎| 3.03G/3.24G [01:51<00:15, 13.6MB/s]\rpytorch_model.bin:  94%|█████████▍| 3.04G/3.24G [01:51<00:14, 13.4MB/s]\rpytorch_model.bin:  94%|█████████▍| 3.05G/3.24G [01:52<00:13, 13.6MB/s]\rpytorch_model.bin:  95%|█████████▍| 3.06G/3.24G [01:53<00:12, 13.7MB/s][2023-12-13 09:47:54,941] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:47:54,941] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  95%|█████████▍| 3.07G/3.24G [01:54<00:12, 13.6MB/s]\rpytorch_model.bin:  95%|█████████▌| 3.08G/3.24G [01:54<00:11, 13.8MB/s]\rpytorch_model.bin:  96%|█████████▌| 3.09G/3.24G [01:55<00:10, 13.7MB/s]\rpytorch_model.bin:  96%|█████████▌| 3.10G/3.24G [01:56<00:09, 13.7MB/s]\rpytorch_model.bin:  96%|█████████▌| 3.11G/3.24G [01:57<00:08, 13.9MB/s]\rpytorch_model.bin:  97%|█████████▋| 3.12G/3.24G [01:57<00:08, 13.8MB/s]\rpytorch_model.bin:  97%|█████████▋| 3.14G/3.24G [01:58<00:07, 14.0MB/s][2023-12-13 09:47:59,945] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:47:59,945] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  97%|█████████▋| 3.15G/3.24G [01:59<00:06, 14.1MB/s]\rpytorch_model.bin:  98%|█████████▊| 3.16G/3.24G [02:00<00:05, 14.5MB/s]\rpytorch_model.bin:  98%|█████████▊| 3.17G/3.24G [02:00<00:04, 14.5MB/s]\rpytorch_model.bin:  98%|█████████▊| 3.18G/3.24G [02:01<00:03, 15.0MB/s]\rpytorch_model.bin:  99%|█████████▊| 3.19G/3.24G [02:02<00:03, 15.4MB/s]\rpytorch_model.bin:  99%|█████████▉| 3.20G/3.24G [02:02<00:02, 16.3MB/s]\rpytorch_model.bin:  99%|█████████▉| 3.21G/3.24G [02:03<00:01, 16.5MB/s]\rpytorch_model.bin:  99%|█████████▉| 3.22G/3.24G [02:03<00:00, 17.2MB/s][2023-12-13 09:48:04,950] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:48:04,950] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin: 100%|█████████▉| 3.23G/3.24G [02:04<00:00, 18.5MB/s]\rpytorch_model.bin: 100%|██████████| 3.24G/3.24G [02:04<00:00, 18.7MB/s]\rpytorch_model.bin: 100%|██████████| 3.24G/3.24G [02:04<00:00, 26.0MB/s]\n",
            "[2023-12-13 09:48:09,954] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:48:09,954] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:48:14,959] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:48:14,959] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rvocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]\rvocab.json: 100%|██████████| 798k/798k [00:00<00:00, 2.43MB/s]\rvocab.json: 100%|██████████| 798k/798k [00:00<00:00, 2.42MB/s]\n",
            "\rmerges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\rmerges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.89MB/s]\rmerges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.88MB/s]\n",
            "[2023-12-13 09:48:19,964] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:48:19,964] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "> --------- MII Settings: ds_optimize=True, replace_with_kernel_inject=True, enable_cuda_graph=False \n",
            "[2023-12-13 09:48:23,355] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.12.4, git-hash=unknown, git-branch=unknown\n",
            "[2023-12-13 09:48:23,356] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
            "[2023-12-13 09:48:23,356] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
            "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/transformer_inference/build.ninja...\n",
            "Building extension module transformer_inference...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module transformer_inference...\n",
            "Time to load transformer_inference op: 0.09351205825805664 seconds\n",
            "[2023-12-13 09:48:23,831] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 2048, 'intermediate_size': 8192, 'heads': 32, 'num_hidden_layers': -1, 'dtype': torch.float32, 'pre_layer_norm': True, 'norm_type': <NormType.LayerNorm: 1>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.ReLU: 2>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'min_out_tokens': 1024, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000}\n",
            "[2023-12-13 09:48:24,965] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:48:24,965] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:48:29,970] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:48:29,970] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:48:29,970] [INFO] [server.py:64:_wait_until_server_is_live] server has started on ports [50051]\n",
            "[2023-12-13 09:48:29,970] [INFO] [server.py:64:_wait_until_server_is_live] server has started on ports [50051]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -i:50051"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_w1nkfndkjKw",
        "outputId": "e4459a1f-70ef-4adc-f8c8-b8d4edd601ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMMAND   PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\n",
            "python3 10919 root   40u  IPv4 262091      0t0  TCP *:50051 (LISTEN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cliet chat\n",
        "!cd DeepSpeed-MII/mii/legacy/examples/local/chat && python chat-client-example.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8snvrDj9are-",
        "outputId": "52efcf48-e2d0-4c08-9df0-8e6318da5c03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-13 09:59:01,271] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "2023-12-13 09:59:03.188707: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-13 09:59:03.188760: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-13 09:59:03.188785: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-13 09:59:04.323178: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2023-12-13 09:59:05,180] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:59:05,180] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "# Start a conversation session. Type 'q' to exit.\n",
            "You: hello , are u a bot?\n",
            "Bot: Yes, I am a bot.  Yes, I am programmed to provide information.\n",
            "You: Can you tell me about deep learning?\n",
            "Bot: Yes, it is a form of deep learning.  It is a technique for developing artificial intelligence that learns from data.  It allows computers to develop their own intelligence.\n",
            "You: I want to try it.\n",
            "Bot: Yes, it is a good idea to try deep learning.  It can help you develop your own intelligence.\n",
            "You: Is it hard to learn?\n",
            "Bot: Yes, it is hard to learn.  It requires a lot of practice and practice is necessary to develop an effective deep learning system.\n",
            "You: Where can I start?\n",
            "Bot: Yes, it is a good idea to start with deep learning.  It can help you develop your own intelligence.  You should start with simple tasks and gradually increase the complexity of the tasks as you become more proficient with the system.\n",
            "You: thank you\n",
            "Bot: You are welcome\n",
            "You: bye\n",
            "Bot: \n",
            "You: 88\n",
            "Bot: \n",
            "You: 拜\n",
            "Bot: \n",
            "You: q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mii\n",
        "mii.terminate(\"chat_example_deployment\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbNiSkexkJcR",
        "outputId": "4a01a0fb-51f2-435d-ac02-feb76ce535d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-13 10:01:33,497] [INFO] [terminate.py:12:terminate] Terminating server for chat_example_deployment\n",
            "[2023-12-13 10:01:33,497] [INFO] [terminate.py:12:terminate] Terminating server for chat_example_deployment\n",
            "[2023-12-13 10:01:33,501] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 10:01:33,502] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 10:01:33,540] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 10:01:33,541] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -i:50051"
      ],
      "metadata": {
        "id": "4XkEGm4ooAOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## more\n",
        "更多示例：\n",
        "https://github.com/microsoft/DeepSpeed-MII/tree/main/mii/legacy/examples/local"
      ],
      "metadata": {
        "id": "0-zJ_kwkYPp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh DeepSpeed-MII/mii/legacy/examples/local"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T40MpO32gcKw",
        "outputId": "012bc6f4-0702-4c7a-fcce-88ff3bc8e4ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 64K\n",
            "drwxr-xr-x 2 root root 4.0K Dec 13 09:11 chat\n",
            "-rw-r--r-- 1 root root  302 Dec 13 09:11 conversational-example.py\n",
            "-rw-r--r-- 1 root root  757 Dec 13 09:11 conversational-query-example.py\n",
            "-rw-r--r-- 1 root root  677 Dec 13 09:11 fill-mask-example.py\n",
            "-rw-r--r-- 1 root root  351 Dec 13 09:11 question-answering-example.py\n",
            "-rw-r--r-- 1 root root  386 Dec 13 09:11 question-answering-query-example.py\n",
            "-rw-r--r-- 1 root root  317 Dec 13 09:11 text-classification-example.py\n",
            "-rw-r--r-- 1 root root  410 Dec 13 09:11 text-classification-query-example.py\n",
            "-rw-r--r-- 1 root root  324 Dec 13 09:11 text-generation-bloom560m-example.py\n",
            "-rw-r--r-- 1 root root  456 Dec 13 09:11 text-generation-bloom-example.py\n",
            "-rw-r--r-- 1 root root  721 Dec 13 09:11 text-generation-fbopt-example.py\n",
            "-rw-r--r-- 1 root root  568 Dec 13 09:11 text-generation-query-example.py\n",
            "-rw-r--r-- 1 root root 1.4K Dec 13 09:11 text-generation-zero-example.py\n",
            "-rw-r--r-- 1 root root  288 Dec 13 09:11 token-classification-example.py\n",
            "-rw-r--r-- 1 root root  403 Dec 13 09:11 token-classification-query-example.py\n",
            "-rw-r--r-- 1 root root 1.3K Dec 13 09:11 txt2img-example.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd DeepSpeed-MII/mii/legacy/examples/local && (nohup python conversational-example.py &)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jd5oPp6ChEAk",
        "outputId": "11270784-6871-4083-d676-315572f46163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat DeepSpeed-MII/mii/legacy/examples/local/nohup.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3YOtxtqYQob",
        "outputId": "0f4597e6-c3c1-4c54-8105-b5647d3d079f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-13 09:34:21,814] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "2023-12-13 09:34:23.741561: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-13 09:34:23.741615: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-13 09:34:23.741646: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-13 09:34:24.887416: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Deploying microsoft/DialoGPT-large...\n",
            "[2023-12-13 09:35:02,358] [INFO] [deployment.py:75:deploy] ************* MII is using DeepSpeed Optimizations to accelerate your model *************\n",
            "[2023-12-13 09:35:02,358] [INFO] [deployment.py:75:deploy] ************* MII is using DeepSpeed Optimizations to accelerate your model *************\n",
            "[2023-12-13 09:35:02,360] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:35:02,360] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:35:02,365] [INFO] [server.py:38:__init__] Hostfile /job/hostfile not found, creating hostfile.\n",
            "[2023-12-13 09:35:02,365] [INFO] [server.py:38:__init__] Hostfile /job/hostfile not found, creating hostfile.\n",
            "[2023-12-13 09:35:02,367] [INFO] [server.py:103:_launch_server_process] MII server server launch: ['deepspeed', '-H', '/tmp/tmp_tzqzbzl', '-i', 'localhost:0', '--master_port', '29500', '--master_addr', 'localhost', '--no_ssh_check', '--no_local_rank', '--no_python', '/usr/bin/python3', '-m', 'mii.legacy.launch.multi_gpu_server', '--deployment-name', 'microsoft/DialoGPT-large_deployment', '--load-balancer-port', '50050', '--restful-gateway-port', '51080', '--server-port', '50051', '--model-config', 'eyJtb2RlbCI6ICJtaWNyb3NvZnQvRGlhbG9HUFQtbGFyZ2UiLCAidGFzayI6ICJjb252ZXJzYXRpb25hbCIsICJkdHlwZSI6ICJ0b3JjaC5mbG9hdDMyIiwgIm1vZGVsX3BhdGgiOiAiL3RtcC9taWlfbW9kZWxzIiwgImxvYWRfd2l0aF9zeXNfbWVtIjogZmFsc2UsICJtZXRhX3RlbnNvciI6IGZhbHNlLCAiZGVwbG95X3JhbmsiOiBbMF0sICJ0b3JjaF9kaXN0X3BvcnQiOiAyOTUwMCwgInJlcGxpY2FfbnVtIjogMSwgInJlcGxpY2FfY29uZmlncyI6IFt7Imhvc3RuYW1lIjogImxvY2FsaG9zdCIsICJ0ZW5zb3JfcGFyYWxsZWxfcG9ydHMiOiBbNTAwNTFdLCAidG9yY2hfZGlzdF9wb3J0IjogMjk1MDAsICJncHVfaW5kaWNlcyI6IFswXX1dLCAicHJvZmlsZV9tb2RlbF90aW1lIjogZmFsc2UsICJza2lwX21vZGVsX2NoZWNrIjogdHJ1ZSwgImhmX2F1dGhfdG9rZW4iOiBudWxsLCAidHJ1c3RfcmVtb3RlX2NvZGUiOiBmYWxzZSwgInBpcGVsaW5lX2t3YXJncyI6IHt9LCAiZW5hYmxlX2RlZXBzcGVlZCI6IHRydWUsICJlbmFibGVfemVybyI6IGZhbHNlLCAiZHNfY29uZmlnIjoge30sICJ0ZW5zb3JfcGFyYWxsZWwiOiAxLCAiZW5hYmxlX2N1ZGFfZ3JhcGgiOiBmYWxzZSwgInJlcGxhY2Vfd2l0aF9rZXJuZWxfaW5qZWN0IjogdHJ1ZSwgImNoZWNrcG9pbnRfZGljdCI6IG51bGwsICJtYXhfdG9rZW5zIjogMTAyNH0=']\n",
            "[2023-12-13 09:35:02,367] [INFO] [server.py:103:_launch_server_process] MII server server launch: ['deepspeed', '-H', '/tmp/tmp_tzqzbzl', '-i', 'localhost:0', '--master_port', '29500', '--master_addr', 'localhost', '--no_ssh_check', '--no_local_rank', '--no_python', '/usr/bin/python3', '-m', 'mii.legacy.launch.multi_gpu_server', '--deployment-name', 'microsoft/DialoGPT-large_deployment', '--load-balancer-port', '50050', '--restful-gateway-port', '51080', '--server-port', '50051', '--model-config', 'eyJtb2RlbCI6ICJtaWNyb3NvZnQvRGlhbG9HUFQtbGFyZ2UiLCAidGFzayI6ICJjb252ZXJzYXRpb25hbCIsICJkdHlwZSI6ICJ0b3JjaC5mbG9hdDMyIiwgIm1vZGVsX3BhdGgiOiAiL3RtcC9taWlfbW9kZWxzIiwgImxvYWRfd2l0aF9zeXNfbWVtIjogZmFsc2UsICJtZXRhX3RlbnNvciI6IGZhbHNlLCAiZGVwbG95X3JhbmsiOiBbMF0sICJ0b3JjaF9kaXN0X3BvcnQiOiAyOTUwMCwgInJlcGxpY2FfbnVtIjogMSwgInJlcGxpY2FfY29uZmlncyI6IFt7Imhvc3RuYW1lIjogImxvY2FsaG9zdCIsICJ0ZW5zb3JfcGFyYWxsZWxfcG9ydHMiOiBbNTAwNTFdLCAidG9yY2hfZGlzdF9wb3J0IjogMjk1MDAsICJncHVfaW5kaWNlcyI6IFswXX1dLCAicHJvZmlsZV9tb2RlbF90aW1lIjogZmFsc2UsICJza2lwX21vZGVsX2NoZWNrIjogdHJ1ZSwgImhmX2F1dGhfdG9rZW4iOiBudWxsLCAidHJ1c3RfcmVtb3RlX2NvZGUiOiBmYWxzZSwgInBpcGVsaW5lX2t3YXJncyI6IHt9LCAiZW5hYmxlX2RlZXBzcGVlZCI6IHRydWUsICJlbmFibGVfemVybyI6IGZhbHNlLCAiZHNfY29uZmlnIjoge30sICJ0ZW5zb3JfcGFyYWxsZWwiOiAxLCAiZW5hYmxlX2N1ZGFfZ3JhcGgiOiBmYWxzZSwgInJlcGxhY2Vfd2l0aF9rZXJuZWxfaW5qZWN0IjogdHJ1ZSwgImNoZWNrcG9pbnRfZGljdCI6IG51bGwsICJtYXhfdG9rZW5zIjogMTAyNH0=']\n",
            "[2023-12-13 09:35:02,368] [INFO] [server.py:103:_launch_server_process] load balancer server launch: ['/usr/bin/python3', '-m', 'mii.legacy.launch.multi_gpu_server', '--deployment-name', 'microsoft/DialoGPT-large_deployment', '--load-balancer-port', '50050', '--restful-gateway-port', '51080', '--load-balancer', '--model-config', 'eyJtb2RlbCI6ICJtaWNyb3NvZnQvRGlhbG9HUFQtbGFyZ2UiLCAidGFzayI6ICJjb252ZXJzYXRpb25hbCIsICJkdHlwZSI6ICJ0b3JjaC5mbG9hdDMyIiwgIm1vZGVsX3BhdGgiOiAiL3RtcC9taWlfbW9kZWxzIiwgImxvYWRfd2l0aF9zeXNfbWVtIjogZmFsc2UsICJtZXRhX3RlbnNvciI6IGZhbHNlLCAiZGVwbG95X3JhbmsiOiBbMF0sICJ0b3JjaF9kaXN0X3BvcnQiOiAyOTUwMCwgInJlcGxpY2FfbnVtIjogMSwgInJlcGxpY2FfY29uZmlncyI6IFt7Imhvc3RuYW1lIjogImxvY2FsaG9zdCIsICJ0ZW5zb3JfcGFyYWxsZWxfcG9ydHMiOiBbNTAwNTFdLCAidG9yY2hfZGlzdF9wb3J0IjogMjk1MDAsICJncHVfaW5kaWNlcyI6IFswXX1dLCAicHJvZmlsZV9tb2RlbF90aW1lIjogZmFsc2UsICJza2lwX21vZGVsX2NoZWNrIjogdHJ1ZSwgImhmX2F1dGhfdG9rZW4iOiBudWxsLCAidHJ1c3RfcmVtb3RlX2NvZGUiOiBmYWxzZSwgInBpcGVsaW5lX2t3YXJncyI6IHt9LCAiZW5hYmxlX2RlZXBzcGVlZCI6IHRydWUsICJlbmFibGVfemVybyI6IGZhbHNlLCAiZHNfY29uZmlnIjoge30sICJ0ZW5zb3JfcGFyYWxsZWwiOiAxLCAiZW5hYmxlX2N1ZGFfZ3JhcGgiOiBmYWxzZSwgInJlcGxhY2Vfd2l0aF9rZXJuZWxfaW5qZWN0IjogdHJ1ZSwgImNoZWNrcG9pbnRfZGljdCI6IG51bGwsICJtYXhfdG9rZW5zIjogMTAyNH0=']\n",
            "[2023-12-13 09:35:02,368] [INFO] [server.py:103:_launch_server_process] load balancer server launch: ['/usr/bin/python3', '-m', 'mii.legacy.launch.multi_gpu_server', '--deployment-name', 'microsoft/DialoGPT-large_deployment', '--load-balancer-port', '50050', '--restful-gateway-port', '51080', '--load-balancer', '--model-config', 'eyJtb2RlbCI6ICJtaWNyb3NvZnQvRGlhbG9HUFQtbGFyZ2UiLCAidGFzayI6ICJjb252ZXJzYXRpb25hbCIsICJkdHlwZSI6ICJ0b3JjaC5mbG9hdDMyIiwgIm1vZGVsX3BhdGgiOiAiL3RtcC9taWlfbW9kZWxzIiwgImxvYWRfd2l0aF9zeXNfbWVtIjogZmFsc2UsICJtZXRhX3RlbnNvciI6IGZhbHNlLCAiZGVwbG95X3JhbmsiOiBbMF0sICJ0b3JjaF9kaXN0X3BvcnQiOiAyOTUwMCwgInJlcGxpY2FfbnVtIjogMSwgInJlcGxpY2FfY29uZmlncyI6IFt7Imhvc3RuYW1lIjogImxvY2FsaG9zdCIsICJ0ZW5zb3JfcGFyYWxsZWxfcG9ydHMiOiBbNTAwNTFdLCAidG9yY2hfZGlzdF9wb3J0IjogMjk1MDAsICJncHVfaW5kaWNlcyI6IFswXX1dLCAicHJvZmlsZV9tb2RlbF90aW1lIjogZmFsc2UsICJza2lwX21vZGVsX2NoZWNrIjogdHJ1ZSwgImhmX2F1dGhfdG9rZW4iOiBudWxsLCAidHJ1c3RfcmVtb3RlX2NvZGUiOiBmYWxzZSwgInBpcGVsaW5lX2t3YXJncyI6IHt9LCAiZW5hYmxlX2RlZXBzcGVlZCI6IHRydWUsICJlbmFibGVfemVybyI6IGZhbHNlLCAiZHNfY29uZmlnIjoge30sICJ0ZW5zb3JfcGFyYWxsZWwiOiAxLCAiZW5hYmxlX2N1ZGFfZ3JhcGgiOiBmYWxzZSwgInJlcGxhY2Vfd2l0aF9rZXJuZWxfaW5qZWN0IjogdHJ1ZSwgImNoZWNrcG9pbnRfZGljdCI6IG51bGwsICJtYXhfdG9rZW5zIjogMTAyNH0=']\n",
            "[2023-12-13 09:35:04,701] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2023-12-13 09:35:04,792] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2023-12-13 09:35:06,441] [INFO] [runner.py:570:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=localhost --master_port=29500 --no_python --no_local_rank --enable_each_rank_log=None /usr/bin/python3 -m mii.legacy.launch.multi_gpu_server --deployment-name microsoft/DialoGPT-large_deployment --load-balancer-port 50050 --restful-gateway-port 51080 --server-port 50051 --model-config eyJtb2RlbCI6ICJtaWNyb3NvZnQvRGlhbG9HUFQtbGFyZ2UiLCAidGFzayI6ICJjb252ZXJzYXRpb25hbCIsICJkdHlwZSI6ICJ0b3JjaC5mbG9hdDMyIiwgIm1vZGVsX3BhdGgiOiAiL3RtcC9taWlfbW9kZWxzIiwgImxvYWRfd2l0aF9zeXNfbWVtIjogZmFsc2UsICJtZXRhX3RlbnNvciI6IGZhbHNlLCAiZGVwbG95X3JhbmsiOiBbMF0sICJ0b3JjaF9kaXN0X3BvcnQiOiAyOTUwMCwgInJlcGxpY2FfbnVtIjogMSwgInJlcGxpY2FfY29uZmlncyI6IFt7Imhvc3RuYW1lIjogImxvY2FsaG9zdCIsICJ0ZW5zb3JfcGFyYWxsZWxfcG9ydHMiOiBbNTAwNTFdLCAidG9yY2hfZGlzdF9wb3J0IjogMjk1MDAsICJncHVfaW5kaWNlcyI6IFswXX1dLCAicHJvZmlsZV9tb2RlbF90aW1lIjogZmFsc2UsICJza2lwX21vZGVsX2NoZWNrIjogdHJ1ZSwgImhmX2F1dGhfdG9rZW4iOiBudWxsLCAidHJ1c3RfcmVtb3RlX2NvZGUiOiBmYWxzZSwgInBpcGVsaW5lX2t3YXJncyI6IHt9LCAiZW5hYmxlX2RlZXBzcGVlZCI6IHRydWUsICJlbmFibGVfemVybyI6IGZhbHNlLCAiZHNfY29uZmlnIjoge30sICJ0ZW5zb3JfcGFyYWxsZWwiOiAxLCAiZW5hYmxlX2N1ZGFfZ3JhcGgiOiBmYWxzZSwgInJlcGxhY2Vfd2l0aF9rZXJuZWxfaW5qZWN0IjogdHJ1ZSwgImNoZWNrcG9pbnRfZGljdCI6IG51bGwsICJtYXhfdG9rZW5zIjogMTAyNH0=\n",
            "2023-12-13 09:35:06.733281: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-13 09:35:06.733335: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-13 09:35:06.733361: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[2023-12-13 09:35:07,373] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:35:07,373] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "2023-12-13 09:35:07.945468: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2023-12-13 09:35:08,670] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2023-12-13 09:35:09,107] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:35:09,107] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:35:10,326] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.15.5-1+cuda11.8\n",
            "[2023-12-13 09:35:10,326] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.15.5-1\n",
            "[2023-12-13 09:35:10,326] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.15.5-1\n",
            "[2023-12-13 09:35:10,326] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
            "[2023-12-13 09:35:10,326] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.15.5-1+cuda11.8\n",
            "[2023-12-13 09:35:10,326] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
            "[2023-12-13 09:35:10,326] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.15.5-1\n",
            "[2023-12-13 09:35:10,326] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2023-12-13 09:35:10,326] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2023-12-13 09:35:10,326] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2023-12-13 09:35:10,326] [INFO] [launch.py:163:main] dist_world_size=1\n",
            "[2023-12-13 09:35:10,326] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "[2023-12-13 09:35:12,378] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:35:12,378] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:35:12,590] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "2023-12-13 09:35:14.450109: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-13 09:35:14.450168: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-13 09:35:14.450206: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-13 09:35:15.559751: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2023-12-13 09:35:16,710] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:35:16,710] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:35:17,382] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:35:17,382] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:35:22,387] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:35:22,387] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:35:27,392] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:35:27,392] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "> --------- MII Settings: ds_optimize=True, replace_with_kernel_inject=True, enable_cuda_graph=False \n",
            "[2023-12-13 09:35:28,542] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.12.4, git-hash=unknown, git-branch=unknown\n",
            "[2023-12-13 09:35:28,542] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
            "[2023-12-13 09:35:28,543] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
            "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/transformer_inference/build.ninja...\n",
            "Building extension module transformer_inference...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module transformer_inference...\n",
            "Time to load transformer_inference op: 0.09584593772888184 seconds\n",
            "[2023-12-13 09:35:29,007] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 1280, 'intermediate_size': 5120, 'heads': 20, 'num_hidden_layers': -1, 'dtype': torch.float32, 'pre_layer_norm': True, 'norm_type': <NormType.LayerNorm: 1>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'min_out_tokens': 1024, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000}\n",
            "[2023-12-13 09:35:32,397] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:35:32,397] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:35:37,402] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:35:37,402] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:35:37,402] [INFO] [server.py:64:_wait_until_server_is_live] server has started on ports [50051]\n",
            "[2023-12-13 09:35:37,402] [INFO] [server.py:64:_wait_until_server_is_live] server has started on ports [50051]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd DeepSpeed-MII/mii/legacy/examples/local && python conversational-query-example.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkEnx0FLgyrr",
        "outputId": "796ea767-bf9c-4e16-abd6-bd3b641ae302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-13 09:41:57,043] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "2023-12-13 09:41:58.932739: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-13 09:41:58.932786: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-13 09:41:58.932813: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-13 09:42:00.069422: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Querying microsoft/DialoGPT-large...\n",
            "[2023-12-13 09:42:00,919] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:42:00,919] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "conversation_id: \"6af613b6-569c-5c22-9c37-2ed93f31d3af\"\n",
            "past_user_inputs: \"DeepSpeed is the greatest\"\n",
            "generated_responses: \"I love it. It\\'s so much fun.\"\n",
            "time_taken: 0.218658924\n",
            "model_time_taken: -1\n",
            "\n",
            "time_taken: 0.2186589241027832\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/DeepSpeed-MII/mii/legacy/examples/local/conversational-query-example.py\", line 30, in <module>\n",
            "    result = generator.query({\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mii/legacy/client.py\", line 77, in query\n",
            "    return self.asyncio_loop.run_until_complete(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 99, in run_until_complete\n",
            "    return f.result()\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mii/legacy/client.py\", line 72, in _request_async_response\n",
            "    proto_request = task_methods.pack_request_to_proto(request_dict, **query_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mii/legacy/method_table.py\", line 227, in pack_request_to_proto\n",
            "    return modelresponse_pb2.ConversationRequest(\n",
            "TypeError: bad argument type for built-in operation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -i:50051"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GnJmsWViIPY",
        "outputId": "01e749fc-ad0c-4e93-82c6-36ec6a0babed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\n",
            "python3 7710 root   23u  IPv4 190929      0t0  TCP localhost:45268->localhost:50051 (ESTABLISHED)\n",
            "python3 7844 root   39u  IPv4 185094      0t0  TCP *:50051 (LISTEN)\n",
            "python3 7844 root   40u  IPv4 189060      0t0  TCP localhost:50051->localhost:45268 (ESTABLISHED)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mii\n",
        "mii.terminate(\"microsoft/DialoGPT-large_deployment\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ob07Ri3Cj37Z",
        "outputId": "ce45648d-049f-417c-bfdf-71892d92d8f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-13 09:43:59,850] [INFO] [terminate.py:12:terminate] Terminating server for microsoft/DialoGPT-large_deployment\n",
            "[2023-12-13 09:43:59,850] [INFO] [terminate.py:12:terminate] Terminating server for microsoft/DialoGPT-large_deployment\n",
            "[2023-12-13 09:43:59,854] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:43:59,855] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:43:59,861] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:43:59,862] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -i:50051"
      ],
      "metadata": {
        "id": "9Hc1PHeij-p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DeepSpeed-FastGen\n",
        "\n",
        "https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-fastgen/chinese/README.md\n",
        "\n",
        "ds-mii-fastgen 是为了解决LLMs推理加速，提高推理吞吐，以及长prompt问题(llm已经支持长token,树万token)。\n",
        "\n",
        "GPT-4 和 LLaMA 这样的大型语言模型（LLMs）已在各个层次上成为了集成 AI 的主流服务应用。从常规聊天模型到文档摘要，从自动驾驶到各个软件中的Copilot功能，这些模型的部署和服务需求正在迅速增加。像 DeepSpeed、PyTorch 和其他几个框架可以在 LLM 训练期间实现良好的硬件利用率。但它们在与用户互动及处理开放式文本生成等任务时，受限于这些操作的计算密集度相对较低，现有系统往往在推理吞吐量上遇到瓶颈。\n",
        "\n",
        "为了解决这一问题， vLLM 这样由 PagedAttention 驱动的框架和 [Orca](https://www.usenix.org/system/files/osdi22-yu.pdf) 这样的系统显著提高了 LLM 推理的性能。然而，这些系统在面对长提示的工作负载时，依旧难以提供良好的服务质量。随着越来越多的模型（例如 MPT-StoryWriter）和系统（例如DeepSpeed Ulysses）支持延伸到数万个令牌的上下文窗口，这些长提示工作负载变得越来越重要。为了更好地理解问题，我们在下文中提供了详细的示例来说明 LLM 的文本生成是如何在“提示处理”和“生成”的这两个阶段中工作的。当系统将它们视为不同的阶段时，生成阶段将被提示处理所抢占，这可能会破坏服务级别协议（SLAs）。\n",
        "\n",
        "tips:\n",
        "- vllm 跟进 https://github.com/vllm-project/vllm/issues/1562\n",
        "\n",
        "DeepSpeed-FastGen 框架，它通过采用动态 SplitFuse 技术，能够提供比vLLM 等先进系统高出多达 2.3 倍的有效吞吐量。DeepSpeed-FastGen 是 DeepSpeed-MII 和 DeepSpeed-Inference 的结合，提供了一个易于使用的服务系统\n",
        "\n",
        "\n",
        "\n",
        "DeepSpeed-FastGen 是 [DeepSpeed-MII](https://github.com/microsoft/DeepSpeed-MII) 和 [DeepSpeed-Inference](https://github.com/microsoft/DeepSpeed) 的协同组合，如下图所示。这两个软件包共同提供了系统的各个组成部分，包括前端 API、用于使用动态 SplitFuse 调度批次的主机和设备基础设施、优化的内核实现，以及构建新模型实现的工具。\n",
        "![](https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-fastgen/assets/images/fastgen-arch-light.png?raw=true)\n"
      ],
      "metadata": {
        "id": "OqJfbV1oPadS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 现有 LLM 服务技术\n",
        "\n",
        "单个序列的文本生成工作负载包含两个阶段：1）提示处理，此阶段系统处理用户输入的文本，将其转换成一系列令牌并构建用于注意力机制的键值（KV）缓存；2）生成令牌，即向缓存中添加单个令牌并产生新的令牌。在生成文本序列的过程中，系统将对模型进行多次前向调用以生成完整的文本序列。现有文献和系统中已经提出了两种主要技术，它们解决了这些阶段中可能出现的各种限制和瓶颈。\n",
        "\n",
        "分块 KV 缓存：\n",
        "\n",
        "vLLM识别出大型单体KV缓存导致的内存碎片化显著降低了大型语言模型服务系统的并发性，并提出了“分页注意力”Paged Attention 机制来实现非连续KV缓存，并增加整个系统的总吞吐量。此技术采用分页缓存机制，从而提升了系统的整体吞吐量。不同于之前分配各个不同大小的连续内存块的做法，分块 KV 缓存中的底层存储是固定大小的块（也称为页面）。分块 KV 缓存通过消除 KV 缓存引起的内存碎片化，增加了潜在的序列并发量，从而增加了系统吞吐量。非连续 KV 缓存也被 HuggingFace TGI 和 NVIDIA TensorRT-LLM 等框架所实现。\n",
        "\n",
        "连续批处理：\n",
        "\n",
        "过去，动态批处理（服务器等待多个请求以同步处理）被用来提高 GPU 利用率。然而，这种方法有缺点，因为它通常需要将输入填充到相同长度或使系统等待以构建更大的批次（batch）。\n",
        "\n",
        "近期大型语言模型（LLM）推理和服务的优化一直专注于细粒度调度和优化内存效率。例如，Orca 提出了 迭代级调度（也称为连续批处理），它在模型的每次前向传递时作出独特的调度决策。这允许请求根据需要加入/离开批次，从而消除了填充请求的需要，提高了总体吞吐量。除了 Orca，NVIDIA TRT-LLM、HuggingFace TGI 和 vLLM 也实现了连续批处理。\n",
        "\n",
        "在当前系统中，有两种主要方法来实现连续批处理。在 TGI 和 vLLM 中，生成阶段被抢占以执行提示处理（在 TGI 中称为填充）然后继续生成。在 Orca 中，这些阶段不被区分；相反，只要总序列数没有达到固定限制，Orca 就会将提示加入正在运行的批次中。这两种方法都在不同程度上需要暂停生成以处理长提示\n",
        "\n",
        "为了解决这些缺点，我们提出了一种新颖的提示和生成组合策略，动态 SplitFuse。\n",
        "\n"
      ],
      "metadata": {
        "id": "hSrI8HhrabUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 动态 SplitFuse：一种新颖的提示和生成组合策略\n",
        "\n",
        "类似于现有的框架如 TRT-LLM、TGI 和 vLLM，DeepSpeed-FastGen 的目标是利用连续批处理和非连续 KV 缓存技术，以提升数据中心服务大型语言模型（LLM）的硬件利用率和响应速度。为了实现更高的性能，DeepSpeed-FastGen 提出了 SplitFuse 技术，它利用动态提示和生成分解, 统一来进一步改善连续批处理和系统吞吐量。\n",
        "\n",
        "\n",
        "### 动态分割融合（Dynamic SplitFuse）\n",
        "动态分割融合是一种用于提示处理和令牌生成的新型令牌组成策略。DeepSpeed-FastGen 利用动态分割融合策略，通过从提示中取出部分令牌并与生成过程相结合，使得模型可以保持一致的前向传递大小（forward size）。具体来说，动态分割融合执行两个关键行为：\n",
        "\n",
        "1. 将长提示分解成更小的块，并在多个前向传递（迭代）中进行调度，只有在最后一个传递中才执行生成。\n",
        "2. 短提示将被组合以精确填满目标令牌预算。即使是短提示也可能被分解，以确保预算被精确满足，前向大小（forward sizes）保持良好对齐。\n",
        "\n",
        "动态分割融合（Dynamic SplitFuse）提升了以下性能指标：\n",
        "\n",
        "1. 更好的响应性： 由于长提示不再需要极长的前向传递来处理，模型将提供更低的客户端延迟。在同一时间窗口内执行的前向传递更多。\n",
        "2. 更高的效率： 短提示的融合到更大的令牌预算使模型能够持续运行在高吞吐量状态。\n",
        "3. 更低的波动和更好的一致性： 由于前向传递的大小一致，且前向传递大小是性能的主要决定因素，每个前向传递的延迟比其他系统更加一致。生成频率也是如此，因为DeepSpeed-FastGen不需要像其他先前的系统那样抢占或长时间运行提示，因此延迟会更低。\n",
        "因此，与现有最先进的服务系统相比，DeepSpeed-FastGen 将以允许快速、持续生成的速率消耗来自提示的令牌，同时向系统添加令牌，提高系统利用率，提供更低的延迟和更高的吞吐量流式生成给所有客户端。\n",
        "\n"
      ],
      "metadata": {
        "id": "I8hGJn6Zam5H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  性能评估\n",
        "\n",
        "DeepSpeed-FastGen 利用分块 KV 缓存和动态分割融合连续批处理，提供了最先进的 LLM 服务性能。我们以下述的基准测试方法对 DeepSpeed-FastGen 和 vLLM 在一系列模型和硬件配置上进行评估。\n",
        "\n",
        "我们采用两种主要的定量方法来衡量性能。\n",
        "\n",
        "吞吐量-延迟曲线： 生产环境的两个关键指标是吞吐量（以每秒请求计）和延迟（每个请求的响应性）。为了衡量这一点，我们模拟了多个客户端（数量从 1 到 32 不等）同时向服务器发送请求（总计 512 个）的情况。每个请求的结果延迟在端点测量，吞吐量通过完成实验的端到端时间来测量。\n",
        "\n",
        "有效吞吐量（用户体验SLA）： 诸如聊天应用程序之类的交互式应用程序可能有比上述指标（如端到端延迟）更严格和复杂的要求。以越来越受欢迎的聊天应用为例：\n",
        "\n",
        "用户通过发送提示（输入）来开始对话。\n",
        "系统处理提示并返回第一个令牌。\n",
        "随着生成的进行，后续令牌被流式传输给用户。\n",
        "在这个过程的每个阶段，系统都有可能提供不利的用户体验；例如，第一个令牌到达得太慢；或生成似乎停止了一段时间。我们提出了一个考虑这两个维度的 SLA 框架。\n",
        "\n",
        "由于提示和生成文本的长度差异很大，影响计算成本，因此设定同一个 SLA 值对于吞吐量和延迟是不切实际的。因此，我们将提示延迟的 SLA 定义为 “|提示中的令牌|/512” 秒（= 512 令牌/秒）。此外，考虑到人类的阅读速度，我们将生成延迟的 SLA 设置在指数移动平均（EMA）上为 2、4 或 6 令牌/秒。能够达到这些 SLA 的请求被认为是成功的，这些成功请求的吞吐量被称为有效吞吐量。\n",
        "\n",
        "我们通过在 NVIDIA A100、H100 和 A6000 上运行 Llama-2 7B、Llama-2 13B 和 Llama-2 70B 对 vLLM 和 DeepSpeed-FastGen进行了评估。\n",
        "\n",
        "DeepSpeed-FastGen 提供了副本级负载均衡，可以将请求均匀分布在多个服务器上，轻松扩展应用程序。\n",
        "\n",
        "Tips:\n",
        "\n",
        "在评估使用推理服务成本的时候，需要结合当前用户请求峰值进行评估，结合A100-40/80G卡作为基础卡来部署，需要评估整体算力成本； gpu利用率上去了，性能可以水平扩展，吞吐线性增长就更好了。\n",
        "\n"
      ],
      "metadata": {
        "id": "qU7ZBXbLeW3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 部署选项\n",
        "示例均可在 DeepSpeedExamples 中运行。安装后，有两种部署方式：交互式非持久管道或持久化服务部署："
      ],
      "metadata": {
        "id": "EqMkpc3OORnE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 非持久管道\n",
        "非持久管道部署是快速入门的好方法，只需几行代码即可完成。非持久模型只在您运行的 python 脚本期间存在，适用于临时交互式会话。\n",
        "\n"
      ],
      "metadata": {
        "id": "joIQE6ufOaR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/microsoft/DeepSpeed-MII/issues/273 need SM>=8.0 (Ampere+) A100\n",
        "# if use T4 GPU , need use https://github.com/microsoft/DeepSpeed-MII/tree/main/mii/legacy\n",
        "from mii import pipeline\n",
        "pipe = pipeline(\"mistralai/Mistral-7B-v0.1\")\n",
        "output = pipe([\"Hello, my name is\", \"DeepSpeed is\"], max_new_tokens=128)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "C-Gkv5ZAHgBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 持久部署\n",
        "持久部署非常适合用于长时间运行和生产的应用。持久部署使用了轻量级的 GRPC 服务器，可以使用以下两行代码创建：\n",
        "\n"
      ],
      "metadata": {
        "id": "VmbRsaW7OdGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mii\n",
        "mii.serve(\"mistralai/Mistral-7B-v0.1\")\n"
      ],
      "metadata": {
        "id": "UdH3NCrBOfjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 上述服务器可以同时被多个客户端查询，这要归功于 DeepSpeed-MII 内置的负载平衡器。创建客户端也只需要两行代码：\n",
        "\n",
        "client = mii.client(\"mistralai/Mistral-7B-v0.1\")\n",
        "output = client.generate(\"Deepspeed is\", max_new_tokens=128)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "KruNB7amO1Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#持久部署可以在不再需要时终止：\n",
        "\n",
        "client.terminate_server()\n"
      ],
      "metadata": {
        "id": "1DzeTAl4O9Vg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}