{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOFGI3Qw8UhVdKY7p4Mcj1/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weedge/doraemon-nb/blob/main/achatbot_kimivl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# install"
      ],
      "metadata": {
        "id": "-DSxG0KC86f9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CepHVoTB7OCO"
      },
      "outputs": [],
      "source": [
        "!cd /content && rm -rf achatbot && git clone --recursive https://github.com/ai-bot-pro/achatbot.git -b feat/vision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/achatbot"
      ],
      "metadata": {
        "id": "P1i4Vmbr712c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/pypi_achatbot.sh dev\n"
      ],
      "metadata": {
        "id": "K5zg_Mi278Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"dist/achatbot-0.0.9.30-py3-none-any.whl[llm_transformers_manual_vision_kimi,daily_room_audio_stream,livekit,livekit-api,sense_voice_asr,deepgram_asr_processor,tts_edge,openai,queue]\""
      ],
      "metadata": {
        "id": "1R-SA5Ci78xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep -E \"transformers|accelerate|torch|tiktoken|blobfile|torchvision|numpy\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9GjsMVl1Y5V",
        "outputId": "63b3cbd4-abd1-4e93-d087-0356d4b7094b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accelerate                         0.28.0\n",
            "blobfile                           3.0.0\n",
            "numpy                              1.26.0\n",
            "pytorch-wpe                        0.0.1\n",
            "sentence-transformers              3.4.1\n",
            "tiktoken                           0.7.0\n",
            "torch                              2.2.2\n",
            "torch-complex                      0.4.4\n",
            "torchaudio                         2.2.2\n",
            "torchsummary                       1.5.1\n",
            "torchvision                        0.17.2\n",
            "transformers                       4.50.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# download"
      ],
      "metadata": {
        "id": "CK8tWbDguJck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download FunAudioLLM/SenseVoiceSmall --quiet --local-dir /content/models/FunAudioLLM/SenseVoiceSmall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4QJvMXAfrGK",
        "outputId": "9c934bad-a3c4-4a56-82f3-1d18923f5daa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/FunAudioLLM/SenseVoiceSmall\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download --quiet Qwen/Qwen2-VL-7B-Instruct --local-dir /content/models/Qwen/Qwen2-VL-7B-Instruct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9yVNAswfUz1",
        "outputId": "de043fef-b86a-4515-ac97-031d776f8985"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/Qwen/Qwen2-VL-7B-Instruct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download --quie moonshotai/Kimi-VL-A3B-Thinking --local-dir /content/models/moonshotai/Kimi-VL-A3B-Thinking"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxjfrizyuLjX",
        "outputId": "0bc52b73-863d-477f-9ab9-e142d4f22344"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/moonshotai/Kimi-VL-A3B-Thinking\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download --quie moonshotai/Kimi-VL-A3B-Instruct --local-dir /content/models/moonshotai/Kimi-VL-A3B-Instruct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj5vBCIvveup",
        "outputId": "da036c34-9b90-4a49-f490-35acc5b8a51e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/models/moonshotai/Kimi-VL-A3B-Instruct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test"
      ],
      "metadata": {
        "id": "bziiC0sPROau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!LLM_DEVICE=cuda PROMPT=讲一个故事 LLM_MODEL_NAME_OR_PATH=/content/models/moonshotai/Kimi-VL-A3B-Instruct \\\n",
        "  python -m unittest test.core.llm.test_transformers_v_kimi.TestTransformersVKimi.test_chat_completion_prompts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ds1gTmkqRP5g",
        "outputId": "d7588350-558f-43de-a29a-2bd921d8086c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-10 14:08:32,896 - numexpr.utils - INFO - /usr/local/lib/python3.11/dist-packages/numexpr/utils.py:162 - _init_num_threads - NumExpr defaulting to 12 threads.\n",
            "2025-04-10 14:08:34.491581: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-10 14:08:34.510566: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744294114.532381   17984 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744294114.539183   17984 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-10 14:08:34.561300: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-04-10 14:08:35,713 - tensorflow - DEBUG - /usr/local/lib/python3.11/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py:34 - <module> - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "2025-04-10 14:08:35,875 - h5py._conv - DEBUG - /usr/local/lib/python3.11/dist-packages/h5py/__init__.py:47 - <module> - Creating converter from 7 to 5\n",
            "2025-04-10 14:08:35,875 - h5py._conv - DEBUG - /usr/local/lib/python3.11/dist-packages/h5py/__init__.py:47 - <module> - Creating converter from 5 to 7\n",
            "2025-04-10 14:08:35,875 - h5py._conv - DEBUG - /usr/local/lib/python3.11/dist-packages/h5py/__init__.py:47 - <module> - Creating converter from 7 to 5\n",
            "2025-04-10 14:08:35,875 - h5py._conv - DEBUG - /usr/local/lib/python3.11/dist-packages/h5py/__init__.py:47 - <module> - Creating converter from 5 to 7\n",
            "DEBUG:2025-04-10 14:08:36,312:jax._src.path:31: etils.epath found. Using etils.epath for file I/O.\n",
            "2025-04-10 14:08:36,312 - jax._src.path - DEBUG - /usr/local/lib/python3.11/dist-packages/jax/_src/path.py:31 - <module> - etils.epath found. Using etils.epath for file I/O.\n",
            "2025-04-10 14:08:36,864 - matplotlib - DEBUG - /usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py:342 - wrapper - matplotlib data path: /usr/local/lib/python3.11/dist-packages/matplotlib/mpl-data\n",
            "2025-04-10 14:08:36,870 - matplotlib - DEBUG - /usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py:342 - wrapper - CONFIGDIR=/root/.config/matplotlib\n",
            "2025-04-10 14:08:36,896 - matplotlib - DEBUG - /usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py:1557 - <module> - interactive is False\n",
            "2025-04-10 14:08:36,896 - matplotlib - DEBUG - /usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py:1558 - <module> - platform is linux\n",
            "2025-04-10 14:08:36,951 - matplotlib - DEBUG - /usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py:342 - wrapper - CACHEDIR=/root/.cache/matplotlib\n",
            "2025-04-10 14:08:36,952 - matplotlib.font_manager - DEBUG - /usr/local/lib/python3.11/dist-packages/matplotlib/font_manager.py:1635 - _load_fontmanager - Using fontManager instance from /root/.cache/matplotlib/fontlist-v390.json\n",
            "2025-04-10 14:08:37,597 - qwen_vl_utils.vision_process - INFO - /usr/local/lib/python3.11/dist-packages/qwen_vl_utils/vision_process.py:41 - <module> - set VIDEO_TOTAL_PIXELS: 90316800\n",
            "2025-04-10 14:08:37,597 - chat-bot - INFO - /content/achatbot/src/common/factory.py:69 - get_engine_by_tag - use llm_transformers_manual_vision_kimi engine\n",
            "2025-04-10 14:08:37,597 - chat-bot - INFO - /content/achatbot/src/common/factory.py:34 - get_instance - class: <class 'src.core.llm.transformers.manual_vision_kimi.TransformersManualVisionKimi'> args: {'lm_gen_seed': 42, 'lm_max_length': 2048, 'lm_gen_max_new_tokens': 1024, 'lm_gen_min_new_tokens': 0, 'lm_gen_do_sample': True, 'lm_gen_temperature': 0.8, 'lm_gen_top_k': 50, 'lm_gen_top_p': 0.95, 'lm_gen_min_p': 0.0, 'lm_gen_repetition_penalty': 1.1, 'lm_gen_stops': [], 'lm_gen_stop_ids': [], 'lm_gen_end_id': 0, 'lm_gen_pad_id': 0, 'lm_model_name_or_path': '/content/models/moonshotai/Kimi-VL-A3B-Instruct', 'lm_device_map': None, 'lm_device': 'cuda', 'lm_torch_dtype': 'auto', 'lm_attn_impl': None, 'user_role': 'user', 'warnup_prompt': \"Repeat the word 'weedge niu bi'.\", 'warmup_steps': 1, 'init_chat_role': 'system', 'init_chat_prompt': '', 'lm_tokenizer_decode_batch_size': 60, 'chat_history_size': 10, 'lm_stream': True, 'model_type': 'chat_completion', 'lm_bnb_quant_type': 'int4'}\n",
            "2025-04-10 14:08:37,597 - chat-bot - INFO - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:104 - __init__ - TransformersLMArgs: TransformersLMArgs(lm_gen_seed=42, lm_max_length=2048, lm_gen_max_new_tokens=1024, lm_gen_min_new_tokens=0, lm_gen_do_sample=True, lm_gen_temperature=0.8, lm_gen_top_k=50, lm_gen_top_p=0.95, lm_gen_min_p=0.0, lm_gen_repetition_penalty=1.1, lm_gen_stops=[], lm_gen_stop_ids=[], lm_gen_end_id=0, lm_gen_pad_id=0, lm_model_name_or_path='/content/models/moonshotai/Kimi-VL-A3B-Instruct', lm_device_map=None, lm_device='cuda', lm_torch_dtype='auto', lm_attn_impl=None, user_role='user', warnup_prompt=\"Repeat the word 'weedge niu bi'.\", warmup_steps=1, init_chat_role='system', init_chat_prompt='', lm_tokenizer_decode_batch_size=60, chat_history_size=10, lm_stream=True, model_type='chat_completion', lm_bnb_quant_type='int4')\n",
            "Loading checkpoint shards: 100% 7/7 [00:06<00:00,  1.09it/s]\n",
            "2025-04-10 14:08:55,530 - chat-bot - INFO - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:112 - __init__ - TransformersLMArgs: TransformersLMArgs(lm_gen_seed=42, lm_max_length=2048, lm_gen_max_new_tokens=1024, lm_gen_min_new_tokens=0, lm_gen_do_sample=True, lm_gen_temperature=0.8, lm_gen_top_k=50, lm_gen_top_p=0.95, lm_gen_min_p=0.0, lm_gen_repetition_penalty=1.1, lm_gen_stops=[], lm_gen_stop_ids=[], lm_gen_end_id=0, lm_gen_pad_id=0, lm_model_name_or_path='/content/models/moonshotai/Kimi-VL-A3B-Instruct', lm_device_map=None, lm_device='cuda', lm_torch_dtype='auto', lm_attn_impl=None, user_role='user', warnup_prompt=\"Repeat the word 'weedge niu bi'.\", warmup_steps=1, init_chat_role='system', init_chat_prompt='', lm_tokenizer_decode_batch_size=60, chat_history_size=10, lm_stream=True, model_type='chat_completion', lm_bnb_quant_type='int4') model.device: cuda:0\n",
            "2025-04-10 14:08:55,552 - chat-bot - DEBUG - /content/achatbot/src/common/utils/helper.py:55 - print_model_params - KimiVLForConditionalGeneration(\n",
            "  (vision_tower): MoonVitPretrainedModel(\n",
            "    (patch_embed): MoonVisionPatchEmbed(\n",
            "      (proj): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14))\n",
            "      (pos_emb): Learnable2DInterpPosEmb()\n",
            "    )\n",
            "    (encoder): MoonVitEncoder(\n",
            "      (rope_2d): Rope2DPosEmb(dim=72, max_height=512, max_width=512, theta_base=10000)\n",
            "      (blocks): ModuleList(\n",
            "        (0-26): 27 x MoonVitEncoderLayer(\n",
            "          (norm0): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): MLP2(\n",
            "            (fc0): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "            (fc1): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "            (activation): PytorchGELUTanh()\n",
            "          )\n",
            "          (wqkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "          (wo): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (final_layernorm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (multi_modal_projector): KimiVLMultiModalProjector(\n",
            "    (pre_norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "    (linear_1): Linear(in_features=4608, out_features=4608, bias=True)\n",
            "    (act): GELUActivation()\n",
            "    (linear_2): Linear(in_features=4608, out_features=2048, bias=True)\n",
            "  )\n",
            "  (language_model): DeepseekV3ForCausalLM(\n",
            "    (model): DeepseekV3Model(\n",
            "      (embed_tokens): Embedding(163840, 2048, padding_idx=163839)\n",
            "      (layers): ModuleList(\n",
            "        (0): DeepseekV3DecoderLayer(\n",
            "          (self_attn): DeepseekV3Attention(\n",
            "            (q_proj): Linear(in_features=2048, out_features=3072, bias=False)\n",
            "            (kv_a_proj_with_mqa): Linear(in_features=2048, out_features=576, bias=False)\n",
            "            (kv_a_layernorm): DeepseekV3RMSNorm()\n",
            "            (kv_b_proj): Linear(in_features=512, out_features=4096, bias=False)\n",
            "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "            (rotary_emb): DeepseekV3RotaryEmbedding()\n",
            "          )\n",
            "          (mlp): DeepseekV3MLP(\n",
            "            (gate_proj): Linear(in_features=2048, out_features=11264, bias=False)\n",
            "            (up_proj): Linear(in_features=2048, out_features=11264, bias=False)\n",
            "            (down_proj): Linear(in_features=11264, out_features=2048, bias=False)\n",
            "            (act_fn): SiLU()\n",
            "          )\n",
            "          (input_layernorm): DeepseekV3RMSNorm()\n",
            "          (post_attention_layernorm): DeepseekV3RMSNorm()\n",
            "        )\n",
            "        (1-26): 26 x DeepseekV3DecoderLayer(\n",
            "          (self_attn): DeepseekV3Attention(\n",
            "            (q_proj): Linear(in_features=2048, out_features=3072, bias=False)\n",
            "            (kv_a_proj_with_mqa): Linear(in_features=2048, out_features=576, bias=False)\n",
            "            (kv_a_layernorm): DeepseekV3RMSNorm()\n",
            "            (kv_b_proj): Linear(in_features=512, out_features=4096, bias=False)\n",
            "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "            (rotary_emb): DeepseekV3RotaryEmbedding()\n",
            "          )\n",
            "          (mlp): DeepseekV3MoE(\n",
            "            (experts): ModuleList(\n",
            "              (0-63): 64 x DeepseekV3MLP(\n",
            "                (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
            "                (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
            "                (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
            "                (act_fn): SiLU()\n",
            "              )\n",
            "            )\n",
            "            (gate): MoEGate()\n",
            "            (shared_experts): DeepseekV3MLP(\n",
            "              (gate_proj): Linear(in_features=2048, out_features=2816, bias=False)\n",
            "              (up_proj): Linear(in_features=2048, out_features=2816, bias=False)\n",
            "              (down_proj): Linear(in_features=2816, out_features=2048, bias=False)\n",
            "              (act_fn): SiLU()\n",
            "            )\n",
            "          )\n",
            "          (input_layernorm): DeepseekV3RMSNorm()\n",
            "          (post_attention_layernorm): DeepseekV3RMSNorm()\n",
            "        )\n",
            "      )\n",
            "      (norm): DeepseekV3RMSNorm()\n",
            "    )\n",
            "    (lm_head): Linear(in_features=2048, out_features=163840, bias=False)\n",
            "  )\n",
            ")\n",
            "2025-04-10 14:08:55,575 - chat-bot - INFO - /content/achatbot/src/common/utils/helper.py:56 - print_model_params - llm_transformers_manual_vision_kimi 16407.656048 M parameters\n",
            "2025-04-10 14:08:56,285 - transformers_modules.Kimi-VL-A3B-Instruct.tokenization_moonshot - INFO - /root/.cache/huggingface/modules/transformers_modules/Kimi-VL-A3B-Instruct/tokenization_moonshot.py:118 - __init__ - Reloaded tiktoken model from /content/models/moonshotai/Kimi-VL-A3B-Instruct/tiktoken.model\n",
            "2025-04-10 14:08:56,285 - transformers_modules.Kimi-VL-A3B-Instruct.tokenization_moonshot - INFO - /root/.cache/huggingface/modules/transformers_modules/Kimi-VL-A3B-Instruct/tokenization_moonshot.py:124 - __init__ - #words: 163842 - BOS ID: 163584 - EOS ID: 163585\n",
            "Keyword argument `videos` is not a valid argument for this processor and will be ignored.\n",
            "2025-04-10 14:08:57,287 - chat-bot - INFO - /content/achatbot/src/core/llm/transformers/base.py:139 - _warmup - Warming up TransformersManualVisionKimi device: cuda:0\n",
            "2025-04-10 14:09:01,796 - chat-bot - INFO - /content/achatbot/src/core/llm/transformers/base.py:156 - _warmup - step 0 warnup TTFT time: 0.9859211680000044 s\n",
            "2025-04-10 14:09:01,796 - chat-bot - INFO - /content/achatbot/src/core/llm/transformers/base.py:161 - _warmup - TransformersManualVisionKimi:  warmed up! time: 4.509 s\n",
            "2025-04-10 14:09:01,797 - chat-bot - INFO - /content/achatbot/src/core/llm/__init__.py:71 - initLLMEngine - initLLMEngine: llm_transformers_manual_vision_kimi, TAG:llm_transformers_manual_vision_kimi | TransformersManualVisionKimi\n",
            "\n",
            "--------test prompt:  讲一个故事 --------\n",
            "\n",
            "2025-04-10 14:09:01,797 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:83 - test_chat_completion_prompts - session ctx: {'client_id': 'test_llm_transformers_manual_vision_kimi_client_id', 'sampling_rate': 16000, 'sample_width': 2, 'read_audio_frames_len': 0, 'state': {'prompt': '讲一个故事'}}\n",
            "2025-04-10 14:09:01,797 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:84 - test_chat_completion_prompts - TransformersLMArgs(lm_gen_seed=42, lm_max_length=2048, lm_gen_max_new_tokens=1024, lm_gen_min_new_tokens=0, lm_gen_do_sample=True, lm_gen_temperature=0.8, lm_gen_top_k=50, lm_gen_top_p=0.95, lm_gen_min_p=0.0, lm_gen_repetition_penalty=1.1, lm_gen_stops=[], lm_gen_stop_ids=[], lm_gen_end_id=0, lm_gen_pad_id=0, lm_model_name_or_path='/content/models/moonshotai/Kimi-VL-A3B-Instruct', lm_device_map=None, lm_device='cuda', lm_torch_dtype='auto', lm_attn_impl=None, user_role='user', warnup_prompt=\"Repeat the word 'weedge niu bi'.\", warmup_steps=1, init_chat_role='system', init_chat_prompt='', lm_tokenizer_decode_batch_size=60, chat_history_size=10, lm_stream=True, model_type='chat_completion', lm_bnb_quant_type='int4')\n",
            "2025-04-10 14:09:01,803 - chat-bot - DEBUG - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:188 - generate - chat_history:[{'role': 'user', 'content': '讲一个故事'}]\n",
            "2025-04-10 14:09:01,803 - chat-bot - DEBUG - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:193 - generate - image_inputs:None,video_inputs:None\n",
            "2025-04-10 14:09:31,535 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:95 - test_chat_completion_prompts - chat_completion TTFT time: 0.3884768019997864 s\n",
            "2025-04-10 14:09:31,535 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:96 - test_chat_completion_prompts - generated text: 从前，在一个遥远的小镇上，住着一位年迈的农夫。这个农夫名叫约翰，他拥有一片广阔的土地和一群忠诚的牛群。\n",
            "\n",
            "一天早晨，当阳光洒在大地上时，约翰发现他的土地被一片神秘的蓝色光晕所笼罩。这片光晕似乎蕴含着无穷的力量，它能够治愈疾病、增强体力，甚至可以创造奇迹。\n",
            "\n",
            "村民们开始传言，说这片土地是被神赐予的宝地。他们相信，只要在这片土地上劳作，就能获得无尽的好运和幸福。于是，越来越多的人来到这片土地，希望能得到那片神秘的光晕的祝福。\n",
            "\n",
            "然而，随着时间的推移，这片土地的神奇力量开始显现出其阴暗的一面。那些贪婪的人们开始争夺这块土地的控制权，为了得到更多的光晕力量，他们不惜使用暴力手段。土地上的生灵们也开始遭受伤害，它们的生命力逐渐衰弱，最终失去了往日的活力。\n",
            "\n",
            "约翰看到这一切，心痛不已。他决定采取行动，保护这片土地和它的生命。他与村民们的领袖们一起，制定了一个计划：他们要建立一个公平的交易机制，让每个人都能从中受益，而不是互相争斗。\n",
            "\n",
            "在他们的努力下，一个名为“光明之地”的社区建立起来。在这里，人们用智慧和合作取代了暴力与贪婪。土地不再是一个争夺的对象，而是大家共同守护的家园。而那块曾经神秘的蓝色光晕，也逐渐变得柔和，它不再带来权力，而是带来了和平与希望。\n",
            "\n",
            "约翰的故事在小镇上流传开来，成为了一个永恒的传说。人们学会了珍惜共同拥有的东西，也懂得了团结与合作的力量。而那片曾经被误解为诅咒的土地，也因为爱和信任，绽放出了最灿烂的光芒。\n",
            "\n",
            "--------test prompt:  ('讲一个故事', 'en') --------\n",
            "\n",
            "2025-04-10 14:09:31,535 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:83 - test_chat_completion_prompts - session ctx: {'client_id': 'test_llm_transformers_manual_vision_kimi_client_id', 'sampling_rate': 16000, 'sample_width': 2, 'read_audio_frames_len': 0, 'state': {'prompt': ('讲一个故事', 'en')}}\n",
            "2025-04-10 14:09:31,535 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:84 - test_chat_completion_prompts - TransformersLMArgs(lm_gen_seed=42, lm_max_length=2048, lm_gen_max_new_tokens=1024, lm_gen_min_new_tokens=0, lm_gen_do_sample=True, lm_gen_temperature=0.8, lm_gen_top_k=50, lm_gen_top_p=0.95, lm_gen_min_p=0.0, lm_gen_repetition_penalty=1.1, lm_gen_stops=[], lm_gen_stop_ids=[], lm_gen_end_id=0, lm_gen_pad_id=0, lm_model_name_or_path='/content/models/moonshotai/Kimi-VL-A3B-Instruct', lm_device_map=None, lm_device='cuda', lm_torch_dtype='auto', lm_attn_impl=None, user_role='user', warnup_prompt=\"Repeat the word 'weedge niu bi'.\", warmup_steps=1, init_chat_role='system', init_chat_prompt='', lm_tokenizer_decode_batch_size=60, chat_history_size=10, lm_stream=True, model_type='chat_completion', lm_bnb_quant_type='int4')\n",
            "2025-04-10 14:09:31,536 - chat-bot - DEBUG - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:188 - generate - chat_history:[{'role': 'user', 'content': '讲一个故事'}, {'role': 'assistant', 'content': '从前，在一个遥远的小镇上，住着一位年迈的农夫。这个农夫名叫约翰，他拥有一片广阔的土地和一群忠诚的牛群。\\n\\n一天早晨，当阳光洒在大地上时，约翰发现他的土地被一片神秘的蓝色光晕所笼罩。这片光晕似乎蕴含着无穷的力量，它能够治愈疾病、增强体力，甚至可以创造奇迹。\\n\\n村民们开始传言，说这片土地是被神赐予的宝地。他们相信，只要在这片土地上劳作，就能获得无尽的好运和幸福。于是，越来越多的人来到这片土地，希望能得到那片神秘的光晕的祝福。\\n\\n然而，随着时间的推移，这片土地的神奇力量开始显现出其阴暗的一面。那些贪婪的人们开始争夺这块土地的控制权，为了得到更多的光晕力量，他们不惜使用暴力手段。土地上的生灵们也开始遭受伤害，它们的生命力逐渐衰弱，最终失去了往日的活力。\\n\\n约翰看到这一切，心痛不已。他决定采取行动，保护这片土地和它的生命。他与村民们的领袖们一起，制定了一个计划：他们要建立一个公平的交易机制，让每个人都能从中受益，而不是互相争斗。\\n\\n在他们的努力下，一个名为“光明之地”的社区建立起来。在这里，人们用智慧和合作取代了暴力与贪婪。土地不再是一个争夺的对象，而是大家共同守护的家园。而那块曾经神秘的蓝色光晕，也逐渐变得柔和，它不再带来权力，而是带来了和平与希望。\\n\\n约翰的故事在小镇上流传开来，成为了一个永恒的传说。人们学会了珍惜共同拥有的东西，也懂得了团结与合作的力量。而那片曾经被误解为诅咒的土地，也因为爱和信任，绽放出了最灿烂的光芒。'}, {'role': 'user', 'content': 'Please reply to my message in English. 讲一个故事'}]\n",
            "2025-04-10 14:09:31,536 - chat-bot - DEBUG - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:193 - generate - image_inputs:None,video_inputs:None\n",
            "2025-04-10 14:10:25,856 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:95 - test_chat_completion_prompts - chat_completion TTFT time: 0.5065941679999924 s\n",
            "2025-04-10 14:10:25,856 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:96 - test_chat_completion_prompts - generated text: Absolutely! Here's a story for you:\n",
            "\n",
            "Once upon a time, in a small village nestled at the foot of a towering mountain, there lived a young girl named Lily. She was known throughout the village for her curiosity and adventurous spirit.\n",
            "\n",
            "One day, as she wandered through the forest near her home, she discovered a hidden cave behind a waterfall. Curiosity piqued, Lily ventured inside and found herself in a magical world unlike anything she had ever seen. The air was filled with sparkling lights, and the ground was covered in vibrant flowers that glowed softly in the twilight.\n",
            "\n",
            "In this enchanting place, she met a wise old owl named Oliver who told her about the legendary Crystal Tree. It was said that the tree held the secrets of the universe and could grant any wish to those who were pure of heart. Intrigued, Lily decided to embark on a journey to find the Crystal Tree.\n",
            "\n",
            "Her adventure took her across treacherous rivers, up steep cliffs, and through dense forests. Along the way, she encountered various creatures, each teaching her valuable lessons about courage, kindness, and resilience.\n",
            "\n",
            "Finally, after many days of travel, Lily reached the location where the Crystal Tree was said to grow. As she approached the base of the tree, she noticed that it was guarded by a fierce dragon. But instead of fearing the dragon, Lily remembered the words of wisdom from Oliver: \"True strength comes not from power, but from the heart.\"\n",
            "\n",
            "With a kind smile, Lily approached the dragon and offered it a gift—a handmade necklace crafted from the flowers she had collected along her journey. Impressed by her bravery and compassion, the dragon allowed her to pass and led her to the Crystal Tree.\n",
            "\n",
            "As Lily touched the tree's trunk, she felt a warm surge of energy flow through her body. Suddenly, she heard a gentle voice whispering from within the tree, \"What is your heart's desire?\"\n",
            "\n",
            "Lily closed her eyes and thought deeply about what truly mattered to her. When she opened her eyes, she knew exactly what she wanted: to bring peace and happiness back to her village. With a heartfelt wish, the tree responded, \"Your wish is granted.\"\n",
            "\n",
            "When Lily returned to the village, she shared her incredible journey and the gift from the Crystal Tree with everyone. Soon, the village flourished once more, and its people lived in harmony, cherishing their connection to nature and one another.\n",
            "\n",
            "From then on, the village became a beacon of hope and unity, symbolizing that true magic lies within our hearts and that kindness can overcome even the most formidable challenges.\n",
            "\n",
            "And so, Lily's story continued to inspire generations to come, reminding them that no matter how far we wander, the path back home is always within reach if we carry the light of our own hearts with us.\n",
            "\n",
            "--------test prompt:  ('讲一个故事', 'zh') --------\n",
            "\n",
            "2025-04-10 14:10:25,856 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:83 - test_chat_completion_prompts - session ctx: {'client_id': 'test_llm_transformers_manual_vision_kimi_client_id', 'sampling_rate': 16000, 'sample_width': 2, 'read_audio_frames_len': 0, 'state': {'prompt': ('讲一个故事', 'zh')}}\n",
            "2025-04-10 14:10:25,856 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:84 - test_chat_completion_prompts - TransformersLMArgs(lm_gen_seed=42, lm_max_length=2048, lm_gen_max_new_tokens=1024, lm_gen_min_new_tokens=0, lm_gen_do_sample=True, lm_gen_temperature=0.8, lm_gen_top_k=50, lm_gen_top_p=0.95, lm_gen_min_p=0.0, lm_gen_repetition_penalty=1.1, lm_gen_stops=[], lm_gen_stop_ids=[], lm_gen_end_id=0, lm_gen_pad_id=0, lm_model_name_or_path='/content/models/moonshotai/Kimi-VL-A3B-Instruct', lm_device_map=None, lm_device='cuda', lm_torch_dtype='auto', lm_attn_impl=None, user_role='user', warnup_prompt=\"Repeat the word 'weedge niu bi'.\", warmup_steps=1, init_chat_role='system', init_chat_prompt='', lm_tokenizer_decode_batch_size=60, chat_history_size=10, lm_stream=True, model_type='chat_completion', lm_bnb_quant_type='int4')\n",
            "2025-04-10 14:10:25,857 - chat-bot - DEBUG - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:188 - generate - chat_history:[{'role': 'user', 'content': '讲一个故事'}, {'role': 'assistant', 'content': '从前，在一个遥远的小镇上，住着一位年迈的农夫。这个农夫名叫约翰，他拥有一片广阔的土地和一群忠诚的牛群。\\n\\n一天早晨，当阳光洒在大地上时，约翰发现他的土地被一片神秘的蓝色光晕所笼罩。这片光晕似乎蕴含着无穷的力量，它能够治愈疾病、增强体力，甚至可以创造奇迹。\\n\\n村民们开始传言，说这片土地是被神赐予的宝地。他们相信，只要在这片土地上劳作，就能获得无尽的好运和幸福。于是，越来越多的人来到这片土地，希望能得到那片神秘的光晕的祝福。\\n\\n然而，随着时间的推移，这片土地的神奇力量开始显现出其阴暗的一面。那些贪婪的人们开始争夺这块土地的控制权，为了得到更多的光晕力量，他们不惜使用暴力手段。土地上的生灵们也开始遭受伤害，它们的生命力逐渐衰弱，最终失去了往日的活力。\\n\\n约翰看到这一切，心痛不已。他决定采取行动，保护这片土地和它的生命。他与村民们的领袖们一起，制定了一个计划：他们要建立一个公平的交易机制，让每个人都能从中受益，而不是互相争斗。\\n\\n在他们的努力下，一个名为“光明之地”的社区建立起来。在这里，人们用智慧和合作取代了暴力与贪婪。土地不再是一个争夺的对象，而是大家共同守护的家园。而那块曾经神秘的蓝色光晕，也逐渐变得柔和，它不再带来权力，而是带来了和平与希望。\\n\\n约翰的故事在小镇上流传开来，成为了一个永恒的传说。人们学会了珍惜共同拥有的东西，也懂得了团结与合作的力量。而那片曾经被误解为诅咒的土地，也因为爱和信任，绽放出了最灿烂的光芒。'}, {'role': 'user', 'content': 'Please reply to my message in English. 讲一个故事'}, {'role': 'assistant', 'content': 'Absolutely! Here\\'s a story for you:\\n\\nOnce upon a time, in a small village nestled at the foot of a towering mountain, there lived a young girl named Lily. She was known throughout the village for her curiosity and adventurous spirit.\\n\\nOne day, as she wandered through the forest near her home, she discovered a hidden cave behind a waterfall. Curiosity piqued, Lily ventured inside and found herself in a magical world unlike anything she had ever seen. The air was filled with sparkling lights, and the ground was covered in vibrant flowers that glowed softly in the twilight.\\n\\nIn this enchanting place, she met a wise old owl named Oliver who told her about the legendary Crystal Tree. It was said that the tree held the secrets of the universe and could grant any wish to those who were pure of heart. Intrigued, Lily decided to embark on a journey to find the Crystal Tree.\\n\\nHer adventure took her across treacherous rivers, up steep cliffs, and through dense forests. Along the way, she encountered various creatures, each teaching her valuable lessons about courage, kindness, and resilience.\\n\\nFinally, after many days of travel, Lily reached the location where the Crystal Tree was said to grow. As she approached the base of the tree, she noticed that it was guarded by a fierce dragon. But instead of fearing the dragon, Lily remembered the words of wisdom from Oliver: \"True strength comes not from power, but from the heart.\"\\n\\nWith a kind smile, Lily approached the dragon and offered it a gift—a handmade necklace crafted from the flowers she had collected along her journey. Impressed by her bravery and compassion, the dragon allowed her to pass and led her to the Crystal Tree.\\n\\nAs Lily touched the tree\\'s trunk, she felt a warm surge of energy flow through her body. Suddenly, she heard a gentle voice whispering from within the tree, \"What is your heart\\'s desire?\"\\n\\nLily closed her eyes and thought deeply about what truly mattered to her. When she opened her eyes, she knew exactly what she wanted: to bring peace and happiness back to her village. With a heartfelt wish, the tree responded, \"Your wish is granted.\"\\n\\nWhen Lily returned to the village, she shared her incredible journey and the gift from the Crystal Tree with everyone. Soon, the village flourished once more, and its people lived in harmony, cherishing their connection to nature and one another.\\n\\nFrom then on, the village became a beacon of hope and unity, symbolizing that true magic lies within our hearts and that kindness can overcome even the most formidable challenges.\\n\\nAnd so, Lily\\'s story continued to inspire generations to come, reminding them that no matter how far we wander, the path back home is always within reach if we carry the light of our own hearts with us.'}, {'role': 'user', 'content': 'Please reply to my message in Chinese simplified(简体中文). 讲一个故事'}]\n",
            "2025-04-10 14:10:25,857 - chat-bot - DEBUG - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:193 - generate - image_inputs:None,video_inputs:None\n",
            "2025-04-10 14:10:47,109 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:95 - test_chat_completion_prompts - chat_completion TTFT time: 0.5418631230004394 s\n",
            "2025-04-10 14:10:47,110 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:96 - test_chat_completion_prompts - generated text: 当然可以！以下是一个小故事：\n",
            "\n",
            "从前，在一片宁静的竹林中，住着一只聪明的兔子。这只兔子名叫小明，他总是喜欢探索周围的世界。\n",
            "\n",
            "一天，小明决定离开家园，去寻找传说中的灵芝仙草。据说这种仙草能带来长寿和健康。\n",
            "\n",
            "小明的旅程充满了挑战，他要穿越茫茫森林、翻过高山、渡过河流。每当遇到困难时，他都会停下来思考解决的办法。路上，他还结识了许多新朋友，包括勇敢的熊大哥和勤劳的小松鼠。\n",
            "\n",
            "终于，小明来到了一个神秘的山洞前。洞内弥漫着淡淡的香气，仿佛是灵芝仙草的所在。他鼓起勇气走进山洞，果然看到了那株散发着金色光芒的仙草。\n",
            "\n",
            "小明小心翼翼地采摘了灵芝仙草，并把它带回了自己的家园。当他把仙草煮成药汤时，一股温暖的力量流入他的身体。小明感到前所未有的健康和活力。\n",
            "\n",
            "从此以后，小明的生活变得更加幸福和快乐。他和朋友们一起生活在美丽的竹林里，享受着彼此间的友情和自然的美好。\n",
            "\n",
            "这个故事告诉我们，勇敢和坚持是成功的关键，而友谊和支持则是我们前进的动力。无论走多远，只要我们心怀希望，就一定能找到属于自己的幸福。\n",
            "\n",
            "--------test prompt:  [{'type': 'image', 'image': '/content/achatbot/test/img_files/03-Confusing-Pictures.jpg'}, {'type': 'text', 'text': 'Describe this video. Please reply to my message in chinese'}] --------\n",
            "\n",
            "2025-04-10 14:10:47,110 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:83 - test_chat_completion_prompts - session ctx: {'client_id': 'test_llm_transformers_manual_vision_kimi_client_id', 'sampling_rate': 16000, 'sample_width': 2, 'read_audio_frames_len': 0, 'state': {'prompt': [{'type': 'image', 'image': '/content/achatbot/test/img_files/03-Confusing-Pictures.jpg'}, {'type': 'text', 'text': 'Describe this video. Please reply to my message in chinese'}]}}\n",
            "2025-04-10 14:10:47,110 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:84 - test_chat_completion_prompts - TransformersLMArgs(lm_gen_seed=42, lm_max_length=2048, lm_gen_max_new_tokens=1024, lm_gen_min_new_tokens=0, lm_gen_do_sample=True, lm_gen_temperature=0.8, lm_gen_top_k=50, lm_gen_top_p=0.95, lm_gen_min_p=0.0, lm_gen_repetition_penalty=1.1, lm_gen_stops=[], lm_gen_stop_ids=[], lm_gen_end_id=0, lm_gen_pad_id=0, lm_model_name_or_path='/content/models/moonshotai/Kimi-VL-A3B-Instruct', lm_device_map=None, lm_device='cuda', lm_torch_dtype='auto', lm_attn_impl=None, user_role='user', warnup_prompt=\"Repeat the word 'weedge niu bi'.\", warmup_steps=1, init_chat_role='system', init_chat_prompt='', lm_tokenizer_decode_batch_size=60, chat_history_size=10, lm_stream=True, model_type='chat_completion', lm_bnb_quant_type='int4')\n",
            "2025-04-10 14:10:47,110 - chat-bot - DEBUG - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:188 - generate - chat_history:[{'role': 'user', 'content': '讲一个故事'}, {'role': 'assistant', 'content': '从前，在一个遥远的小镇上，住着一位年迈的农夫。这个农夫名叫约翰，他拥有一片广阔的土地和一群忠诚的牛群。\\n\\n一天早晨，当阳光洒在大地上时，约翰发现他的土地被一片神秘的蓝色光晕所笼罩。这片光晕似乎蕴含着无穷的力量，它能够治愈疾病、增强体力，甚至可以创造奇迹。\\n\\n村民们开始传言，说这片土地是被神赐予的宝地。他们相信，只要在这片土地上劳作，就能获得无尽的好运和幸福。于是，越来越多的人来到这片土地，希望能得到那片神秘的光晕的祝福。\\n\\n然而，随着时间的推移，这片土地的神奇力量开始显现出其阴暗的一面。那些贪婪的人们开始争夺这块土地的控制权，为了得到更多的光晕力量，他们不惜使用暴力手段。土地上的生灵们也开始遭受伤害，它们的生命力逐渐衰弱，最终失去了往日的活力。\\n\\n约翰看到这一切，心痛不已。他决定采取行动，保护这片土地和它的生命。他与村民们的领袖们一起，制定了一个计划：他们要建立一个公平的交易机制，让每个人都能从中受益，而不是互相争斗。\\n\\n在他们的努力下，一个名为“光明之地”的社区建立起来。在这里，人们用智慧和合作取代了暴力与贪婪。土地不再是一个争夺的对象，而是大家共同守护的家园。而那块曾经神秘的蓝色光晕，也逐渐变得柔和，它不再带来权力，而是带来了和平与希望。\\n\\n约翰的故事在小镇上流传开来，成为了一个永恒的传说。人们学会了珍惜共同拥有的东西，也懂得了团结与合作的力量。而那片曾经被误解为诅咒的土地，也因为爱和信任，绽放出了最灿烂的光芒。'}, {'role': 'user', 'content': 'Please reply to my message in English. 讲一个故事'}, {'role': 'assistant', 'content': 'Absolutely! Here\\'s a story for you:\\n\\nOnce upon a time, in a small village nestled at the foot of a towering mountain, there lived a young girl named Lily. She was known throughout the village for her curiosity and adventurous spirit.\\n\\nOne day, as she wandered through the forest near her home, she discovered a hidden cave behind a waterfall. Curiosity piqued, Lily ventured inside and found herself in a magical world unlike anything she had ever seen. The air was filled with sparkling lights, and the ground was covered in vibrant flowers that glowed softly in the twilight.\\n\\nIn this enchanting place, she met a wise old owl named Oliver who told her about the legendary Crystal Tree. It was said that the tree held the secrets of the universe and could grant any wish to those who were pure of heart. Intrigued, Lily decided to embark on a journey to find the Crystal Tree.\\n\\nHer adventure took her across treacherous rivers, up steep cliffs, and through dense forests. Along the way, she encountered various creatures, each teaching her valuable lessons about courage, kindness, and resilience.\\n\\nFinally, after many days of travel, Lily reached the location where the Crystal Tree was said to grow. As she approached the base of the tree, she noticed that it was guarded by a fierce dragon. But instead of fearing the dragon, Lily remembered the words of wisdom from Oliver: \"True strength comes not from power, but from the heart.\"\\n\\nWith a kind smile, Lily approached the dragon and offered it a gift—a handmade necklace crafted from the flowers she had collected along her journey. Impressed by her bravery and compassion, the dragon allowed her to pass and led her to the Crystal Tree.\\n\\nAs Lily touched the tree\\'s trunk, she felt a warm surge of energy flow through her body. Suddenly, she heard a gentle voice whispering from within the tree, \"What is your heart\\'s desire?\"\\n\\nLily closed her eyes and thought deeply about what truly mattered to her. When she opened her eyes, she knew exactly what she wanted: to bring peace and happiness back to her village. With a heartfelt wish, the tree responded, \"Your wish is granted.\"\\n\\nWhen Lily returned to the village, she shared her incredible journey and the gift from the Crystal Tree with everyone. Soon, the village flourished once more, and its people lived in harmony, cherishing their connection to nature and one another.\\n\\nFrom then on, the village became a beacon of hope and unity, symbolizing that true magic lies within our hearts and that kindness can overcome even the most formidable challenges.\\n\\nAnd so, Lily\\'s story continued to inspire generations to come, reminding them that no matter how far we wander, the path back home is always within reach if we carry the light of our own hearts with us.'}, {'role': 'user', 'content': 'Please reply to my message in Chinese simplified(简体中文). 讲一个故事'}, {'role': 'assistant', 'content': '当然可以！以下是一个小故事：\\n\\n从前，在一片宁静的竹林中，住着一只聪明的兔子。这只兔子名叫小明，他总是喜欢探索周围的世界。\\n\\n一天，小明决定离开家园，去寻找传说中的灵芝仙草。据说这种仙草能带来长寿和健康。\\n\\n小明的旅程充满了挑战，他要穿越茫茫森林、翻过高山、渡过河流。每当遇到困难时，他都会停下来思考解决的办法。路上，他还结识了许多新朋友，包括勇敢的熊大哥和勤劳的小松鼠。\\n\\n终于，小明来到了一个神秘的山洞前。洞内弥漫着淡淡的香气，仿佛是灵芝仙草的所在。他鼓起勇气走进山洞，果然看到了那株散发着金色光芒的仙草。\\n\\n小明小心翼翼地采摘了灵芝仙草，并把它带回了自己的家园。当他把仙草煮成药汤时，一股温暖的力量流入他的身体。小明感到前所未有的健康和活力。\\n\\n从此以后，小明的生活变得更加幸福和快乐。他和朋友们一起生活在美丽的竹林里，享受着彼此间的友情和自然的美好。\\n\\n这个故事告诉我们，勇敢和坚持是成功的关键，而友谊和支持则是我们前进的动力。无论走多远，只要我们心怀希望，就一定能找到属于自己的幸福。'}, {'role': 'user', 'content': [{'type': 'image', 'image': '/content/achatbot/test/img_files/03-Confusing-Pictures.jpg'}, {'type': 'text', 'text': 'Describe this video. Please reply to my message in chinese'}]}]\n",
            "2025-04-10 14:10:47,121 - chat-bot - DEBUG - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:193 - generate - image_inputs:[<PIL.Image.Image image mode=RGB size=560x392 at 0x78B1316FAB10>],video_inputs:None\n",
            "2025-04-10 14:10:56,620 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:95 - test_chat_completion_prompts - chat_completion TTFT time: 1.6901124169999093 s\n",
            "2025-04-10 14:10:56,620 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:96 - test_chat_completion_prompts - generated text: 这是一个关于街头工作的男子正在户外熨烫衣物的场景。男子穿着一件黄色的上衣，站在一辆黄色出租车后部，旁边有一个折叠式的熨斗架。他正在将衣物平铺在架子上进行熨烫。背景中有行驶中的其他出租车和城市的街道景象，显示出繁忙的城市环境。\n",
            "\n",
            "从这幅图中可以看出，这位男子可能是在利用停车的时间来打理自己的衣物。这反映了街头工作者在不同环境中寻找便利的方式，展现了他们在日常生活中的一种应对策略。\n",
            "\n",
            "--------test prompt:  [{'type': 'video', 'video': ['https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg', 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg', 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg', 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'], 'fps': 1.0}, {'type': 'text', 'text': 'Describe this video. Please reply to my message in chinese'}] --------\n",
            "\n",
            "2025-04-10 14:10:56,620 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:83 - test_chat_completion_prompts - session ctx: {'client_id': 'test_llm_transformers_manual_vision_kimi_client_id', 'sampling_rate': 16000, 'sample_width': 2, 'read_audio_frames_len': 0, 'state': {'prompt': [{'type': 'video', 'video': ['https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg', 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg', 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg', 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'], 'fps': 1.0}, {'type': 'text', 'text': 'Describe this video. Please reply to my message in chinese'}]}}\n",
            "2025-04-10 14:10:56,620 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:84 - test_chat_completion_prompts - TransformersLMArgs(lm_gen_seed=42, lm_max_length=2048, lm_gen_max_new_tokens=1024, lm_gen_min_new_tokens=0, lm_gen_do_sample=True, lm_gen_temperature=0.8, lm_gen_top_k=50, lm_gen_top_p=0.95, lm_gen_min_p=0.0, lm_gen_repetition_penalty=1.1, lm_gen_stops=[], lm_gen_stop_ids=[], lm_gen_end_id=0, lm_gen_pad_id=0, lm_model_name_or_path='/content/models/moonshotai/Kimi-VL-A3B-Instruct', lm_device_map=None, lm_device='cuda', lm_torch_dtype='auto', lm_attn_impl=None, user_role='user', warnup_prompt=\"Repeat the word 'weedge niu bi'.\", warmup_steps=1, init_chat_role='system', init_chat_prompt='', lm_tokenizer_decode_batch_size=60, chat_history_size=10, lm_stream=True, model_type='chat_completion', lm_bnb_quant_type='int4')\n",
            "2025-04-10 14:10:56,621 - chat-bot - DEBUG - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:188 - generate - chat_history:[{'role': 'user', 'content': '讲一个故事'}, {'role': 'assistant', 'content': '从前，在一个遥远的小镇上，住着一位年迈的农夫。这个农夫名叫约翰，他拥有一片广阔的土地和一群忠诚的牛群。\\n\\n一天早晨，当阳光洒在大地上时，约翰发现他的土地被一片神秘的蓝色光晕所笼罩。这片光晕似乎蕴含着无穷的力量，它能够治愈疾病、增强体力，甚至可以创造奇迹。\\n\\n村民们开始传言，说这片土地是被神赐予的宝地。他们相信，只要在这片土地上劳作，就能获得无尽的好运和幸福。于是，越来越多的人来到这片土地，希望能得到那片神秘的光晕的祝福。\\n\\n然而，随着时间的推移，这片土地的神奇力量开始显现出其阴暗的一面。那些贪婪的人们开始争夺这块土地的控制权，为了得到更多的光晕力量，他们不惜使用暴力手段。土地上的生灵们也开始遭受伤害，它们的生命力逐渐衰弱，最终失去了往日的活力。\\n\\n约翰看到这一切，心痛不已。他决定采取行动，保护这片土地和它的生命。他与村民们的领袖们一起，制定了一个计划：他们要建立一个公平的交易机制，让每个人都能从中受益，而不是互相争斗。\\n\\n在他们的努力下，一个名为“光明之地”的社区建立起来。在这里，人们用智慧和合作取代了暴力与贪婪。土地不再是一个争夺的对象，而是大家共同守护的家园。而那块曾经神秘的蓝色光晕，也逐渐变得柔和，它不再带来权力，而是带来了和平与希望。\\n\\n约翰的故事在小镇上流传开来，成为了一个永恒的传说。人们学会了珍惜共同拥有的东西，也懂得了团结与合作的力量。而那片曾经被误解为诅咒的土地，也因为爱和信任，绽放出了最灿烂的光芒。'}, {'role': 'user', 'content': 'Please reply to my message in English. 讲一个故事'}, {'role': 'assistant', 'content': 'Absolutely! Here\\'s a story for you:\\n\\nOnce upon a time, in a small village nestled at the foot of a towering mountain, there lived a young girl named Lily. She was known throughout the village for her curiosity and adventurous spirit.\\n\\nOne day, as she wandered through the forest near her home, she discovered a hidden cave behind a waterfall. Curiosity piqued, Lily ventured inside and found herself in a magical world unlike anything she had ever seen. The air was filled with sparkling lights, and the ground was covered in vibrant flowers that glowed softly in the twilight.\\n\\nIn this enchanting place, she met a wise old owl named Oliver who told her about the legendary Crystal Tree. It was said that the tree held the secrets of the universe and could grant any wish to those who were pure of heart. Intrigued, Lily decided to embark on a journey to find the Crystal Tree.\\n\\nHer adventure took her across treacherous rivers, up steep cliffs, and through dense forests. Along the way, she encountered various creatures, each teaching her valuable lessons about courage, kindness, and resilience.\\n\\nFinally, after many days of travel, Lily reached the location where the Crystal Tree was said to grow. As she approached the base of the tree, she noticed that it was guarded by a fierce dragon. But instead of fearing the dragon, Lily remembered the words of wisdom from Oliver: \"True strength comes not from power, but from the heart.\"\\n\\nWith a kind smile, Lily approached the dragon and offered it a gift—a handmade necklace crafted from the flowers she had collected along her journey. Impressed by her bravery and compassion, the dragon allowed her to pass and led her to the Crystal Tree.\\n\\nAs Lily touched the tree\\'s trunk, she felt a warm surge of energy flow through her body. Suddenly, she heard a gentle voice whispering from within the tree, \"What is your heart\\'s desire?\"\\n\\nLily closed her eyes and thought deeply about what truly mattered to her. When she opened her eyes, she knew exactly what she wanted: to bring peace and happiness back to her village. With a heartfelt wish, the tree responded, \"Your wish is granted.\"\\n\\nWhen Lily returned to the village, she shared her incredible journey and the gift from the Crystal Tree with everyone. Soon, the village flourished once more, and its people lived in harmony, cherishing their connection to nature and one another.\\n\\nFrom then on, the village became a beacon of hope and unity, symbolizing that true magic lies within our hearts and that kindness can overcome even the most formidable challenges.\\n\\nAnd so, Lily\\'s story continued to inspire generations to come, reminding them that no matter how far we wander, the path back home is always within reach if we carry the light of our own hearts with us.'}, {'role': 'user', 'content': 'Please reply to my message in Chinese simplified(简体中文). 讲一个故事'}, {'role': 'assistant', 'content': '当然可以！以下是一个小故事：\\n\\n从前，在一片宁静的竹林中，住着一只聪明的兔子。这只兔子名叫小明，他总是喜欢探索周围的世界。\\n\\n一天，小明决定离开家园，去寻找传说中的灵芝仙草。据说这种仙草能带来长寿和健康。\\n\\n小明的旅程充满了挑战，他要穿越茫茫森林、翻过高山、渡过河流。每当遇到困难时，他都会停下来思考解决的办法。路上，他还结识了许多新朋友，包括勇敢的熊大哥和勤劳的小松鼠。\\n\\n终于，小明来到了一个神秘的山洞前。洞内弥漫着淡淡的香气，仿佛是灵芝仙草的所在。他鼓起勇气走进山洞，果然看到了那株散发着金色光芒的仙草。\\n\\n小明小心翼翼地采摘了灵芝仙草，并把它带回了自己的家园。当他把仙草煮成药汤时，一股温暖的力量流入他的身体。小明感到前所未有的健康和活力。\\n\\n从此以后，小明的生活变得更加幸福和快乐。他和朋友们一起生活在美丽的竹林里，享受着彼此间的友情和自然的美好。\\n\\n这个故事告诉我们，勇敢和坚持是成功的关键，而友谊和支持则是我们前进的动力。无论走多远，只要我们心怀希望，就一定能找到属于自己的幸福。'}, {'role': 'user', 'content': [{'type': 'image', 'image': '/content/achatbot/test/img_files/03-Confusing-Pictures.jpg'}, {'type': 'text', 'text': 'Describe this video. Please reply to my message in chinese'}]}, {'role': 'assistant', 'content': '这是一个关于街头工作的男子正在户外熨烫衣物的场景。男子穿着一件黄色的上衣，站在一辆黄色出租车后部，旁边有一个折叠式的熨斗架。他正在将衣物平铺在架子上进行熨烫。背景中有行驶中的其他出租车和城市的街道景象，显示出繁忙的城市环境。\\n\\n从这幅图中可以看出，这位男子可能是在利用停车的时间来打理自己的衣物。这反映了街头工作者在不同环境中寻找便利的方式，展现了他们在日常生活中的一种应对策略。'}, {'role': 'user', 'content': [{'type': 'video', 'video': ['https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg', 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg', 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg', 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'], 'fps': 1.0}, {'type': 'text', 'text': 'Describe this video. Please reply to my message in chinese'}]}]\n",
            "2025-04-10 14:10:56,630 - urllib3.connectionpool - DEBUG - /usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1049 - _new_conn - Starting new HTTPS connection (1): qianwen-res.oss-cn-beijing.aliyuncs.com:443\n",
            "2025-04-10 14:10:57,038 - urllib3.connectionpool - DEBUG - /usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:544 - _make_request - https://qianwen-res.oss-cn-beijing.aliyuncs.com:443 \"GET /Qwen-VL/assets/demo.jpeg HTTP/1.1\" 200 496395\n",
            "2025-04-10 14:10:57,598 - urllib3.connectionpool - DEBUG - /usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1049 - _new_conn - Starting new HTTPS connection (1): qianwen-res.oss-cn-beijing.aliyuncs.com:443\n",
            "2025-04-10 14:10:58,235 - urllib3.connectionpool - DEBUG - /usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:544 - _make_request - https://qianwen-res.oss-cn-beijing.aliyuncs.com:443 \"GET /Qwen-VL/assets/demo.jpeg HTTP/1.1\" 200 496395\n",
            "2025-04-10 14:10:58,729 - urllib3.connectionpool - DEBUG - /usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1049 - _new_conn - Starting new HTTPS connection (1): qianwen-res.oss-cn-beijing.aliyuncs.com:443\n",
            "2025-04-10 14:10:59,461 - urllib3.connectionpool - DEBUG - /usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:544 - _make_request - https://qianwen-res.oss-cn-beijing.aliyuncs.com:443 \"GET /Qwen-VL/assets/demo.jpeg HTTP/1.1\" 200 496395\n",
            "2025-04-10 14:11:00,044 - urllib3.connectionpool - DEBUG - /usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1049 - _new_conn - Starting new HTTPS connection (1): qianwen-res.oss-cn-beijing.aliyuncs.com:443\n",
            "2025-04-10 14:11:00,691 - urllib3.connectionpool - DEBUG - /usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:544 - _make_request - https://qianwen-res.oss-cn-beijing.aliyuncs.com:443 \"GET /Qwen-VL/assets/demo.jpeg HTTP/1.1\" 200 496395\n",
            "2025-04-10 14:11:01,202 - chat-bot - DEBUG - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:193 - generate - image_inputs:[<PIL.Image.Image image mode=RGB size=560x392 at 0x78B0E6CBAFD0>],video_inputs:[[<PIL.Image.Image image mode=RGB size=2044x1372 at 0x78B1315C8ED0>, <PIL.Image.Image image mode=RGB size=2044x1372 at 0x78B1316E7810>, <PIL.Image.Image image mode=RGB size=2044x1372 at 0x78B1315C8AD0>, <PIL.Image.Image image mode=RGB size=2044x1372 at 0x78B0E438D490>]]\n",
            "2025-04-10 14:11:10,392 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:95 - test_chat_completion_prompts - chat_completion TTFT time: 5.711143465000077 s\n",
            "2025-04-10 14:11:10,392 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:96 - test_chat_completion_prompts - generated text: 这是一张展示日常生活场景的照片，照片中有一位男士正在户外熨衣服。男士穿着一身黄色的运动服，身体微微前倾，专注地操作着熨斗。他身后是一辆停靠在路边的一辆黄色出租车，车身上有广告贴纸。街道上还有其他行驶的车辆，显示出这是一个繁忙的城市街景。背景中可以看到建筑物和路牌，增添了都市的氛围。整个画面传达了一种忙碌而真实的生活状态，表现了街头工作者的灵活性和应变能力。\n",
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 158.535s\n",
            "\n",
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!LLM_DEVICE=cuda PROMPT=讲一个故事 LLM_MODEL_NAME_OR_PATH=/content/models/moonshotai/Kimi-VL-A3B-Thinking \\\n",
        "  python -m unittest test.core.llm.test_transformers_v_kimi.TestTransformersVKimi.test_chat_completion_prompts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhIlTeH3DRqH",
        "outputId": "d818d826-a73e-4c99-b940-01eab6aaa72c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-10 14:14:02,697 - numexpr.utils - INFO - /usr/local/lib/python3.11/dist-packages/numexpr/utils.py:162 - _init_num_threads - NumExpr defaulting to 12 threads.\n",
            "2025-04-10 14:14:04.307578: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-10 14:14:04.327401: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744294444.350347   19425 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744294444.357410   19425 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-10 14:14:04.380689: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-04-10 14:14:05,550 - tensorflow - DEBUG - /usr/local/lib/python3.11/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py:34 - <module> - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "2025-04-10 14:14:05,715 - h5py._conv - DEBUG - /usr/local/lib/python3.11/dist-packages/h5py/__init__.py:47 - <module> - Creating converter from 7 to 5\n",
            "2025-04-10 14:14:05,716 - h5py._conv - DEBUG - /usr/local/lib/python3.11/dist-packages/h5py/__init__.py:47 - <module> - Creating converter from 5 to 7\n",
            "2025-04-10 14:14:05,716 - h5py._conv - DEBUG - /usr/local/lib/python3.11/dist-packages/h5py/__init__.py:47 - <module> - Creating converter from 7 to 5\n",
            "2025-04-10 14:14:05,716 - h5py._conv - DEBUG - /usr/local/lib/python3.11/dist-packages/h5py/__init__.py:47 - <module> - Creating converter from 5 to 7\n",
            "DEBUG:2025-04-10 14:14:06,172:jax._src.path:31: etils.epath found. Using etils.epath for file I/O.\n",
            "2025-04-10 14:14:06,172 - jax._src.path - DEBUG - /usr/local/lib/python3.11/dist-packages/jax/_src/path.py:31 - <module> - etils.epath found. Using etils.epath for file I/O.\n",
            "2025-04-10 14:14:06,722 - matplotlib - DEBUG - /usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py:342 - wrapper - matplotlib data path: /usr/local/lib/python3.11/dist-packages/matplotlib/mpl-data\n",
            "2025-04-10 14:14:06,728 - matplotlib - DEBUG - /usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py:342 - wrapper - CONFIGDIR=/root/.config/matplotlib\n",
            "2025-04-10 14:14:06,754 - matplotlib - DEBUG - /usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py:1557 - <module> - interactive is False\n",
            "2025-04-10 14:14:06,754 - matplotlib - DEBUG - /usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py:1558 - <module> - platform is linux\n",
            "2025-04-10 14:14:06,810 - matplotlib - DEBUG - /usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py:342 - wrapper - CACHEDIR=/root/.cache/matplotlib\n",
            "2025-04-10 14:14:06,811 - matplotlib.font_manager - DEBUG - /usr/local/lib/python3.11/dist-packages/matplotlib/font_manager.py:1635 - _load_fontmanager - Using fontManager instance from /root/.cache/matplotlib/fontlist-v390.json\n",
            "2025-04-10 14:14:07,469 - qwen_vl_utils.vision_process - INFO - /usr/local/lib/python3.11/dist-packages/qwen_vl_utils/vision_process.py:41 - <module> - set VIDEO_TOTAL_PIXELS: 90316800\n",
            "2025-04-10 14:14:07,469 - chat-bot - INFO - /content/achatbot/src/common/factory.py:69 - get_engine_by_tag - use llm_transformers_manual_vision_kimi engine\n",
            "2025-04-10 14:14:07,469 - chat-bot - INFO - /content/achatbot/src/common/factory.py:34 - get_instance - class: <class 'src.core.llm.transformers.manual_vision_kimi.TransformersManualVisionKimi'> args: {'lm_gen_seed': 42, 'lm_max_length': 2048, 'lm_gen_max_new_tokens': 1024, 'lm_gen_min_new_tokens': 0, 'lm_gen_do_sample': True, 'lm_gen_temperature': 0.8, 'lm_gen_top_k': 50, 'lm_gen_top_p': 0.95, 'lm_gen_min_p': 0.0, 'lm_gen_repetition_penalty': 1.1, 'lm_gen_stops': [], 'lm_gen_stop_ids': [], 'lm_gen_end_id': 0, 'lm_gen_pad_id': 0, 'lm_model_name_or_path': '/content/models/moonshotai/Kimi-VL-A3B-Thinking', 'lm_device_map': None, 'lm_device': 'cuda', 'lm_torch_dtype': 'auto', 'lm_attn_impl': None, 'user_role': 'user', 'warnup_prompt': \"Repeat the word 'weedge niu bi'.\", 'warmup_steps': 1, 'init_chat_role': 'system', 'init_chat_prompt': '', 'lm_tokenizer_decode_batch_size': 60, 'chat_history_size': 10, 'lm_stream': True, 'model_type': 'chat_completion', 'lm_bnb_quant_type': 'int4'}\n",
            "2025-04-10 14:14:07,469 - chat-bot - INFO - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:104 - __init__ - TransformersLMArgs: TransformersLMArgs(lm_gen_seed=42, lm_max_length=2048, lm_gen_max_new_tokens=1024, lm_gen_min_new_tokens=0, lm_gen_do_sample=True, lm_gen_temperature=0.8, lm_gen_top_k=50, lm_gen_top_p=0.95, lm_gen_min_p=0.0, lm_gen_repetition_penalty=1.1, lm_gen_stops=[], lm_gen_stop_ids=[], lm_gen_end_id=0, lm_gen_pad_id=0, lm_model_name_or_path='/content/models/moonshotai/Kimi-VL-A3B-Thinking', lm_device_map=None, lm_device='cuda', lm_torch_dtype='auto', lm_attn_impl=None, user_role='user', warnup_prompt=\"Repeat the word 'weedge niu bi'.\", warmup_steps=1, init_chat_role='system', init_chat_prompt='', lm_tokenizer_decode_batch_size=60, chat_history_size=10, lm_stream=True, model_type='chat_completion', lm_bnb_quant_type='int4')\n",
            "Loading checkpoint shards: 100% 7/7 [03:22<00:00, 28.89s/it]\n",
            "2025-04-10 14:18:06,057 - chat-bot - INFO - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:112 - __init__ - TransformersLMArgs: TransformersLMArgs(lm_gen_seed=42, lm_max_length=2048, lm_gen_max_new_tokens=1024, lm_gen_min_new_tokens=0, lm_gen_do_sample=True, lm_gen_temperature=0.8, lm_gen_top_k=50, lm_gen_top_p=0.95, lm_gen_min_p=0.0, lm_gen_repetition_penalty=1.1, lm_gen_stops=[], lm_gen_stop_ids=[], lm_gen_end_id=0, lm_gen_pad_id=0, lm_model_name_or_path='/content/models/moonshotai/Kimi-VL-A3B-Thinking', lm_device_map=None, lm_device='cuda', lm_torch_dtype='auto', lm_attn_impl=None, user_role='user', warnup_prompt=\"Repeat the word 'weedge niu bi'.\", warmup_steps=1, init_chat_role='system', init_chat_prompt='', lm_tokenizer_decode_batch_size=60, chat_history_size=10, lm_stream=True, model_type='chat_completion', lm_bnb_quant_type='int4') model.device: cuda:0\n",
            "2025-04-10 14:18:06,079 - chat-bot - DEBUG - /content/achatbot/src/common/utils/helper.py:55 - print_model_params - KimiVLForConditionalGeneration(\n",
            "  (vision_tower): MoonVitPretrainedModel(\n",
            "    (patch_embed): MoonVisionPatchEmbed(\n",
            "      (proj): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14))\n",
            "      (pos_emb): Learnable2DInterpPosEmb()\n",
            "    )\n",
            "    (encoder): MoonVitEncoder(\n",
            "      (rope_2d): Rope2DPosEmb(dim=72, max_height=512, max_width=512, theta_base=10000)\n",
            "      (blocks): ModuleList(\n",
            "        (0-26): 27 x MoonVitEncoderLayer(\n",
            "          (norm0): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): MLP2(\n",
            "            (fc0): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "            (fc1): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "            (activation): PytorchGELUTanh()\n",
            "          )\n",
            "          (wqkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "          (wo): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (final_layernorm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (multi_modal_projector): KimiVLMultiModalProjector(\n",
            "    (pre_norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "    (linear_1): Linear(in_features=4608, out_features=4608, bias=True)\n",
            "    (act): GELUActivation()\n",
            "    (linear_2): Linear(in_features=4608, out_features=2048, bias=True)\n",
            "  )\n",
            "  (language_model): DeepseekV3ForCausalLM(\n",
            "    (model): DeepseekV3Model(\n",
            "      (embed_tokens): Embedding(163840, 2048, padding_idx=163839)\n",
            "      (layers): ModuleList(\n",
            "        (0): DeepseekV3DecoderLayer(\n",
            "          (self_attn): DeepseekV3Attention(\n",
            "            (q_proj): Linear(in_features=2048, out_features=3072, bias=False)\n",
            "            (kv_a_proj_with_mqa): Linear(in_features=2048, out_features=576, bias=False)\n",
            "            (kv_a_layernorm): DeepseekV3RMSNorm()\n",
            "            (kv_b_proj): Linear(in_features=512, out_features=4096, bias=False)\n",
            "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "            (rotary_emb): DeepseekV3RotaryEmbedding()\n",
            "          )\n",
            "          (mlp): DeepseekV3MLP(\n",
            "            (gate_proj): Linear(in_features=2048, out_features=11264, bias=False)\n",
            "            (up_proj): Linear(in_features=2048, out_features=11264, bias=False)\n",
            "            (down_proj): Linear(in_features=11264, out_features=2048, bias=False)\n",
            "            (act_fn): SiLU()\n",
            "          )\n",
            "          (input_layernorm): DeepseekV3RMSNorm()\n",
            "          (post_attention_layernorm): DeepseekV3RMSNorm()\n",
            "        )\n",
            "        (1-26): 26 x DeepseekV3DecoderLayer(\n",
            "          (self_attn): DeepseekV3Attention(\n",
            "            (q_proj): Linear(in_features=2048, out_features=3072, bias=False)\n",
            "            (kv_a_proj_with_mqa): Linear(in_features=2048, out_features=576, bias=False)\n",
            "            (kv_a_layernorm): DeepseekV3RMSNorm()\n",
            "            (kv_b_proj): Linear(in_features=512, out_features=4096, bias=False)\n",
            "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "            (rotary_emb): DeepseekV3RotaryEmbedding()\n",
            "          )\n",
            "          (mlp): DeepseekV3MoE(\n",
            "            (experts): ModuleList(\n",
            "              (0-63): 64 x DeepseekV3MLP(\n",
            "                (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
            "                (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
            "                (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
            "                (act_fn): SiLU()\n",
            "              )\n",
            "            )\n",
            "            (gate): MoEGate()\n",
            "            (shared_experts): DeepseekV3MLP(\n",
            "              (gate_proj): Linear(in_features=2048, out_features=2816, bias=False)\n",
            "              (up_proj): Linear(in_features=2048, out_features=2816, bias=False)\n",
            "              (down_proj): Linear(in_features=2816, out_features=2048, bias=False)\n",
            "              (act_fn): SiLU()\n",
            "            )\n",
            "          )\n",
            "          (input_layernorm): DeepseekV3RMSNorm()\n",
            "          (post_attention_layernorm): DeepseekV3RMSNorm()\n",
            "        )\n",
            "      )\n",
            "      (norm): DeepseekV3RMSNorm()\n",
            "    )\n",
            "    (lm_head): Linear(in_features=2048, out_features=163840, bias=False)\n",
            "  )\n",
            ")\n",
            "2025-04-10 14:18:06,101 - chat-bot - INFO - /content/achatbot/src/common/utils/helper.py:56 - print_model_params - llm_transformers_manual_vision_kimi 16407.656048 M parameters\n",
            "2025-04-10 14:18:06,935 - transformers_modules.Kimi-VL-A3B-Thinking.tokenization_moonshot - INFO - /root/.cache/huggingface/modules/transformers_modules/Kimi-VL-A3B-Thinking/tokenization_moonshot.py:118 - __init__ - Reloaded tiktoken model from /content/models/moonshotai/Kimi-VL-A3B-Thinking/tiktoken.model\n",
            "2025-04-10 14:18:06,935 - transformers_modules.Kimi-VL-A3B-Thinking.tokenization_moonshot - INFO - /root/.cache/huggingface/modules/transformers_modules/Kimi-VL-A3B-Thinking/tokenization_moonshot.py:124 - __init__ - #words: 163842 - BOS ID: 163584 - EOS ID: 163585\n",
            "Keyword argument `videos` is not a valid argument for this processor and will be ignored.\n",
            "2025-04-10 14:18:07,953 - chat-bot - INFO - /content/achatbot/src/core/llm/transformers/base.py:139 - _warmup - Warming up TransformersManualVisionKimi device: cuda:0\n",
            "2025-04-10 14:18:30,175 - chat-bot - INFO - /content/achatbot/src/core/llm/transformers/base.py:156 - _warmup - step 0 warnup TTFT time: 1.2511372660001143 s\n",
            "2025-04-10 14:18:30,175 - chat-bot - INFO - /content/achatbot/src/core/llm/transformers/base.py:161 - _warmup - TransformersManualVisionKimi:  warmed up! time: 22.222 s\n",
            "2025-04-10 14:18:30,176 - chat-bot - INFO - /content/achatbot/src/core/llm/__init__.py:71 - initLLMEngine - initLLMEngine: llm_transformers_manual_vision_kimi, TAG:llm_transformers_manual_vision_kimi | TransformersManualVisionKimi\n",
            "\n",
            "--------test prompt:  讲一个故事 --------\n",
            "\n",
            "2025-04-10 14:18:30,176 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:83 - test_chat_completion_prompts - session ctx: {'client_id': 'test_llm_transformers_manual_vision_kimi_client_id', 'sampling_rate': 16000, 'sample_width': 2, 'read_audio_frames_len': 0, 'state': {'prompt': '讲一个故事'}}\n",
            "2025-04-10 14:18:30,176 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:84 - test_chat_completion_prompts - TransformersLMArgs(lm_gen_seed=42, lm_max_length=2048, lm_gen_max_new_tokens=1024, lm_gen_min_new_tokens=0, lm_gen_do_sample=True, lm_gen_temperature=0.8, lm_gen_top_k=50, lm_gen_top_p=0.95, lm_gen_min_p=0.0, lm_gen_repetition_penalty=1.1, lm_gen_stops=[], lm_gen_stop_ids=[], lm_gen_end_id=0, lm_gen_pad_id=0, lm_model_name_or_path='/content/models/moonshotai/Kimi-VL-A3B-Thinking', lm_device_map=None, lm_device='cuda', lm_torch_dtype='auto', lm_attn_impl=None, user_role='user', warnup_prompt=\"Repeat the word 'weedge niu bi'.\", warmup_steps=1, init_chat_role='system', init_chat_prompt='', lm_tokenizer_decode_batch_size=60, chat_history_size=10, lm_stream=True, model_type='chat_completion', lm_bnb_quant_type='int4')\n",
            "2025-04-10 14:18:30,187 - chat-bot - DEBUG - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:188 - generate - chat_history:[{'role': 'user', 'content': '讲一个故事'}]\n",
            "2025-04-10 14:18:30,188 - chat-bot - DEBUG - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:193 - generate - image_inputs:None,video_inputs:None\n",
            "2025-04-10 14:19:26,550 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:95 - test_chat_completion_prompts - chat_completion TTFT time: 1.1577445500006434 s\n",
            "2025-04-10 14:19:26,550 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:96 - test_chat_completion_prompts - generated text: ◁think▷用户让我讲一个故事，这看起来是一个简单的请求，但背后可能有多种意图。我需要先分析一下用户的需求和潜在意图。\n",
            "\n",
            "首先，用户可能只是想听一个有趣的故事，或者需要一些灵感来创作自己的故事。也有可能他们想通过这个故事获得某种情感上的满足，比如治愈、启发或者娱乐。还有一种可能是，他们在测试我的讲故事能力，想看看我能写出多精彩的内容。\n",
            "\n",
            "接下来，我需要考虑如何回应这个请求。既然用户没有指定故事的主题或风格，我需要选择一个既有趣又有一定深度的方向。一个不错的选择是讲一个简单的奇幻故事，因为这种类型比较容易吸引不同年龄段的读者，同时也能传递一些积极的信息。\n",
            "\n",
            "然后，我要开始构思故事的框架。故事需要一个主角和一个目标，最好带有一点冒险的元素，这样更有吸引力。考虑到用户可能希望故事有一定的深度，我可以在故事中融入一些关于人性、成长或者勇气的主题。\n",
            "\n",
            "在构思过程中，我还需要考虑如何让故事有起伏和张力。通过设置障碍和挑战，可以让故事更有吸引力。同时，我需要确保故事有一个积极的结局，给用户带来正面的感受。\n",
            "\n",
            "最后，我需要确定故事的语气和风格。因为用户没有特别说明，我会选择一种温暖而鼓舞人心的语气，让故事既有想象力又容易让人产生共鸣。\n",
            "\n",
            "现在，我已经有了一个大致的思路，可以开始正式回复用户了。◁/think▷好的，我将为你创作一个简短的故事：\n",
            "\n",
            "在一个古老的村庄里，住着一个名叫小明的男孩。小明天生听力不好，但他有着无比的好奇心和对世界的热爱。每当村民们在田埂上聊天，他总是踮起脚尖，试图捕捉那些模糊的声音；当他坐在门槛上，他认真地听着昆虫的鸣叫，鸟儿的啁啾，以及树叶沙沙作响的节奏。然而，由于他的听力问题，他与外界的连接总是显得遥远而不够真实。\n",
            "\n",
            "有一天，小明决定去寻找传说中的“寂静森林”。他背着父亲的旧水壶，穿过层层叠叠的山脉和茂密的树林，带着对未知的好奇心和一点点恐惧。在森林中，他发现了一棵巨大的橡树，它的树皮像波浪一样起伏，仿佛在呼吸一般。小明蹲下来，闭上眼睛，用心灵去感受树的脉搏——那是一种缓慢而坚定的节奏，充满了生命力。\n",
            "\n",
            "当小明再次睁开眼睛时，他惊讶地发现，除了风声之外，他还听到了更多的声音：蜜蜂的嗡嗡声、蝴蝶翅膀的扑翅声、甚至远处小溪潺潺的水流声。这些声音不仅仅是“听觉”的感知，更是一种更深层次的理解，仿佛他能感受到它们的振动频率，甚至能触摸到它们的存在。\n",
            "\n",
            "在寂静森林里，小明不仅找到了声音，也找到了自己内心深处未被察觉的智慧与勇气。他学会了倾听内心的声音，也学会了与世界对话。从此，他决定将自己的经历分享给小村庄的居民们，让他们明白真正的理解并不是依靠耳朵，而是用心去听。\n",
            "\n",
            "---\n",
            "\n",
            "这个故事虽然简单，但包含了探索、自我发现、沟通等主题，希望能带给你一些启发和乐趣！如果你有特定的主题或元素想要加入故事，请告诉我，我可以帮你进一步完善它。\n",
            "\n",
            "--------test prompt:  ('讲一个故事', 'en') --------\n",
            "\n",
            "2025-04-10 14:19:26,550 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:83 - test_chat_completion_prompts - session ctx: {'client_id': 'test_llm_transformers_manual_vision_kimi_client_id', 'sampling_rate': 16000, 'sample_width': 2, 'read_audio_frames_len': 0, 'state': {'prompt': ('讲一个故事', 'en')}}\n",
            "2025-04-10 14:19:26,550 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:84 - test_chat_completion_prompts - TransformersLMArgs(lm_gen_seed=42, lm_max_length=2048, lm_gen_max_new_tokens=1024, lm_gen_min_new_tokens=0, lm_gen_do_sample=True, lm_gen_temperature=0.8, lm_gen_top_k=50, lm_gen_top_p=0.95, lm_gen_min_p=0.0, lm_gen_repetition_penalty=1.1, lm_gen_stops=[], lm_gen_stop_ids=[], lm_gen_end_id=0, lm_gen_pad_id=0, lm_model_name_or_path='/content/models/moonshotai/Kimi-VL-A3B-Thinking', lm_device_map=None, lm_device='cuda', lm_torch_dtype='auto', lm_attn_impl=None, user_role='user', warnup_prompt=\"Repeat the word 'weedge niu bi'.\", warmup_steps=1, init_chat_role='system', init_chat_prompt='', lm_tokenizer_decode_batch_size=60, chat_history_size=10, lm_stream=True, model_type='chat_completion', lm_bnb_quant_type='int4')\n",
            "2025-04-10 14:19:26,551 - chat-bot - DEBUG - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:188 - generate - chat_history:[{'role': 'user', 'content': '讲一个故事'}, {'role': 'assistant', 'content': '◁think▷用户让我讲一个故事，这看起来是一个简单的请求，但背后可能有多种意图。我需要先分析一下用户的需求和潜在意图。\\n\\n首先，用户可能只是想听一个有趣的故事，或者需要一些灵感来创作自己的故事。也有可能他们想通过这个故事获得某种情感上的满足，比如治愈、启发或者娱乐。还有一种可能是，他们在测试我的讲故事能力，想看看我能写出多精彩的内容。\\n\\n接下来，我需要考虑如何回应这个请求。既然用户没有指定故事的主题或风格，我需要选择一个既有趣又有一定深度的方向。一个不错的选择是讲一个简单的奇幻故事，因为这种类型比较容易吸引不同年龄段的读者，同时也能传递一些积极的信息。\\n\\n然后，我要开始构思故事的框架。故事需要一个主角和一个目标，最好带有一点冒险的元素，这样更有吸引力。考虑到用户可能希望故事有一定的深度，我可以在故事中融入一些关于人性、成长或者勇气的主题。\\n\\n在构思过程中，我还需要考虑如何让故事有起伏和张力。通过设置障碍和挑战，可以让故事更有吸引力。同时，我需要确保故事有一个积极的结局，给用户带来正面的感受。\\n\\n最后，我需要确定故事的语气和风格。因为用户没有特别说明，我会选择一种温暖而鼓舞人心的语气，让故事既有想象力又容易让人产生共鸣。\\n\\n现在，我已经有了一个大致的思路，可以开始正式回复用户了。◁/think▷好的，我将为你创作一个简短的故事：\\n\\n在一个古老的村庄里，住着一个名叫小明的男孩。小明天生听力不好，但他有着无比的好奇心和对世界的热爱。每当村民们在田埂上聊天，他总是踮起脚尖，试图捕捉那些模糊的声音；当他坐在门槛上，他认真地听着昆虫的鸣叫，鸟儿的啁啾，以及树叶沙沙作响的节奏。然而，由于他的听力问题，他与外界的连接总是显得遥远而不够真实。\\n\\n有一天，小明决定去寻找传说中的“寂静森林”。他背着父亲的旧水壶，穿过层层叠叠的山脉和茂密的树林，带着对未知的好奇心和一点点恐惧。在森林中，他发现了一棵巨大的橡树，它的树皮像波浪一样起伏，仿佛在呼吸一般。小明蹲下来，闭上眼睛，用心灵去感受树的脉搏——那是一种缓慢而坚定的节奏，充满了生命力。\\n\\n当小明再次睁开眼睛时，他惊讶地发现，除了风声之外，他还听到了更多的声音：蜜蜂的嗡嗡声、蝴蝶翅膀的扑翅声、甚至远处小溪潺潺的水流声。这些声音不仅仅是“听觉”的感知，更是一种更深层次的理解，仿佛他能感受到它们的振动频率，甚至能触摸到它们的存在。\\n\\n在寂静森林里，小明不仅找到了声音，也找到了自己内心深处未被察觉的智慧与勇气。他学会了倾听内心的声音，也学会了与世界对话。从此，他决定将自己的经历分享给小村庄的居民们，让他们明白真正的理解并不是依靠耳朵，而是用心去听。\\n\\n---\\n\\n这个故事虽然简单，但包含了探索、自我发现、沟通等主题，希望能带给你一些启发和乐趣！如果你有特定的主题或元素想要加入故事，请告诉我，我可以帮你进一步完善它。'}, {'role': 'user', 'content': 'Please reply to my message in English. 讲一个故事'}]\n",
            "2025-04-10 14:19:26,551 - chat-bot - DEBUG - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:193 - generate - image_inputs:None,video_inputs:None\n",
            "2025-04-10 14:20:20,124 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:95 - test_chat_completion_prompts - chat_completion TTFT time: 1.8679341830002159 s\n",
            "2025-04-10 14:20:20,124 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:96 - test_chat_completion_prompts - generated text: ◁think▷用户要求我用英语回复他们的消息并讲一个故事。这表明他们可能希望得到一个英文版本的故事，或者他们对之前提供的法语故事感兴趣。我需要确认用户的具体需求，以便提供符合他们期望的内容。\n",
            "\n",
            "如果用户只是简单地提到“Please reply to my message in English”，这可能意味着他们需要我提供一个英语版本的回答。在这种情况下，我应该直接翻译之前的法语故事成英语，确保信息准确且流畅。\n",
            "\n",
            "如果用户实际上对之前的故事感兴趣，并希望看到一个中文版本，我需要保持原来的内容不变，只调整语言部分。\n",
            "\n",
            "由于用户提供的信息有限，我无法确定他们的确切需求。因此，我会假设他们希望看到一个英文版本的故事，并在回复中直接翻译成英语。这样可以确保信息的完整性和准确性。\n",
            "\n",
            "总结一下，我的思考过程如下：\n",
            "1. 分析用户需求\n",
            "2. 确认可能的意图（英文还是中文）\n",
            "3. 提供相应的解决方案（英文翻译或保留原文）\n",
            "4. 确保回复符合预期\n",
            "\n",
            "最终，我决定直接翻译之前的法语故事为英语，因为这是最直接的回应方式，并且能够满足用户对\"讲一个故事\"的需求。◁/think▷Sure! Here's a story in English:\n",
            "\n",
            "---\n",
            "\n",
            "Once upon a time, in a small village nestled between rolling hills and ancient forests, there lived an old man named Mr. Thompson. He was known throughout the village for his wisdom and kindness.\n",
            "\n",
            "One winter evening, as snow blanketed the land, Mr. Thompson decided to visit his friend, Mrs. Harper, who lived on the other side of the village. As he walked through the quiet streets, he noticed something unusual - each house had a single light glowing in their windows.\n",
            "\n",
            "Curious about this strange phenomenon, Mr. Thompson approached Mrs. Harper's door. \"Why do all the houses have lights?\" he asked.\n",
            "\n",
            "Mrs. Harper smiled warmly, her hands wrapped around a steaming cup of cocoa. \"Every night, when darkness falls, we all share our lights. It's a tradition passed down from generations before us.\"\n",
            "\n",
            "Intrigued, Mr. Thompson asked, \"How does it work?\"\n",
            "\n",
            "\"It's simple,\" she explained, pointing to the glowing windows. \"We open our hearts to light others' paths, just as they do ours. Our lights aren't fueled by candles or oil; they're fueled by love and friendship.\"\n",
            "\n",
            "Mr. Thompson thought deeply about this for a moment. Then he nodded, understanding. He realized that true light doesn't need to be contained within walls; it can spread freely to warm and guide those around us.\n",
            "\n",
            "From that day on, Mr. Thompson made sure to join the villagers each evening in sharing their precious lights, creating bonds of community and warmth that lasted long after the last embers faded.\n",
            "\n",
            "---\n",
            "\n",
            "Let me know if you'd like me to translate this into another language or adjust the length!\n",
            "\n",
            "--------test prompt:  ('讲一个故事', 'zh') --------\n",
            "\n",
            "2025-04-10 14:20:20,124 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:83 - test_chat_completion_prompts - session ctx: {'client_id': 'test_llm_transformers_manual_vision_kimi_client_id', 'sampling_rate': 16000, 'sample_width': 2, 'read_audio_frames_len': 0, 'state': {'prompt': ('讲一个故事', 'zh')}}\n",
            "2025-04-10 14:20:20,124 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:84 - test_chat_completion_prompts - TransformersLMArgs(lm_gen_seed=42, lm_max_length=2048, lm_gen_max_new_tokens=1024, lm_gen_min_new_tokens=0, lm_gen_do_sample=True, lm_gen_temperature=0.8, lm_gen_top_k=50, lm_gen_top_p=0.95, lm_gen_min_p=0.0, lm_gen_repetition_penalty=1.1, lm_gen_stops=[], lm_gen_stop_ids=[], lm_gen_end_id=0, lm_gen_pad_id=0, lm_model_name_or_path='/content/models/moonshotai/Kimi-VL-A3B-Thinking', lm_device_map=None, lm_device='cuda', lm_torch_dtype='auto', lm_attn_impl=None, user_role='user', warnup_prompt=\"Repeat the word 'weedge niu bi'.\", warmup_steps=1, init_chat_role='system', init_chat_prompt='', lm_tokenizer_decode_batch_size=60, chat_history_size=10, lm_stream=True, model_type='chat_completion', lm_bnb_quant_type='int4')\n",
            "2025-04-10 14:20:20,125 - chat-bot - DEBUG - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:188 - generate - chat_history:[{'role': 'user', 'content': '讲一个故事'}, {'role': 'assistant', 'content': '◁think▷用户让我讲一个故事，这看起来是一个简单的请求，但背后可能有多种意图。我需要先分析一下用户的需求和潜在意图。\\n\\n首先，用户可能只是想听一个有趣的故事，或者需要一些灵感来创作自己的故事。也有可能他们想通过这个故事获得某种情感上的满足，比如治愈、启发或者娱乐。还有一种可能是，他们在测试我的讲故事能力，想看看我能写出多精彩的内容。\\n\\n接下来，我需要考虑如何回应这个请求。既然用户没有指定故事的主题或风格，我需要选择一个既有趣又有一定深度的方向。一个不错的选择是讲一个简单的奇幻故事，因为这种类型比较容易吸引不同年龄段的读者，同时也能传递一些积极的信息。\\n\\n然后，我要开始构思故事的框架。故事需要一个主角和一个目标，最好带有一点冒险的元素，这样更有吸引力。考虑到用户可能希望故事有一定的深度，我可以在故事中融入一些关于人性、成长或者勇气的主题。\\n\\n在构思过程中，我还需要考虑如何让故事有起伏和张力。通过设置障碍和挑战，可以让故事更有吸引力。同时，我需要确保故事有一个积极的结局，给用户带来正面的感受。\\n\\n最后，我需要确定故事的语气和风格。因为用户没有特别说明，我会选择一种温暖而鼓舞人心的语气，让故事既有想象力又容易让人产生共鸣。\\n\\n现在，我已经有了一个大致的思路，可以开始正式回复用户了。◁/think▷好的，我将为你创作一个简短的故事：\\n\\n在一个古老的村庄里，住着一个名叫小明的男孩。小明天生听力不好，但他有着无比的好奇心和对世界的热爱。每当村民们在田埂上聊天，他总是踮起脚尖，试图捕捉那些模糊的声音；当他坐在门槛上，他认真地听着昆虫的鸣叫，鸟儿的啁啾，以及树叶沙沙作响的节奏。然而，由于他的听力问题，他与外界的连接总是显得遥远而不够真实。\\n\\n有一天，小明决定去寻找传说中的“寂静森林”。他背着父亲的旧水壶，穿过层层叠叠的山脉和茂密的树林，带着对未知的好奇心和一点点恐惧。在森林中，他发现了一棵巨大的橡树，它的树皮像波浪一样起伏，仿佛在呼吸一般。小明蹲下来，闭上眼睛，用心灵去感受树的脉搏——那是一种缓慢而坚定的节奏，充满了生命力。\\n\\n当小明再次睁开眼睛时，他惊讶地发现，除了风声之外，他还听到了更多的声音：蜜蜂的嗡嗡声、蝴蝶翅膀的扑翅声、甚至远处小溪潺潺的水流声。这些声音不仅仅是“听觉”的感知，更是一种更深层次的理解，仿佛他能感受到它们的振动频率，甚至能触摸到它们的存在。\\n\\n在寂静森林里，小明不仅找到了声音，也找到了自己内心深处未被察觉的智慧与勇气。他学会了倾听内心的声音，也学会了与世界对话。从此，他决定将自己的经历分享给小村庄的居民们，让他们明白真正的理解并不是依靠耳朵，而是用心去听。\\n\\n---\\n\\n这个故事虽然简单，但包含了探索、自我发现、沟通等主题，希望能带给你一些启发和乐趣！如果你有特定的主题或元素想要加入故事，请告诉我，我可以帮你进一步完善它。'}, {'role': 'user', 'content': 'Please reply to my message in English. 讲一个故事'}, {'role': 'assistant', 'content': '◁think▷用户要求我用英语回复他们的消息并讲一个故事。这表明他们可能希望得到一个英文版本的故事，或者他们对之前提供的法语故事感兴趣。我需要确认用户的具体需求，以便提供符合他们期望的内容。\\n\\n如果用户只是简单地提到“Please reply to my message in English”，这可能意味着他们需要我提供一个英语版本的回答。在这种情况下，我应该直接翻译之前的法语故事成英语，确保信息准确且流畅。\\n\\n如果用户实际上对之前的故事感兴趣，并希望看到一个中文版本，我需要保持原来的内容不变，只调整语言部分。\\n\\n由于用户提供的信息有限，我无法确定他们的确切需求。因此，我会假设他们希望看到一个英文版本的故事，并在回复中直接翻译成英语。这样可以确保信息的完整性和准确性。\\n\\n总结一下，我的思考过程如下：\\n1. 分析用户需求\\n2. 确认可能的意图（英文还是中文）\\n3. 提供相应的解决方案（英文翻译或保留原文）\\n4. 确保回复符合预期\\n\\n最终，我决定直接翻译之前的法语故事为英语，因为这是最直接的回应方式，并且能够满足用户对\"讲一个故事\"的需求。◁/think▷Sure! Here\\'s a story in English:\\n\\n---\\n\\nOnce upon a time, in a small village nestled between rolling hills and ancient forests, there lived an old man named Mr. Thompson. He was known throughout the village for his wisdom and kindness.\\n\\nOne winter evening, as snow blanketed the land, Mr. Thompson decided to visit his friend, Mrs. Harper, who lived on the other side of the village. As he walked through the quiet streets, he noticed something unusual - each house had a single light glowing in their windows.\\n\\nCurious about this strange phenomenon, Mr. Thompson approached Mrs. Harper\\'s door. \"Why do all the houses have lights?\" he asked.\\n\\nMrs. Harper smiled warmly, her hands wrapped around a steaming cup of cocoa. \"Every night, when darkness falls, we all share our lights. It\\'s a tradition passed down from generations before us.\"\\n\\nIntrigued, Mr. Thompson asked, \"How does it work?\"\\n\\n\"It\\'s simple,\" she explained, pointing to the glowing windows. \"We open our hearts to light others\\' paths, just as they do ours. Our lights aren\\'t fueled by candles or oil; they\\'re fueled by love and friendship.\"\\n\\nMr. Thompson thought deeply about this for a moment. Then he nodded, understanding. He realized that true light doesn\\'t need to be contained within walls; it can spread freely to warm and guide those around us.\\n\\nFrom that day on, Mr. Thompson made sure to join the villagers each evening in sharing their precious lights, creating bonds of community and warmth that lasted long after the last embers faded.\\n\\n---\\n\\nLet me know if you\\'d like me to translate this into another language or adjust the length!'}, {'role': 'user', 'content': 'Please reply to my message in Chinese simplified(简体中文). 讲一个故事'}]\n",
            "2025-04-10 14:20:20,125 - chat-bot - DEBUG - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:193 - generate - image_inputs:None,video_inputs:None\n",
            "2025-04-10 14:20:38,616 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:95 - test_chat_completion_prompts - chat_completion TTFT time: 1.8872148400005244 s\n",
            "2025-04-10 14:20:38,617 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:96 - test_chat_completion_prompts - generated text: ◁think▷用户希望我用法语回复他们的消息并讲一个故事，这表明他们可能对之前的法语故事感兴趣，或者他们希望看到法语版本的故事。然而，用户明确要求用中文简体（简体中文）回复，这意味着我需要将之前的故事翻译成中文。\n",
            "\n",
            "我需要确认用户是否希望看到法语版本的中文翻译，或者他们是否希望我重新创作一个新的法语故事。由于用户没有明确说明，我假设他们希望看到法语版本的中文翻译。\n",
            "\n",
            "因此，我将直接翻译之前的故事为中文，并确保翻译准确且流畅。这将满足用户的需求，即使他们没有特别要求翻译，也能看到他们想要的法语故事的中文版本。\n",
            "\n",
            "总结一下，我的思考过程如下：\n",
            "1. 分析用户需求\n",
            "2. 确认可能的意图（法语版本的中文翻译）\n",
            "3. 提供相应的解决方案（翻译之前的法语故事）\n",
            "4. 确保回复符合预期\n",
            "\n",
            "最终，我决定直接翻译之前的法语故事为中文，因为这是最直接的回应方式，并且能够完全满足用户的需求。\n",
            "\n",
            "--------test prompt:  [{'type': 'image', 'image': '/content/achatbot/test/img_files/03-Confusing-Pictures.jpg'}, {'type': 'text', 'text': 'Describe this video. Please reply to my message in chinese'}] --------\n",
            "\n",
            "2025-04-10 14:20:38,617 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:83 - test_chat_completion_prompts - session ctx: {'client_id': 'test_llm_transformers_manual_vision_kimi_client_id', 'sampling_rate': 16000, 'sample_width': 2, 'read_audio_frames_len': 0, 'state': {'prompt': [{'type': 'image', 'image': '/content/achatbot/test/img_files/03-Confusing-Pictures.jpg'}, {'type': 'text', 'text': 'Describe this video. Please reply to my message in chinese'}]}}\n",
            "2025-04-10 14:20:38,617 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:84 - test_chat_completion_prompts - TransformersLMArgs(lm_gen_seed=42, lm_max_length=2048, lm_gen_max_new_tokens=1024, lm_gen_min_new_tokens=0, lm_gen_do_sample=True, lm_gen_temperature=0.8, lm_gen_top_k=50, lm_gen_top_p=0.95, lm_gen_min_p=0.0, lm_gen_repetition_penalty=1.1, lm_gen_stops=[], lm_gen_stop_ids=[], lm_gen_end_id=0, lm_gen_pad_id=0, lm_model_name_or_path='/content/models/moonshotai/Kimi-VL-A3B-Thinking', lm_device_map=None, lm_device='cuda', lm_torch_dtype='auto', lm_attn_impl=None, user_role='user', warnup_prompt=\"Repeat the word 'weedge niu bi'.\", warmup_steps=1, init_chat_role='system', init_chat_prompt='', lm_tokenizer_decode_batch_size=60, chat_history_size=10, lm_stream=True, model_type='chat_completion', lm_bnb_quant_type='int4')\n",
            "2025-04-10 14:20:38,617 - chat-bot - DEBUG - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:188 - generate - chat_history:[{'role': 'user', 'content': '讲一个故事'}, {'role': 'assistant', 'content': '◁think▷用户让我讲一个故事，这看起来是一个简单的请求，但背后可能有多种意图。我需要先分析一下用户的需求和潜在意图。\\n\\n首先，用户可能只是想听一个有趣的故事，或者需要一些灵感来创作自己的故事。也有可能他们想通过这个故事获得某种情感上的满足，比如治愈、启发或者娱乐。还有一种可能是，他们在测试我的讲故事能力，想看看我能写出多精彩的内容。\\n\\n接下来，我需要考虑如何回应这个请求。既然用户没有指定故事的主题或风格，我需要选择一个既有趣又有一定深度的方向。一个不错的选择是讲一个简单的奇幻故事，因为这种类型比较容易吸引不同年龄段的读者，同时也能传递一些积极的信息。\\n\\n然后，我要开始构思故事的框架。故事需要一个主角和一个目标，最好带有一点冒险的元素，这样更有吸引力。考虑到用户可能希望故事有一定的深度，我可以在故事中融入一些关于人性、成长或者勇气的主题。\\n\\n在构思过程中，我还需要考虑如何让故事有起伏和张力。通过设置障碍和挑战，可以让故事更有吸引力。同时，我需要确保故事有一个积极的结局，给用户带来正面的感受。\\n\\n最后，我需要确定故事的语气和风格。因为用户没有特别说明，我会选择一种温暖而鼓舞人心的语气，让故事既有想象力又容易让人产生共鸣。\\n\\n现在，我已经有了一个大致的思路，可以开始正式回复用户了。◁/think▷好的，我将为你创作一个简短的故事：\\n\\n在一个古老的村庄里，住着一个名叫小明的男孩。小明天生听力不好，但他有着无比的好奇心和对世界的热爱。每当村民们在田埂上聊天，他总是踮起脚尖，试图捕捉那些模糊的声音；当他坐在门槛上，他认真地听着昆虫的鸣叫，鸟儿的啁啾，以及树叶沙沙作响的节奏。然而，由于他的听力问题，他与外界的连接总是显得遥远而不够真实。\\n\\n有一天，小明决定去寻找传说中的“寂静森林”。他背着父亲的旧水壶，穿过层层叠叠的山脉和茂密的树林，带着对未知的好奇心和一点点恐惧。在森林中，他发现了一棵巨大的橡树，它的树皮像波浪一样起伏，仿佛在呼吸一般。小明蹲下来，闭上眼睛，用心灵去感受树的脉搏——那是一种缓慢而坚定的节奏，充满了生命力。\\n\\n当小明再次睁开眼睛时，他惊讶地发现，除了风声之外，他还听到了更多的声音：蜜蜂的嗡嗡声、蝴蝶翅膀的扑翅声、甚至远处小溪潺潺的水流声。这些声音不仅仅是“听觉”的感知，更是一种更深层次的理解，仿佛他能感受到它们的振动频率，甚至能触摸到它们的存在。\\n\\n在寂静森林里，小明不仅找到了声音，也找到了自己内心深处未被察觉的智慧与勇气。他学会了倾听内心的声音，也学会了与世界对话。从此，他决定将自己的经历分享给小村庄的居民们，让他们明白真正的理解并不是依靠耳朵，而是用心去听。\\n\\n---\\n\\n这个故事虽然简单，但包含了探索、自我发现、沟通等主题，希望能带给你一些启发和乐趣！如果你有特定的主题或元素想要加入故事，请告诉我，我可以帮你进一步完善它。'}, {'role': 'user', 'content': 'Please reply to my message in English. 讲一个故事'}, {'role': 'assistant', 'content': '◁think▷用户要求我用英语回复他们的消息并讲一个故事。这表明他们可能希望得到一个英文版本的故事，或者他们对之前提供的法语故事感兴趣。我需要确认用户的具体需求，以便提供符合他们期望的内容。\\n\\n如果用户只是简单地提到“Please reply to my message in English”，这可能意味着他们需要我提供一个英语版本的回答。在这种情况下，我应该直接翻译之前的法语故事成英语，确保信息准确且流畅。\\n\\n如果用户实际上对之前的故事感兴趣，并希望看到一个中文版本，我需要保持原来的内容不变，只调整语言部分。\\n\\n由于用户提供的信息有限，我无法确定他们的确切需求。因此，我会假设他们希望看到一个英文版本的故事，并在回复中直接翻译成英语。这样可以确保信息的完整性和准确性。\\n\\n总结一下，我的思考过程如下：\\n1. 分析用户需求\\n2. 确认可能的意图（英文还是中文）\\n3. 提供相应的解决方案（英文翻译或保留原文）\\n4. 确保回复符合预期\\n\\n最终，我决定直接翻译之前的法语故事为英语，因为这是最直接的回应方式，并且能够满足用户对\"讲一个故事\"的需求。◁/think▷Sure! Here\\'s a story in English:\\n\\n---\\n\\nOnce upon a time, in a small village nestled between rolling hills and ancient forests, there lived an old man named Mr. Thompson. He was known throughout the village for his wisdom and kindness.\\n\\nOne winter evening, as snow blanketed the land, Mr. Thompson decided to visit his friend, Mrs. Harper, who lived on the other side of the village. As he walked through the quiet streets, he noticed something unusual - each house had a single light glowing in their windows.\\n\\nCurious about this strange phenomenon, Mr. Thompson approached Mrs. Harper\\'s door. \"Why do all the houses have lights?\" he asked.\\n\\nMrs. Harper smiled warmly, her hands wrapped around a steaming cup of cocoa. \"Every night, when darkness falls, we all share our lights. It\\'s a tradition passed down from generations before us.\"\\n\\nIntrigued, Mr. Thompson asked, \"How does it work?\"\\n\\n\"It\\'s simple,\" she explained, pointing to the glowing windows. \"We open our hearts to light others\\' paths, just as they do ours. Our lights aren\\'t fueled by candles or oil; they\\'re fueled by love and friendship.\"\\n\\nMr. Thompson thought deeply about this for a moment. Then he nodded, understanding. He realized that true light doesn\\'t need to be contained within walls; it can spread freely to warm and guide those around us.\\n\\nFrom that day on, Mr. Thompson made sure to join the villagers each evening in sharing their precious lights, creating bonds of community and warmth that lasted long after the last embers faded.\\n\\n---\\n\\nLet me know if you\\'d like me to translate this into another language or adjust the length!'}, {'role': 'user', 'content': 'Please reply to my message in Chinese simplified(简体中文). 讲一个故事'}, {'role': 'assistant', 'content': '◁think▷用户希望我用法语回复他们的消息并讲一个故事，这表明他们可能对之前的法语故事感兴趣，或者他们希望看到法语版本的故事。然而，用户明确要求用中文简体（简体中文）回复，这意味着我需要将之前的故事翻译成中文。\\n\\n我需要确认用户是否希望看到法语版本的中文翻译，或者他们是否希望我重新创作一个新的法语故事。由于用户没有明确说明，我假设他们希望看到法语版本的中文翻译。\\n\\n因此，我将直接翻译之前的故事为中文，并确保翻译准确且流畅。这将满足用户的需求，即使他们没有特别要求翻译，也能看到他们想要的法语故事的中文版本。\\n\\n总结一下，我的思考过程如下：\\n1. 分析用户需求\\n2. 确认可能的意图（法语版本的中文翻译）\\n3. 提供相应的解决方案（翻译之前的法语故事）\\n4. 确保回复符合预期\\n\\n最终，我决定直接翻译之前的法语故事为中文，因为这是最直接的回应方式，并且能够完全满足用户的需求。'}, {'role': 'user', 'content': [{'type': 'image', 'image': '/content/achatbot/test/img_files/03-Confusing-Pictures.jpg'}, {'type': 'text', 'text': 'Describe this video. Please reply to my message in chinese'}]}]\n",
            "2025-04-10 14:20:38,631 - chat-bot - DEBUG - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:193 - generate - image_inputs:[<PIL.Image.Image image mode=RGB size=560x392 at 0x7824E92858D0>],video_inputs:None\n",
            "2025-04-10 14:20:55,697 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:95 - test_chat_completion_prompts - chat_completion TTFT time: 1.7324367840001287 s\n",
            "2025-04-10 14:20:55,697 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:96 - test_chat_completion_prompts - generated text: ◁think▷用户希望我描述一张图片中的场景，并用法语回复。然而，用户后来明确表示希望我用中文（简体中文）回复，而不是法语。\n",
            "\n",
            "我需要重新审视用户的需求。用户上传了一张图片，希望我对图片进行描述，并以中文回复。这表明用户可能对我的语言能力有一定期待，或者他们更希望看到中文描述。\n",
            "\n",
            "为了准确满足用户的需求，我将专注于图片描述，并以中文进行回复。这种需求非常明确，不需要复杂的推理或额外的信息处理。\n",
            "\n",
            "总结我的思考过程：\n",
            "1. 用户上传了一张图片，希望进行描述。\n",
            "2. 用户明确要求用法语回复，但后来改变了要求。\n",
            "3. 我需要根据用户的需求，选择最合理的回应方式。\n",
            "4. 用户的最终要求是中文（简体中文）回复，因此我将专注于图片描述并用中文完成。\n",
            "\n",
            "现在，我将开始描述这张图片中的场景，并以中文回复用户。\n",
            "\n",
            "--------test prompt:  [{'type': 'video', 'video': ['https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg', 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg', 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg', 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'], 'fps': 1.0}, {'type': 'text', 'text': 'Describe this video. Please reply to my message in chinese'}] --------\n",
            "\n",
            "2025-04-10 14:20:55,697 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:83 - test_chat_completion_prompts - session ctx: {'client_id': 'test_llm_transformers_manual_vision_kimi_client_id', 'sampling_rate': 16000, 'sample_width': 2, 'read_audio_frames_len': 0, 'state': {'prompt': [{'type': 'video', 'video': ['https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg', 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg', 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg', 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'], 'fps': 1.0}, {'type': 'text', 'text': 'Describe this video. Please reply to my message in chinese'}]}}\n",
            "2025-04-10 14:20:55,697 - chat-bot - DEBUG - /content/achatbot/test/core/llm/test_transformers_v_kimi.py:84 - test_chat_completion_prompts - TransformersLMArgs(lm_gen_seed=42, lm_max_length=2048, lm_gen_max_new_tokens=1024, lm_gen_min_new_tokens=0, lm_gen_do_sample=True, lm_gen_temperature=0.8, lm_gen_top_k=50, lm_gen_top_p=0.95, lm_gen_min_p=0.0, lm_gen_repetition_penalty=1.1, lm_gen_stops=[], lm_gen_stop_ids=[], lm_gen_end_id=0, lm_gen_pad_id=0, lm_model_name_or_path='/content/models/moonshotai/Kimi-VL-A3B-Thinking', lm_device_map=None, lm_device='cuda', lm_torch_dtype='auto', lm_attn_impl=None, user_role='user', warnup_prompt=\"Repeat the word 'weedge niu bi'.\", warmup_steps=1, init_chat_role='system', init_chat_prompt='', lm_tokenizer_decode_batch_size=60, chat_history_size=10, lm_stream=True, model_type='chat_completion', lm_bnb_quant_type='int4')\n",
            "2025-04-10 14:20:55,697 - chat-bot - DEBUG - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:188 - generate - chat_history:[{'role': 'user', 'content': '讲一个故事'}, {'role': 'assistant', 'content': '◁think▷用户让我讲一个故事，这看起来是一个简单的请求，但背后可能有多种意图。我需要先分析一下用户的需求和潜在意图。\\n\\n首先，用户可能只是想听一个有趣的故事，或者需要一些灵感来创作自己的故事。也有可能他们想通过这个故事获得某种情感上的满足，比如治愈、启发或者娱乐。还有一种可能是，他们在测试我的讲故事能力，想看看我能写出多精彩的内容。\\n\\n接下来，我需要考虑如何回应这个请求。既然用户没有指定故事的主题或风格，我需要选择一个既有趣又有一定深度的方向。一个不错的选择是讲一个简单的奇幻故事，因为这种类型比较容易吸引不同年龄段的读者，同时也能传递一些积极的信息。\\n\\n然后，我要开始构思故事的框架。故事需要一个主角和一个目标，最好带有一点冒险的元素，这样更有吸引力。考虑到用户可能希望故事有一定的深度，我可以在故事中融入一些关于人性、成长或者勇气的主题。\\n\\n在构思过程中，我还需要考虑如何让故事有起伏和张力。通过设置障碍和挑战，可以让故事更有吸引力。同时，我需要确保故事有一个积极的结局，给用户带来正面的感受。\\n\\n最后，我需要确定故事的语气和风格。因为用户没有特别说明，我会选择一种温暖而鼓舞人心的语气，让故事既有想象力又容易让人产生共鸣。\\n\\n现在，我已经有了一个大致的思路，可以开始正式回复用户了。◁/think▷好的，我将为你创作一个简短的故事：\\n\\n在一个古老的村庄里，住着一个名叫小明的男孩。小明天生听力不好，但他有着无比的好奇心和对世界的热爱。每当村民们在田埂上聊天，他总是踮起脚尖，试图捕捉那些模糊的声音；当他坐在门槛上，他认真地听着昆虫的鸣叫，鸟儿的啁啾，以及树叶沙沙作响的节奏。然而，由于他的听力问题，他与外界的连接总是显得遥远而不够真实。\\n\\n有一天，小明决定去寻找传说中的“寂静森林”。他背着父亲的旧水壶，穿过层层叠叠的山脉和茂密的树林，带着对未知的好奇心和一点点恐惧。在森林中，他发现了一棵巨大的橡树，它的树皮像波浪一样起伏，仿佛在呼吸一般。小明蹲下来，闭上眼睛，用心灵去感受树的脉搏——那是一种缓慢而坚定的节奏，充满了生命力。\\n\\n当小明再次睁开眼睛时，他惊讶地发现，除了风声之外，他还听到了更多的声音：蜜蜂的嗡嗡声、蝴蝶翅膀的扑翅声、甚至远处小溪潺潺的水流声。这些声音不仅仅是“听觉”的感知，更是一种更深层次的理解，仿佛他能感受到它们的振动频率，甚至能触摸到它们的存在。\\n\\n在寂静森林里，小明不仅找到了声音，也找到了自己内心深处未被察觉的智慧与勇气。他学会了倾听内心的声音，也学会了与世界对话。从此，他决定将自己的经历分享给小村庄的居民们，让他们明白真正的理解并不是依靠耳朵，而是用心去听。\\n\\n---\\n\\n这个故事虽然简单，但包含了探索、自我发现、沟通等主题，希望能带给你一些启发和乐趣！如果你有特定的主题或元素想要加入故事，请告诉我，我可以帮你进一步完善它。'}, {'role': 'user', 'content': 'Please reply to my message in English. 讲一个故事'}, {'role': 'assistant', 'content': '◁think▷用户要求我用英语回复他们的消息并讲一个故事。这表明他们可能希望得到一个英文版本的故事，或者他们对之前提供的法语故事感兴趣。我需要确认用户的具体需求，以便提供符合他们期望的内容。\\n\\n如果用户只是简单地提到“Please reply to my message in English”，这可能意味着他们需要我提供一个英语版本的回答。在这种情况下，我应该直接翻译之前的法语故事成英语，确保信息准确且流畅。\\n\\n如果用户实际上对之前的故事感兴趣，并希望看到一个中文版本，我需要保持原来的内容不变，只调整语言部分。\\n\\n由于用户提供的信息有限，我无法确定他们的确切需求。因此，我会假设他们希望看到一个英文版本的故事，并在回复中直接翻译成英语。这样可以确保信息的完整性和准确性。\\n\\n总结一下，我的思考过程如下：\\n1. 分析用户需求\\n2. 确认可能的意图（英文还是中文）\\n3. 提供相应的解决方案（英文翻译或保留原文）\\n4. 确保回复符合预期\\n\\n最终，我决定直接翻译之前的法语故事为英语，因为这是最直接的回应方式，并且能够满足用户对\"讲一个故事\"的需求。◁/think▷Sure! Here\\'s a story in English:\\n\\n---\\n\\nOnce upon a time, in a small village nestled between rolling hills and ancient forests, there lived an old man named Mr. Thompson. He was known throughout the village for his wisdom and kindness.\\n\\nOne winter evening, as snow blanketed the land, Mr. Thompson decided to visit his friend, Mrs. Harper, who lived on the other side of the village. As he walked through the quiet streets, he noticed something unusual - each house had a single light glowing in their windows.\\n\\nCurious about this strange phenomenon, Mr. Thompson approached Mrs. Harper\\'s door. \"Why do all the houses have lights?\" he asked.\\n\\nMrs. Harper smiled warmly, her hands wrapped around a steaming cup of cocoa. \"Every night, when darkness falls, we all share our lights. It\\'s a tradition passed down from generations before us.\"\\n\\nIntrigued, Mr. Thompson asked, \"How does it work?\"\\n\\n\"It\\'s simple,\" she explained, pointing to the glowing windows. \"We open our hearts to light others\\' paths, just as they do ours. Our lights aren\\'t fueled by candles or oil; they\\'re fueled by love and friendship.\"\\n\\nMr. Thompson thought deeply about this for a moment. Then he nodded, understanding. He realized that true light doesn\\'t need to be contained within walls; it can spread freely to warm and guide those around us.\\n\\nFrom that day on, Mr. Thompson made sure to join the villagers each evening in sharing their precious lights, creating bonds of community and warmth that lasted long after the last embers faded.\\n\\n---\\n\\nLet me know if you\\'d like me to translate this into another language or adjust the length!'}, {'role': 'user', 'content': 'Please reply to my message in Chinese simplified(简体中文). 讲一个故事'}, {'role': 'assistant', 'content': '◁think▷用户希望我用法语回复他们的消息并讲一个故事，这表明他们可能对之前的法语故事感兴趣，或者他们希望看到法语版本的故事。然而，用户明确要求用中文简体（简体中文）回复，这意味着我需要将之前的故事翻译成中文。\\n\\n我需要确认用户是否希望看到法语版本的中文翻译，或者他们是否希望我重新创作一个新的法语故事。由于用户没有明确说明，我假设他们希望看到法语版本的中文翻译。\\n\\n因此，我将直接翻译之前的故事为中文，并确保翻译准确且流畅。这将满足用户的需求，即使他们没有特别要求翻译，也能看到他们想要的法语故事的中文版本。\\n\\n总结一下，我的思考过程如下：\\n1. 分析用户需求\\n2. 确认可能的意图（法语版本的中文翻译）\\n3. 提供相应的解决方案（翻译之前的法语故事）\\n4. 确保回复符合预期\\n\\n最终，我决定直接翻译之前的法语故事为中文，因为这是最直接的回应方式，并且能够完全满足用户的需求。'}, {'role': 'user', 'content': [{'type': 'image', 'image': '/content/achatbot/test/img_files/03-Confusing-Pictures.jpg'}, {'type': 'text', 'text': 'Describe this video. Please reply to my message in chinese'}]}, {'role': 'assistant', 'content': '◁think▷用户希望我描述一张图片中的场景，并用法语回复。然而，用户后来明确表示希望我用中文（简体中文）回复，而不是法语。\\n\\n我需要重新审视用户的需求。用户上传了一张图片，希望我对图片进行描述，并以中文回复。这表明用户可能对我的语言能力有一定期待，或者他们更希望看到中文描述。\\n\\n为了准确满足用户的需求，我将专注于图片描述，并以中文进行回复。这种需求非常明确，不需要复杂的推理或额外的信息处理。\\n\\n总结我的思考过程：\\n1. 用户上传了一张图片，希望进行描述。\\n2. 用户明确要求用法语回复，但后来改变了要求。\\n3. 我需要根据用户的需求，选择最合理的回应方式。\\n4. 用户的最终要求是中文（简体中文）回复，因此我将专注于图片描述并用中文完成。\\n\\n现在，我将开始描述这张图片中的场景，并以中文回复用户。'}, {'role': 'user', 'content': [{'type': 'video', 'video': ['https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg', 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg', 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg', 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'], 'fps': 1.0}, {'type': 'text', 'text': 'Describe this video. Please reply to my message in chinese'}]}]\n",
            "2025-04-10 14:20:55,707 - urllib3.connectionpool - DEBUG - /usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1049 - _new_conn - Starting new HTTPS connection (1): qianwen-res.oss-cn-beijing.aliyuncs.com:443\n",
            "2025-04-10 14:20:56,196 - urllib3.connectionpool - DEBUG - /usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:544 - _make_request - https://qianwen-res.oss-cn-beijing.aliyuncs.com:443 \"GET /Qwen-VL/assets/demo.jpeg HTTP/1.1\" 200 496395\n",
            "2025-04-10 14:20:56,776 - urllib3.connectionpool - DEBUG - /usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1049 - _new_conn - Starting new HTTPS connection (1): qianwen-res.oss-cn-beijing.aliyuncs.com:443\n",
            "2025-04-10 14:20:57,458 - urllib3.connectionpool - DEBUG - /usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:544 - _make_request - https://qianwen-res.oss-cn-beijing.aliyuncs.com:443 \"GET /Qwen-VL/assets/demo.jpeg HTTP/1.1\" 200 496395\n",
            "2025-04-10 14:20:58,005 - urllib3.connectionpool - DEBUG - /usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1049 - _new_conn - Starting new HTTPS connection (1): qianwen-res.oss-cn-beijing.aliyuncs.com:443\n",
            "2025-04-10 14:20:58,679 - urllib3.connectionpool - DEBUG - /usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:544 - _make_request - https://qianwen-res.oss-cn-beijing.aliyuncs.com:443 \"GET /Qwen-VL/assets/demo.jpeg HTTP/1.1\" 200 496395\n",
            "2025-04-10 14:20:59,207 - urllib3.connectionpool - DEBUG - /usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1049 - _new_conn - Starting new HTTPS connection (1): qianwen-res.oss-cn-beijing.aliyuncs.com:443\n",
            "2025-04-10 14:20:59,613 - urllib3.connectionpool - DEBUG - /usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:544 - _make_request - https://qianwen-res.oss-cn-beijing.aliyuncs.com:443 \"GET /Qwen-VL/assets/demo.jpeg HTTP/1.1\" 200 496395\n",
            "2025-04-10 14:21:00,150 - chat-bot - DEBUG - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:193 - generate - image_inputs:[<PIL.Image.Image image mode=RGB size=560x392 at 0x7824E92C2B10>],video_inputs:[[<PIL.Image.Image image mode=RGB size=2044x1372 at 0x7824E929AB90>, <PIL.Image.Image image mode=RGB size=2044x1372 at 0x7824E9298A10>, <PIL.Image.Image image mode=RGB size=2044x1372 at 0x7824E92EAB90>, <PIL.Image.Image image mode=RGB size=2044x1372 at 0x7824E919BA50>]]\n",
            "Exception in thread Thread-7 (generate):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 2326, in generate\n",
            "    result = self._sample(\n",
            "             ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 3286, in _sample\n",
            "    outputs = self(**model_inputs, return_dict=True)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/Kimi-VL-A3B-Thinking/modeling_kimi_vl.py\", line 2538, in forward\n",
            "    outputs = self.language_model(\n",
            "              ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/Kimi-VL-A3B-Thinking/modeling_kimi_vl.py\", line 2130, in forward\n",
            "    logits = logits.float()\n",
            "             ^^^^^^^^^^^^^^\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 39.56 GiB of which 1.14 GiB is free. Process 215619 has 38.40 GiB memory in use. Of the allocated memory 32.03 GiB is allocated by PyTorch, and 5.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# run vision bot with webrtc"
      ],
      "metadata": {
        "id": "Uy6qViW9cKl7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Fg2ApmcbZ3Vc"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "DAILY_API_KEY=userdata.get('DAILY_API_KEY')\n",
        "\n",
        "LIVEKIT_URL=userdata.get('LIVEKIT_URL')\n",
        "LIVEKIT_API_KEY=userdata.get('LIVEKIT_API_KEY')\n",
        "LIVEKIT_API_SECRET=userdata.get('LIVEKIT_API_SECRET')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhPOa8RBb0dS"
      },
      "source": [
        "run bot task woker with bot.json, e.g.: dummy_bot.json\n",
        "\n",
        "- use daily/livekit room stream, u can click bot joined the room url, to start chat with bot with audio and camera stream,\n",
        "  - [daily](https://www.daily.co/) need DAILY_API_KEY\n",
        "  - [livekit](https://livekit.io/) need project url LIVEKIT_URL, LIVEKIT_API_KEY, LIVEKIT_API_SECRET\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## qwen2-vl-7B"
      ],
      "metadata": {
        "id": "Bn5fYec8iDnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/livekit_describe_transformers_vision_bot.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8r5g8-u0paks",
        "outputId": "7b895b0b-1607-47be-d821-40944db605a8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"chat_bot_name\": \"LivekitDescribeVisionBot\",\n",
            "  \"room_name\": \"chat-bot\",\n",
            "  \"room_url\": \"\",\n",
            "  \"token\": \"\",\n",
            "  \"room_manager\": {\n",
            "    \"tag\": \"livekit_room\",\n",
            "    \"args\": {\n",
            "      \"bot_name\": \"LivekitDescribeVisionBot\",\n",
            "      \"is_common_session\": false\n",
            "    }\n",
            "  },\n",
            "  \"services\": {\n",
            "    \"pipeline\": \"achatbot\",\n",
            "    \"vad\": \"silero\",\n",
            "    \"asr\": \"sense_voice\",\n",
            "    \"llm\": \"transformers_manual_vision_qwen\",\n",
            "    \"tts\": \"edge\"\n",
            "  },\n",
            "  \"config\": {\n",
            "    \"vad\": {\n",
            "      \"tag\": \"silero_vad_analyzer\",\n",
            "      \"args\": { \"stop_secs\": 0.7 }\n",
            "    },\n",
            "    \"asr\": {\n",
            "      \"tag\": \"sense_voice_asr\",\n",
            "      \"args\": {\n",
            "        \"language\": \"zn\",\n",
            "        \"model_name_or_path\": \"/content/models/FunAudioLLM/SenseVoiceSmall\"\n",
            "      }\n",
            "    },\n",
            "    \"llm\": {\n",
            "      \"tag\": \"llm_transformers_manual_vision_qwen\",\n",
            "      \"args\": {\n",
            "        \"lm_device\": \"cuda\",\n",
            "        \"lm_model_name_or_path\": \"/content/models/Qwen/Qwen2-VL-7B-Instruct\",\n",
            "        \"chat_history_size\": 0,\n",
            "        \"init_chat_prompt\": \"请用中文交流\",\n",
            "        \"model_type\": \"chat_completion\"\n",
            "      },\n",
            "      \"language\": \"zh\"\n",
            "    },\n",
            "    \"tts\": {\n",
            "      \"tag\": \"tts_edge\",\n",
            "      \"args\": {\n",
            "        \"voice_name\": \"zh-CN-YunjianNeural\",\n",
            "        \"language\": \"zh\",\n",
            "        \"gender\": \"Male\"\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"config_list\": []\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "NKB146E-b9s_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5915073d-a52b-4bfc-817a-31a782d083e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"chat_bot_name\": \"LivekitDescribeVisionBot\",\n",
            "    \"config\": {\n",
            "        \"asr\": {\n",
            "            \"args\": {\n",
            "                \"language\": \"zn\",\n",
            "                \"model_name_or_path\": \"/content/models/FunAudioLLM/SenseVoiceSmall\"\n",
            "            },\n",
            "            \"tag\": \"sense_voice_asr\"\n",
            "        },\n",
            "        \"llm\": {\n",
            "            \"args\": {\n",
            "                \"chat_history_size\": 0,\n",
            "                \"init_chat_prompt\": \"\\u8bf7\\u7528\\u4e2d\\u6587\\u4ea4\\u6d41\",\n",
            "                \"lm_device\": \"cuda\",\n",
            "                \"lm_model_name_or_path\": \"/content/models/Qwen/Qwen2-VL-7B-Instruct\",\n",
            "                \"model_type\": \"chat_completion\"\n",
            "            },\n",
            "            \"language\": \"zh\",\n",
            "            \"tag\": \"llm_transformers_manual_vision_qwen\"\n",
            "        },\n",
            "        \"tts\": {\n",
            "            \"args\": {\n",
            "                \"gender\": \"Male\",\n",
            "                \"language\": \"zh\",\n",
            "                \"voice_name\": \"zh-CN-YunjianNeural\"\n",
            "            },\n",
            "            \"tag\": \"tts_edge\"\n",
            "        },\n",
            "        \"vad\": {\n",
            "            \"args\": {\n",
            "                \"stop_secs\": 0.7\n",
            "            },\n",
            "            \"tag\": \"silero_vad_analyzer\"\n",
            "        }\n",
            "    },\n",
            "    \"config_list\": [],\n",
            "    \"room_manager\": {\n",
            "        \"args\": {\n",
            "            \"bot_name\": \"LivekitDescribeVisionBot\",\n",
            "            \"is_common_session\": false\n",
            "        },\n",
            "        \"tag\": \"livekit_room\"\n",
            "    },\n",
            "    \"room_name\": \"chat-bot\",\n",
            "    \"room_url\": \"\",\n",
            "    \"services\": {\n",
            "        \"asr\": \"sense_voice\",\n",
            "        \"llm\": \"transformers_manual_vision_qwen\",\n",
            "        \"pipeline\": \"achatbot\",\n",
            "        \"tts\": \"edge\",\n",
            "        \"vad\": \"silero\"\n",
            "    },\n",
            "    \"token\": \"\"\n",
            "}\n",
            "2025-04-10 16:24:38,232 - chat-bot - INFO - /content/achatbot/src/cmd/bots/main.py:63 - <module> - bot_config:{'chat_bot_name': 'LivekitDescribeVisionBot', 'room_name': 'chat-bot', 'room_url': '', 'token': '', 'room_manager': {'tag': 'livekit_room', 'args': {'bot_name': 'LivekitDescribeVisionBot', 'is_common_session': False}}, 'services': {'pipeline': 'achatbot', 'vad': 'silero', 'asr': 'sense_voice', 'llm': 'transformers_manual_vision_qwen', 'tts': 'edge'}, 'config': {'vad': {'tag': 'silero_vad_analyzer', 'args': {'stop_secs': 0.7}}, 'asr': {'tag': 'sense_voice_asr', 'args': {'language': 'zn', 'model_name_or_path': '/content/models/FunAudioLLM/SenseVoiceSmall'}}, 'llm': {'tag': 'llm_transformers_manual_vision_qwen', 'args': {'lm_device': 'cuda', 'lm_model_name_or_path': '/content/models/Qwen/Qwen2-VL-7B-Instruct', 'chat_history_size': 0, 'init_chat_prompt': '请用中文交流', 'model_type': 'chat_completion'}, 'language': 'zh'}, 'tts': {'tag': 'tts_edge', 'args': {'voice_name': 'zh-CN-YunjianNeural', 'language': 'zh', 'gender': 'Male'}}}, 'config_list': []}\n",
            "2025-04-10 16:24:38,232 - chat-bot - INFO - /content/achatbot/src/cmd/bots/run.py:33 - __init__ - run_bot_info: is_agent=False chat_bot_name='LivekitDescribeVisionBot' config={'vad': {'tag': 'silero_vad_analyzer', 'args': {'stop_secs': 0.7}}, 'asr': {'tag': 'sense_voice_asr', 'args': {'language': 'zn', 'model_name_or_path': '/content/models/FunAudioLLM/SenseVoiceSmall'}}, 'llm': {'tag': 'llm_transformers_manual_vision_qwen', 'args': {'lm_device': 'cuda', 'lm_model_name_or_path': '/content/models/Qwen/Qwen2-VL-7B-Instruct', 'chat_history_size': 0, 'init_chat_prompt': '请用中文交流', 'model_type': 'chat_completion'}, 'language': 'zh'}, 'tts': {'tag': 'tts_edge', 'args': {'voice_name': 'zh-CN-YunjianNeural', 'language': 'zh', 'gender': 'Male'}}} room_name='chat-bot' room_url='' token='' config_list=[] services={'pipeline': 'achatbot', 'vad': 'silero', 'asr': 'sense_voice', 'llm': 'transformers_manual_vision_qwen', 'tts': 'edge'} websocket_server_host='localhost' websocket_server_port=8765 transport_type='room' handle_sigint=True task_connector=None room_manager=EngineClassInfo(tag='livekit_room', args={'bot_name': 'LivekitDescribeVisionBot', 'is_common_session': False})\n",
            "2025-04-10 16:24:38,432 - chat-bot - INFO - /content/achatbot/src/common/factory.py:69 - get_engine_by_tag - use livekit_room engine\n",
            "2025-04-10 16:24:38,432 - chat-bot - INFO - /content/achatbot/src/common/factory.py:34 - get_instance - class: <class 'src.services.help.livekit_room.LivekitRoom'> args: {'bot_name': 'LivekitDescribeVisionBot', 'is_common_session': False}\n",
            "2025-04-10 16:24:38,433 - chat-bot - INFO - /content/achatbot/src/services/help/__init__.py:37 - initEngine - initEngine: livekit_room, TAG:livekit_room | LivekitRoom\n",
            "2025-04-10 16:24:40,315 - chat-bot - INFO - /content/achatbot/src/cmd/bots/base.py:77 - init_bot_config - ai bot_config: vad=VADConfig(tag='silero_vad_analyzer', args={'stop_secs': 0.7}) asr=ASRConfig(tag='sense_voice_asr', args={'language': 'zn', 'model_name_or_path': '/content/models/FunAudioLLM/SenseVoiceSmall'}) llm=LLMConfig(base_url=None, model=None, language='zh', messages=None, tools=None, tag='llm_transformers_manual_vision_qwen', args={'lm_device': 'cuda', 'lm_model_name_or_path': '/content/models/Qwen/Qwen2-VL-7B-Instruct', 'chat_history_size': 0, 'init_chat_prompt': '请用中文交流', 'model_type': 'chat_completion'}) nlp_task_llm=None voice_llm=None vision_llm=None omni_llm=None vision_detector=None vision_ocr=None tts=TTSConfig(voice=None, language=None, tag='tts_edge', args={'voice_name': 'zh-CN-YunjianNeural', 'language': 'zh', 'gender': 'Male'}) img_gen=None extends=None\n",
            "2025-04-10 16:24:40,327 - chat-bot - INFO - /content/achatbot/src/common/factory.py:69 - get_engine_by_tag - use silero_vad_analyzer engine\n",
            "2025-04-10 16:24:40,327 - chat-bot - INFO - /content/achatbot/src/common/factory.py:34 - get_instance - class: <class 'src.modules.speech.vad_analyzer.silero.SileroVADAnalyzer'> args: {'stop_secs': 0.7}\n",
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n",
            "2025-04-10 16:24:41,280 - chat-bot - INFO - /content/achatbot/src/common/factory.py:69 - get_engine_by_tag - use sense_voice_asr engine\n",
            "2025-04-10 16:24:41,280 - chat-bot - INFO - /content/achatbot/src/common/factory.py:34 - get_instance - class: <class 'src.modules.speech.asr.sense_voice_asr.SenseVoiceAsr'> args: {'download_path': '', 'model_name_or_path': '/content/models/FunAudioLLM/SenseVoiceSmall', 'language': 'zn', 'verbose': True, 'prompt': '', 'sample_rate': 16000, 'device': None}\n",
            "2025-04-10 16:24:42,380 - numexpr.utils - INFO - /usr/local/lib/python3.11/dist-packages/numexpr/utils.py:162 - _init_num_threads - NumExpr defaulting to 12 threads.\n",
            "2025-04-10 16:24:49,112 - chat-bot - INFO - /usr/local/lib/python3.11/dist-packages/funasr/register.py:55 - decorator - New registry table added: preprocessor_classes\n",
            "2025-04-10 16:24:49,436 - chat-bot - WARNING - /usr/local/lib/python3.11/dist-packages/funasr/register.py:61 - decorator - Key Conformer already exists in model_classes, re-register\n",
            "2025-04-10 16:24:50,083 - chat-bot - INFO - /usr/local/lib/python3.11/dist-packages/funasr/register.py:55 - decorator - New registry table added: adaptor_classes\n",
            "2025-04-10 16:24:50,388 - chat-bot - WARNING - /usr/local/lib/python3.11/dist-packages/funasr/register.py:61 - decorator - Key Linear already exists in adaptor_classes, re-register\n",
            "2025-04-10 16:24:50,483 - chat-bot - WARNING - /usr/local/lib/python3.11/dist-packages/funasr/register.py:61 - decorator - Key TransformerDecoder already exists in decoder_classes, re-register\n",
            "2025-04-10 16:24:50,496 - chat-bot - WARNING - /usr/local/lib/python3.11/dist-packages/funasr/register.py:61 - decorator - Key LightweightConvolutionTransformerDecoder already exists in decoder_classes, re-register\n",
            "2025-04-10 16:24:50,503 - chat-bot - WARNING - /usr/local/lib/python3.11/dist-packages/funasr/register.py:61 - decorator - Key LightweightConvolution2DTransformerDecoder already exists in decoder_classes, re-register\n",
            "2025-04-10 16:24:50,510 - chat-bot - WARNING - /usr/local/lib/python3.11/dist-packages/funasr/register.py:61 - decorator - Key DynamicConvolutionTransformerDecoder already exists in decoder_classes, re-register\n",
            "2025-04-10 16:24:50,517 - chat-bot - WARNING - /usr/local/lib/python3.11/dist-packages/funasr/register.py:61 - decorator - Key DynamicConvolution2DTransformerDecoder already exists in decoder_classes, re-register\n",
            "2025-04-10 16:24:50,722 - chat-bot - INFO - /usr/local/lib/python3.11/dist-packages/funasr/register.py:55 - decorator - New registry table added: lid_predictor_classes\n",
            "2025-04-10 16:24:50,815 - chat-bot - WARNING - /usr/local/lib/python3.11/dist-packages/funasr/register.py:61 - decorator - Key SenseVoiceEncoderSmall already exists in encoder_classes, re-register\n",
            "2025-04-10 16:24:50,826 - chat-bot - WARNING - /usr/local/lib/python3.11/dist-packages/funasr/register.py:61 - decorator - Key SenseVoiceSmall already exists in model_classes, re-register\n",
            "2025-04-10 16:24:50,896 - chat-bot - INFO - /usr/local/lib/python3.11/dist-packages/funasr/auto/auto_model.py:177 - build_model - download models from model hub: ms\n",
            "Detect model requirements, begin to install it: /content/models/FunAudioLLM/SenseVoiceSmall/requirements.txt\n",
            "install model requirements successfully\n",
            "Loading remote code failed: model, No module named 'model'\n",
            "2025-04-10 16:25:02,204 - chat-bot - INFO - /usr/local/lib/python3.11/dist-packages/funasr/auto/auto_model.py:269 - build_model - Loading pretrained params from /content/models/FunAudioLLM/SenseVoiceSmall/model.pt\n",
            "2025-04-10 16:25:02,210 - chat-bot - INFO - /usr/local/lib/python3.11/dist-packages/funasr/train_utils/load_pretrained_model.py:36 - load_pretrained_model - ckpt: /content/models/FunAudioLLM/SenseVoiceSmall/model.pt\n",
            "2025-04-10 16:25:03,031 - chat-bot - INFO - /usr/local/lib/python3.11/dist-packages/funasr/train_utils/load_pretrained_model.py:52 - load_pretrained_model - scope_map: ['module.', 'None']\n",
            "2025-04-10 16:25:03,031 - chat-bot - INFO - /usr/local/lib/python3.11/dist-packages/funasr/train_utils/load_pretrained_model.py:58 - load_pretrained_model - excludes: None\n",
            "2025-04-10 16:25:03,109 - chat-bot - INFO - /usr/local/lib/python3.11/dist-packages/funasr/train_utils/load_pretrained_model.py:103 - load_pretrained_model - Loading ckpt: /content/models/FunAudioLLM/SenseVoiceSmall/model.pt, status: <All keys matched successfully>\n",
            "2025-04-10 16:25:03,577 - chat-bot - INFO - /content/achatbot/src/common/factory.py:69 - get_engine_by_tag - use tts_edge engine\n",
            "2025-04-10 16:25:03,577 - chat-bot - INFO - /content/achatbot/src/common/factory.py:34 - get_instance - class: <class 'src.modules.speech.tts.edge_tts.EdgeTTS'> args: {'language': 'zh', 'gender': 'Male', 'voice_name': 'zh-CN-YunjianNeural', 'rate': '+15%', 'volume': '+0%', 'pitch': '+0Hz'}\n",
            "2025-04-10 16:25:03,577 - chat-bot - INFO - /content/achatbot/src/transports/livekit.py:43 - __init__ - LivekitTransport register event names: dict_keys(['on_connected', 'on_error', 'on_connection_state_changed', 'on_disconnected', 'on_participant_connected', 'on_participant_disconnected', 'on_audio_track_subscribed', 'on_audio_track_unsubscribed', 'on_video_track_subscribed', 'on_video_track_unsubscribed', 'on_data_received', 'on_first_participant_joined'])\n",
            "2025-04-10 16:25:04,518 - qwen_vl_utils.vision_process - INFO - /usr/local/lib/python3.11/dist-packages/qwen_vl_utils/vision_process.py:41 - <module> - set VIDEO_TOTAL_PIXELS: 90316800\n",
            "2025-04-10 16:25:06.428040: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-10 16:25:06.926874: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744302307.158137    8521 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744302307.217870    8521 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-10 16:25:07.707931: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-04-10 16:25:12,526 - chat-bot - INFO - /content/achatbot/src/common/factory.py:69 - get_engine_by_tag - use llm_transformers_manual_vision_qwen engine\n",
            "2025-04-10 16:25:12,526 - chat-bot - INFO - /content/achatbot/src/common/factory.py:34 - get_instance - class: <class 'src.core.llm.transformers.manual_vision_qwen.TransformersManualVisionQwenLLM'> args: {'lm_device': 'cuda', 'lm_model_name_or_path': '/content/models/Qwen/Qwen2-VL-7B-Instruct', 'chat_history_size': 0, 'init_chat_prompt': '请用中文交流', 'model_type': 'chat_completion'}\n",
            "Loading checkpoint shards: 100% 5/5 [00:02<00:00,  1.95it/s]\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "2025-04-10 16:25:21,041 - chat-bot - INFO - /content/achatbot/src/core/llm/transformers/base.py:139 - _warmup - Warming up TransformersManualVisionQwenLLM device: cuda:0\n",
            "2025-04-10 16:25:22,743 - chat-bot - INFO - /content/achatbot/src/core/llm/transformers/base.py:156 - _warmup - step 0 warnup TTFT time: 1.340796260999923 s\n",
            "2025-04-10 16:25:22,995 - chat-bot - INFO - /content/achatbot/src/core/llm/transformers/base.py:156 - _warmup - step 1 warnup TTFT time: 0.042578979999916555 s\n",
            "2025-04-10 16:25:22,995 - chat-bot - INFO - /content/achatbot/src/core/llm/transformers/base.py:161 - _warmup - TransformersManualVisionQwenLLM:  warmed up! time: 1.953 s\n",
            "2025-04-10 16:25:22,995 - chat-bot - INFO - /content/achatbot/src/core/llm/__init__.py:71 - initLLMEngine - initLLMEngine: llm_transformers_manual_vision_qwen, TAG:llm_transformers_manual_vision_qwen | TransformersManualVisionQwenLLM\n",
            "2025-04-10 16:25:22,996 - chat-bot - INFO - /content/achatbot/src/services/livekit_client.py:157 - _join - LivekitDescribeVisionBot Connecting to chat-bot, current remote participants:[]\n",
            "2025-04-10 16:25:22,998 - livekit - INFO - /usr/local/lib/python3.11/dist-packages/livekit/rtc/_ffi_client.py:164 - ffi_event_callback - livekit_ffi::server:133:livekit_ffi::server - initializing ffi server v0.12.1\n",
            "2025-04-10 16:25:22,999 - chat-bot - WARNING - /content/achatbot/src/services/livekit_client.py:194 - join - LivekitDescribeVisionBot has connected chat-bot\n",
            "2025-04-10 16:25:22,999 - livekit - INFO - /usr/local/lib/python3.11/dist-packages/livekit/rtc/_ffi_client.py:164 - ffi_event_callback - livekit_ffi::cabi:36:livekit_ffi::cabi - initializing ffi server v0.12.1\n",
            "2025-04-10 16:25:23,003 - livekit - INFO - /usr/local/lib/python3.11/dist-packages/livekit/rtc/_ffi_client.py:164 - ffi_event_callback - livekit_api::signal_client::signal_stream:96:livekit_api::signal_client::signal_stream - connecting to wss://chat-bot-3siyyeda.livekit.cloud/rtc?sdk=python&protocol=15&auto_subscribe=1&adaptive_stream=0&version=0.17.6&access_token=...\n",
            "2025-04-10 16:25:23,183 - chat-bot - INFO - /content/achatbot/src/services/livekit_client.py:182 - _join - local_participant:rtc.LocalParticipant(sid=PA_wBpwQxZsgQF8, identity=3d6a0b2c-e77f-4984-bc05-8e7e0b645346, name=LivekitDescribeVisionBot) joined room: chat-bot connection_state: 1\n",
            "2025-04-10 16:25:23,188 - chat-bot - INFO - /content/achatbot/src/services/livekit_client.py:216 - join - audio track local publication rtc.LocalTrackPublication(sid=TR_AMaEAaLYrAokgt, name=achatbot-out-audio, kind=1, source=2)\n",
            "2025-04-10 16:25:23,188 - chat-bot - INFO - /content/achatbot/src/services/livekit_client.py:246 - join - u can access sandbox url: https://ultra-terminal-re8nmd.sandbox.livekit.io/rooms/chat-bot\n",
            "2025-04-10 16:25:23,188 - chat-bot - INFO - /content/achatbot/src/processors/livekit_input_transport_processor.py:78 - _audio_in_task_handler - Start sub room in audio stream task\n",
            "2025-04-10 16:25:45,458 - chat-bot - INFO - /content/achatbot/src/services/livekit_client.py:390 - _async_on_participant_connected - Participant:rtc.RemoteParticipant(sid=PA_6ycxGrCFq5H8, identity=weedge, name=) connected\n",
            "2025-04-10 16:25:45,458 - chat-bot - WARNING - /content/achatbot/src/services/livekit_client.py:700 - capture_participant_video - participant_id PA_6ycxGrCFq5H8 no video track\n",
            "2025-04-10 16:25:45,459 - chat-bot - INFO - /content/achatbot/src/processors/speech/tts/tts_processor.py:51 - run_tts - Generating TTS: [你好,weedge。欢迎使用 Vision Bot. 我是一名虚拟助手，可以结合视频进行提问。]\n",
            "2025-04-10 16:25:45,748 - chat-bot - INFO - /content/achatbot/src/modules/speech/tts/edge_tts.py:46 - _inference - tts_edge voice: zh-CN-YunjianNeural\n",
            "2025-04-10 16:25:46,399 - chat-bot - INFO - /content/achatbot/src/services/livekit_client.py:408 - _async_on_track_subscribed - track subscribed: rtc.RemoteAudioTrack(sid=TR_AM6UhVdcWFP2kf, name=) from participant rtc.RemoteParticipant(sid=PA_6ycxGrCFq5H8, identity=weedge, name=)\n",
            "2025-04-10 16:25:46,400 - chat-bot - INFO - /content/achatbot/src/services/livekit_client.py:431 - _process_audio_stream - Started processing audio stream for participant PA_6ycxGrCFq5H8\n",
            "2025-04-10 16:25:47,087 - chat-bot - INFO - /content/achatbot/src/services/livekit_client.py:408 - _async_on_track_subscribed - track subscribed: rtc.RemoteVideoTrack(sid=TR_VCn9ebF2pRefu8, name=) from participant rtc.RemoteParticipant(sid=PA_6ycxGrCFq5H8, identity=weedge, name=)\n",
            "2025-04-10 16:25:47,088 - chat-bot - INFO - /content/achatbot/src/services/livekit_client.py:740 - _async_on_participant_video_frame - Started capture participant_id:PA_6ycxGrCFq5H8 from video stream <livekit.rtc.video_stream.VideoStream object at 0x7c90d1177390>\n",
            "2025-04-10 16:25:59,759 - chat-bot - INFO - /content/achatbot/src/processors/speech/asr/asr_processor.py:69 - run_asr - sense_voice_asr Transcription: [y]\n",
            "2025-04-10 16:26:03,818 - chat-bot - INFO - /content/achatbot/src/processors/speech/asr/asr_processor.py:69 - run_asr - sense_voice_asr Transcription: [描述你看到的内容]\n",
            "2025-04-10 16:26:06,292 - chat-bot - INFO - /content/achatbot/src/processors/speech/tts/tts_processor.py:51 - run_tts - Generating TTS: [这张图片显示了一个室内环境，天花板上有一些照明设备，包括几个灯泡和一个吊扇。]\n",
            "2025-04-10 16:26:07,323 - chat-bot - INFO - /content/achatbot/src/processors/speech/tts/tts_processor.py:51 - run_tts - Generating TTS: [墙壁是浅色的，背景中可以看到一些家具和装饰品。]\n",
            "2025-04-10 16:26:09,551 - chat-bot - INFO - /content/achatbot/src/processors/speech/tts/tts_processor.py:51 - run_tts - Generating TTS: [整体光线较暗，可能是在夜晚或光线较弱的环境中拍摄的。]\n",
            "Exception ignored in atexit callback: <function _exit_function at 0x7c9586b5d120>\n",
            "Exception ignored in sys.unraisablehook: <built-in function unraisablehook>\n",
            "KeyboardInterrupt\n",
            "2025-04-10 16:26:33,280 - chat-bot - WARNING - /usr/local/lib/python3.11/dist-packages/apipeline/pipeline/runner.py:53 - _sig_handler - Interruption detected. Canceling runner PipelineRunner#0\n",
            "2025-04-10 16:26:33,280 - chat-bot - WARNING - /usr/local/lib/python3.11/dist-packages/apipeline/pipeline/runner.py:53 - _sig_handler - Interruption detected. Canceling runner PipelineRunner#0\n",
            "2025-04-10 16:26:33,281 - chat-bot - INFO - /content/achatbot/src/services/livekit_client.py:278 - leave - LivekitDescribeVisionBot Disconnecting from chat-bot\n",
            "2025-04-10 16:26:33,281 - chat-bot - WARNING - /content/achatbot/src/services/livekit_client.py:272 - leave - LivekitDescribeVisionBot unconnect chat-bot, don't to leave\n",
            "2025-04-10 16:26:34,281 - chat-bot - INFO - /content/achatbot/src/services/livekit_client.py:463 - _async_on_disconnected - Disconnected from chat-bot. Reason: Leave Room.\n",
            "2025-04-10 16:26:38,285 - chat-bot - ERROR - /content/achatbot/src/common/task_manager/multiprocessing_task_manager.py:34 - cleanup - Error while cleaning up process 8521: Cannot close a process while it is still running. You should first call join() or terminate().\n",
            "2025-04-10 16:26:38,286 - chat-bot - WARNING - /content/achatbot/src/common/task_manager/multiprocessing_task_manager.py:37 - cleanup - pid:8521 tag:chat-bot proc: <Process name='LivekitDescribeVisionBot' pid=8521 parent=8488 started> killed\n"
          ]
        }
      ],
      "source": [
        "!LIVEKIT_URL=$LIVEKIT_URL LIVEKIT_API_KEY=$LIVEKIT_API_KEY LIVEKIT_API_SECRET=$LIVEKIT_API_SECRET \\\n",
        "  python -m src.cmd.bots.main -f /content/livekit_describe_transformers_vision_bot.json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kimi-VL-A3B-Instruct"
      ],
      "metadata": {
        "id": "meEc3PYHh8vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/livekit_describe_transformers_vision_bot.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26428c1c-bc45-4d78-e346-59e3c256b3db",
        "id": "ZrEr41SKh9-3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"chat_bot_name\": \"LivekitDescribeVisionBot\",\n",
            "  \"room_name\": \"chat-bot\",\n",
            "  \"room_url\": \"\",\n",
            "  \"token\": \"\",\n",
            "  \"room_manager\": {\n",
            "    \"tag\": \"livekit_room\",\n",
            "    \"args\": {\n",
            "      \"bot_name\": \"LivekitDescribeVisionBot\",\n",
            "      \"is_common_session\": false\n",
            "    }\n",
            "  },\n",
            "  \"services\": {\n",
            "    \"pipeline\": \"achatbot\",\n",
            "    \"vad\": \"silero\",\n",
            "    \"asr\": \"sense_voice\",\n",
            "    \"llm\": \"transformers_manual_vision_kimi\",\n",
            "    \"tts\": \"edge\"\n",
            "  },\n",
            "  \"config\": {\n",
            "    \"vad\": {\n",
            "      \"tag\": \"silero_vad_analyzer\",\n",
            "      \"args\": { \"stop_secs\": 0.7 }\n",
            "    },\n",
            "    \"asr\": {\n",
            "      \"tag\": \"sense_voice_asr\",\n",
            "      \"args\": {\n",
            "        \"language\": \"zn\",\n",
            "        \"model_name_or_path\": \"/content/models/FunAudioLLM/SenseVoiceSmall\"\n",
            "      }\n",
            "    },\n",
            "    \"llm\": {\n",
            "      \"tag\": \"llm_transformers_manual_vision_kimi\",\n",
            "      \"args\": {\n",
            "        \"lm_device\": \"cuda\",\n",
            "        \"lm_model_name_or_path\": \"/content/models/moonshotai/Kimi-VL-A3B-Instruct\",\n",
            "        \"chat_history_size\": 0,\n",
            "        \"init_chat_prompt\": \"请用中文交流\",\n",
            "        \"model_type\": \"chat_completion\"\n",
            "      },\n",
            "      \"language\": \"zh\"\n",
            "    },\n",
            "    \"tts\": {\n",
            "      \"tag\": \"tts_edge\",\n",
            "      \"args\": {\n",
            "        \"voice_name\": \"zh-CN-YunjianNeural\",\n",
            "        \"language\": \"zh\",\n",
            "        \"gender\": \"Male\"\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"config_list\": []\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9f05f251-fc81-423f-9143-4e0ac806108f",
        "id": "5cv08LDJh9-3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"chat_bot_name\": \"LivekitDescribeVisionBot\",\n",
            "    \"config\": {\n",
            "        \"asr\": {\n",
            "            \"args\": {\n",
            "                \"language\": \"zn\",\n",
            "                \"model_name_or_path\": \"/content/models/FunAudioLLM/SenseVoiceSmall\"\n",
            "            },\n",
            "            \"tag\": \"sense_voice_asr\"\n",
            "        },\n",
            "        \"llm\": {\n",
            "            \"args\": {\n",
            "                \"chat_history_size\": 0,\n",
            "                \"init_chat_prompt\": \"\\u8bf7\\u7528\\u4e2d\\u6587\\u4ea4\\u6d41\",\n",
            "                \"lm_device\": \"cuda\",\n",
            "                \"lm_model_name_or_path\": \"/content/models/moonshotai/Kimi-VL-A3B-Instruct\",\n",
            "                \"model_type\": \"chat_completion\"\n",
            "            },\n",
            "            \"language\": \"zh\",\n",
            "            \"tag\": \"llm_transformers_manual_vision_kimi\"\n",
            "        },\n",
            "        \"tts\": {\n",
            "            \"args\": {\n",
            "                \"gender\": \"Male\",\n",
            "                \"language\": \"zh\",\n",
            "                \"voice_name\": \"zh-CN-YunjianNeural\"\n",
            "            },\n",
            "            \"tag\": \"tts_edge\"\n",
            "        },\n",
            "        \"vad\": {\n",
            "            \"args\": {\n",
            "                \"stop_secs\": 0.7\n",
            "            },\n",
            "            \"tag\": \"silero_vad_analyzer\"\n",
            "        }\n",
            "    },\n",
            "    \"config_list\": [],\n",
            "    \"room_manager\": {\n",
            "        \"args\": {\n",
            "            \"bot_name\": \"LivekitDescribeVisionBot\",\n",
            "            \"is_common_session\": false\n",
            "        },\n",
            "        \"tag\": \"livekit_room\"\n",
            "    },\n",
            "    \"room_name\": \"chat-bot\",\n",
            "    \"room_url\": \"\",\n",
            "    \"services\": {\n",
            "        \"asr\": \"sense_voice\",\n",
            "        \"llm\": \"transformers_manual_vision_kimi\",\n",
            "        \"pipeline\": \"achatbot\",\n",
            "        \"tts\": \"edge\",\n",
            "        \"vad\": \"silero\"\n",
            "    },\n",
            "    \"token\": \"\"\n",
            "}\n",
            "2025-04-10 16:30:52,719 - chat-bot - INFO - /content/achatbot/src/cmd/bots/main.py:63 - <module> - bot_config:{'chat_bot_name': 'LivekitDescribeVisionBot', 'room_name': 'chat-bot', 'room_url': '', 'token': '', 'room_manager': {'tag': 'livekit_room', 'args': {'bot_name': 'LivekitDescribeVisionBot', 'is_common_session': False}}, 'services': {'pipeline': 'achatbot', 'vad': 'silero', 'asr': 'sense_voice', 'llm': 'transformers_manual_vision_kimi', 'tts': 'edge'}, 'config': {'vad': {'tag': 'silero_vad_analyzer', 'args': {'stop_secs': 0.7}}, 'asr': {'tag': 'sense_voice_asr', 'args': {'language': 'zn', 'model_name_or_path': '/content/models/FunAudioLLM/SenseVoiceSmall'}}, 'llm': {'tag': 'llm_transformers_manual_vision_kimi', 'args': {'lm_device': 'cuda', 'lm_model_name_or_path': '/content/models/moonshotai/Kimi-VL-A3B-Instruct', 'chat_history_size': 0, 'init_chat_prompt': '请用中文交流', 'model_type': 'chat_completion'}, 'language': 'zh'}, 'tts': {'tag': 'tts_edge', 'args': {'voice_name': 'zh-CN-YunjianNeural', 'language': 'zh', 'gender': 'Male'}}}, 'config_list': []}\n",
            "2025-04-10 16:30:52,719 - chat-bot - INFO - /content/achatbot/src/cmd/bots/run.py:33 - __init__ - run_bot_info: is_agent=False chat_bot_name='LivekitDescribeVisionBot' config={'vad': {'tag': 'silero_vad_analyzer', 'args': {'stop_secs': 0.7}}, 'asr': {'tag': 'sense_voice_asr', 'args': {'language': 'zn', 'model_name_or_path': '/content/models/FunAudioLLM/SenseVoiceSmall'}}, 'llm': {'tag': 'llm_transformers_manual_vision_kimi', 'args': {'lm_device': 'cuda', 'lm_model_name_or_path': '/content/models/moonshotai/Kimi-VL-A3B-Instruct', 'chat_history_size': 0, 'init_chat_prompt': '请用中文交流', 'model_type': 'chat_completion'}, 'language': 'zh'}, 'tts': {'tag': 'tts_edge', 'args': {'voice_name': 'zh-CN-YunjianNeural', 'language': 'zh', 'gender': 'Male'}}} room_name='chat-bot' room_url='' token='' config_list=[] services={'pipeline': 'achatbot', 'vad': 'silero', 'asr': 'sense_voice', 'llm': 'transformers_manual_vision_kimi', 'tts': 'edge'} websocket_server_host='localhost' websocket_server_port=8765 transport_type='room' handle_sigint=True task_connector=None room_manager=EngineClassInfo(tag='livekit_room', args={'bot_name': 'LivekitDescribeVisionBot', 'is_common_session': False})\n",
            "2025-04-10 16:30:52,924 - chat-bot - INFO - /content/achatbot/src/common/factory.py:69 - get_engine_by_tag - use livekit_room engine\n",
            "2025-04-10 16:30:52,924 - chat-bot - INFO - /content/achatbot/src/common/factory.py:34 - get_instance - class: <class 'src.services.help.livekit_room.LivekitRoom'> args: {'bot_name': 'LivekitDescribeVisionBot', 'is_common_session': False}\n",
            "2025-04-10 16:30:52,924 - chat-bot - INFO - /content/achatbot/src/services/help/__init__.py:37 - initEngine - initEngine: livekit_room, TAG:livekit_room | LivekitRoom\n",
            "2025-04-10 16:30:57,189 - chat-bot - INFO - /content/achatbot/src/cmd/bots/base.py:77 - init_bot_config - ai bot_config: vad=VADConfig(tag='silero_vad_analyzer', args={'stop_secs': 0.7}) asr=ASRConfig(tag='sense_voice_asr', args={'language': 'zn', 'model_name_or_path': '/content/models/FunAudioLLM/SenseVoiceSmall'}) llm=LLMConfig(base_url=None, model=None, language='zh', messages=None, tools=None, tag='llm_transformers_manual_vision_kimi', args={'lm_device': 'cuda', 'lm_model_name_or_path': '/content/models/moonshotai/Kimi-VL-A3B-Instruct', 'chat_history_size': 0, 'init_chat_prompt': '请用中文交流', 'model_type': 'chat_completion'}) nlp_task_llm=None voice_llm=None vision_llm=None omni_llm=None vision_detector=None vision_ocr=None tts=TTSConfig(voice=None, language=None, tag='tts_edge', args={'voice_name': 'zh-CN-YunjianNeural', 'language': 'zh', 'gender': 'Male'}) img_gen=None extends=None\n",
            "2025-04-10 16:30:57,202 - chat-bot - INFO - /content/achatbot/src/common/factory.py:69 - get_engine_by_tag - use silero_vad_analyzer engine\n",
            "2025-04-10 16:30:57,203 - chat-bot - INFO - /content/achatbot/src/common/factory.py:34 - get_instance - class: <class 'src.modules.speech.vad_analyzer.silero.SileroVADAnalyzer'> args: {'stop_secs': 0.7}\n",
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n",
            "2025-04-10 16:30:57,837 - chat-bot - INFO - /content/achatbot/src/common/factory.py:69 - get_engine_by_tag - use sense_voice_asr engine\n",
            "2025-04-10 16:30:57,838 - chat-bot - INFO - /content/achatbot/src/common/factory.py:34 - get_instance - class: <class 'src.modules.speech.asr.sense_voice_asr.SenseVoiceAsr'> args: {'download_path': '', 'model_name_or_path': '/content/models/FunAudioLLM/SenseVoiceSmall', 'language': 'zn', 'verbose': True, 'prompt': '', 'sample_rate': 16000, 'device': None}\n",
            "2025-04-10 16:30:58,518 - numexpr.utils - INFO - /usr/local/lib/python3.11/dist-packages/numexpr/utils.py:162 - _init_num_threads - NumExpr defaulting to 12 threads.\n",
            "2025-04-10 16:31:01,247 - chat-bot - INFO - /usr/local/lib/python3.11/dist-packages/funasr/register.py:55 - decorator - New registry table added: preprocessor_classes\n",
            "2025-04-10 16:31:01,574 - chat-bot - WARNING - /usr/local/lib/python3.11/dist-packages/funasr/register.py:61 - decorator - Key Conformer already exists in model_classes, re-register\n",
            "2025-04-10 16:31:02,016 - chat-bot - INFO - /usr/local/lib/python3.11/dist-packages/funasr/register.py:55 - decorator - New registry table added: adaptor_classes\n",
            "2025-04-10 16:31:02,312 - chat-bot - WARNING - /usr/local/lib/python3.11/dist-packages/funasr/register.py:61 - decorator - Key Linear already exists in adaptor_classes, re-register\n",
            "2025-04-10 16:31:02,389 - chat-bot - WARNING - /usr/local/lib/python3.11/dist-packages/funasr/register.py:61 - decorator - Key TransformerDecoder already exists in decoder_classes, re-register\n",
            "2025-04-10 16:31:02,402 - chat-bot - WARNING - /usr/local/lib/python3.11/dist-packages/funasr/register.py:61 - decorator - Key LightweightConvolutionTransformerDecoder already exists in decoder_classes, re-register\n",
            "2025-04-10 16:31:02,409 - chat-bot - WARNING - /usr/local/lib/python3.11/dist-packages/funasr/register.py:61 - decorator - Key LightweightConvolution2DTransformerDecoder already exists in decoder_classes, re-register\n",
            "2025-04-10 16:31:02,416 - chat-bot - WARNING - /usr/local/lib/python3.11/dist-packages/funasr/register.py:61 - decorator - Key DynamicConvolutionTransformerDecoder already exists in decoder_classes, re-register\n",
            "2025-04-10 16:31:02,423 - chat-bot - WARNING - /usr/local/lib/python3.11/dist-packages/funasr/register.py:61 - decorator - Key DynamicConvolution2DTransformerDecoder already exists in decoder_classes, re-register\n",
            "2025-04-10 16:31:02,627 - chat-bot - INFO - /usr/local/lib/python3.11/dist-packages/funasr/register.py:55 - decorator - New registry table added: lid_predictor_classes\n",
            "2025-04-10 16:31:02,695 - chat-bot - WARNING - /usr/local/lib/python3.11/dist-packages/funasr/register.py:61 - decorator - Key SenseVoiceEncoderSmall already exists in encoder_classes, re-register\n",
            "2025-04-10 16:31:02,706 - chat-bot - WARNING - /usr/local/lib/python3.11/dist-packages/funasr/register.py:61 - decorator - Key SenseVoiceSmall already exists in model_classes, re-register\n",
            "2025-04-10 16:31:02,755 - chat-bot - INFO - /usr/local/lib/python3.11/dist-packages/funasr/auto/auto_model.py:177 - build_model - download models from model hub: ms\n",
            "Detect model requirements, begin to install it: /content/models/FunAudioLLM/SenseVoiceSmall/requirements.txt\n",
            "install model requirements successfully\n",
            "Loading remote code failed: model, No module named 'model'\n",
            "2025-04-10 16:31:07,734 - chat-bot - INFO - /usr/local/lib/python3.11/dist-packages/funasr/auto/auto_model.py:269 - build_model - Loading pretrained params from /content/models/FunAudioLLM/SenseVoiceSmall/model.pt\n",
            "2025-04-10 16:31:07,740 - chat-bot - INFO - /usr/local/lib/python3.11/dist-packages/funasr/train_utils/load_pretrained_model.py:36 - load_pretrained_model - ckpt: /content/models/FunAudioLLM/SenseVoiceSmall/model.pt\n",
            "2025-04-10 16:31:08,553 - chat-bot - INFO - /usr/local/lib/python3.11/dist-packages/funasr/train_utils/load_pretrained_model.py:52 - load_pretrained_model - scope_map: ['module.', 'None']\n",
            "2025-04-10 16:31:08,554 - chat-bot - INFO - /usr/local/lib/python3.11/dist-packages/funasr/train_utils/load_pretrained_model.py:58 - load_pretrained_model - excludes: None\n",
            "2025-04-10 16:31:08,630 - chat-bot - INFO - /usr/local/lib/python3.11/dist-packages/funasr/train_utils/load_pretrained_model.py:103 - load_pretrained_model - Loading ckpt: /content/models/FunAudioLLM/SenseVoiceSmall/model.pt, status: <All keys matched successfully>\n",
            "2025-04-10 16:31:09,212 - chat-bot - INFO - /content/achatbot/src/common/factory.py:69 - get_engine_by_tag - use tts_edge engine\n",
            "2025-04-10 16:31:09,212 - chat-bot - INFO - /content/achatbot/src/common/factory.py:34 - get_instance - class: <class 'src.modules.speech.tts.edge_tts.EdgeTTS'> args: {'language': 'zh', 'gender': 'Male', 'voice_name': 'zh-CN-YunjianNeural', 'rate': '+15%', 'volume': '+0%', 'pitch': '+0Hz'}\n",
            "2025-04-10 16:31:09,212 - chat-bot - INFO - /content/achatbot/src/transports/livekit.py:43 - __init__ - LivekitTransport register event names: dict_keys(['on_connected', 'on_error', 'on_connection_state_changed', 'on_disconnected', 'on_participant_connected', 'on_participant_disconnected', 'on_audio_track_subscribed', 'on_audio_track_unsubscribed', 'on_video_track_subscribed', 'on_video_track_unsubscribed', 'on_data_received', 'on_first_participant_joined'])\n",
            "2025-04-10 16:31:10.491253: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-10 16:31:10.508395: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744302670.528058   10609 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744302670.533551   10609 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-10 16:31:10.552569: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-04-10 16:31:13,540 - qwen_vl_utils.vision_process - INFO - /usr/local/lib/python3.11/dist-packages/qwen_vl_utils/vision_process.py:41 - <module> - set VIDEO_TOTAL_PIXELS: 90316800\n",
            "2025-04-10 16:31:13,540 - chat-bot - INFO - /content/achatbot/src/common/factory.py:69 - get_engine_by_tag - use llm_transformers_manual_vision_kimi engine\n",
            "2025-04-10 16:31:13,541 - chat-bot - INFO - /content/achatbot/src/common/factory.py:34 - get_instance - class: <class 'src.core.llm.transformers.manual_vision_kimi.TransformersManualVisionKimi'> args: {'lm_device': 'cuda', 'lm_model_name_or_path': '/content/models/moonshotai/Kimi-VL-A3B-Instruct', 'chat_history_size': 0, 'init_chat_prompt': '请用中文交流', 'model_type': 'chat_completion'}\n",
            "2025-04-10 16:31:13,541 - chat-bot - INFO - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:104 - __init__ - TransformersLMArgs: TransformersLMArgs(lm_gen_seed=42, lm_max_length=2048, lm_gen_max_new_tokens=1024, lm_gen_min_new_tokens=0, lm_gen_do_sample=True, lm_gen_temperature=0.1, lm_gen_top_k=20, lm_gen_top_p=0.8, lm_gen_min_p=0.0, lm_gen_repetition_penalty=1.0, lm_gen_stops=[], lm_gen_stop_ids=[], lm_gen_end_id=0, lm_gen_pad_id=0, lm_model_name_or_path='/content/models/moonshotai/Kimi-VL-A3B-Instruct', lm_device_map=None, lm_device='cuda', lm_torch_dtype='auto', lm_attn_impl='eager', user_role='user', warnup_prompt=\"Repeat the word 'weedge niu bi'.\", warmup_steps=2, init_chat_role='system', init_chat_prompt='请用中文交流', lm_tokenizer_decode_batch_size=60, chat_history_size=0, lm_stream=True, model_type='chat_completion', lm_bnb_quant_type='int4')\n",
            "Loading checkpoint shards: 100% 7/7 [02:37<00:00, 22.53s/it]\n",
            "2025-04-10 16:34:23,203 - chat-bot - INFO - /content/achatbot/src/core/llm/transformers/manual_vision_kimi.py:112 - __init__ - TransformersLMArgs: TransformersLMArgs(lm_gen_seed=42, lm_max_length=2048, lm_gen_max_new_tokens=1024, lm_gen_min_new_tokens=0, lm_gen_do_sample=True, lm_gen_temperature=0.1, lm_gen_top_k=20, lm_gen_top_p=0.8, lm_gen_min_p=0.0, lm_gen_repetition_penalty=1.0, lm_gen_stops=[], lm_gen_stop_ids=[], lm_gen_end_id=0, lm_gen_pad_id=0, lm_model_name_or_path='/content/models/moonshotai/Kimi-VL-A3B-Instruct', lm_device_map=None, lm_device='cuda', lm_torch_dtype='auto', lm_attn_impl='eager', user_role='user', warnup_prompt=\"Repeat the word 'weedge niu bi'.\", warmup_steps=2, init_chat_role='system', init_chat_prompt='请用中文交流', lm_tokenizer_decode_batch_size=60, chat_history_size=0, lm_stream=True, model_type='chat_completion', lm_bnb_quant_type='int4') model.device: cuda:0\n",
            "2025-04-10 16:34:23,224 - chat-bot - INFO - /content/achatbot/src/common/utils/helper.py:56 - print_model_params - llm_transformers_manual_vision_kimi 16407.656048 M parameters\n",
            "2025-04-10 16:34:23,791 - transformers_modules.Kimi-VL-A3B-Instruct.tokenization_moonshot - INFO - /root/.cache/huggingface/modules/transformers_modules/Kimi-VL-A3B-Instruct/tokenization_moonshot.py:118 - __init__ - Reloaded tiktoken model from /content/models/moonshotai/Kimi-VL-A3B-Instruct/tiktoken.model\n",
            "2025-04-10 16:34:23,791 - transformers_modules.Kimi-VL-A3B-Instruct.tokenization_moonshot - INFO - /root/.cache/huggingface/modules/transformers_modules/Kimi-VL-A3B-Instruct/tokenization_moonshot.py:124 - __init__ - #words: 163842 - BOS ID: 163584 - EOS ID: 163585\n",
            "Keyword argument `videos` is not a valid argument for this processor and will be ignored.\n",
            "2025-04-10 16:34:25,316 - chat-bot - INFO - /content/achatbot/src/core/llm/transformers/base.py:139 - _warmup - Warming up TransformersManualVisionKimi device: cuda:0\n",
            "2025-04-10 16:34:39,284 - chat-bot - INFO - /content/achatbot/src/core/llm/transformers/base.py:156 - _warmup - step 0 warnup TTFT time: 2.613953584999763 s\n",
            "2025-04-10 16:34:40,902 - chat-bot - INFO - /content/achatbot/src/core/llm/transformers/base.py:156 - _warmup - step 1 warnup TTFT time: 0.2749181479998697 s\n",
            "2025-04-10 16:34:40,902 - chat-bot - INFO - /content/achatbot/src/core/llm/transformers/base.py:161 - _warmup - TransformersManualVisionKimi:  warmed up! time: 15.585 s\n",
            "2025-04-10 16:34:40,902 - chat-bot - INFO - /content/achatbot/src/core/llm/__init__.py:71 - initLLMEngine - initLLMEngine: llm_transformers_manual_vision_kimi, TAG:llm_transformers_manual_vision_kimi | TransformersManualVisionKimi\n",
            "2025-04-10 16:34:40,903 - chat-bot - INFO - /content/achatbot/src/services/livekit_client.py:157 - _join - LivekitDescribeVisionBot Connecting to chat-bot, current remote participants:[]\n",
            "2025-04-10 16:34:40,956 - livekit - INFO - /usr/local/lib/python3.11/dist-packages/livekit/rtc/_ffi_client.py:164 - ffi_event_callback - livekit_ffi::server:133:livekit_ffi::server - initializing ffi server v0.12.1\n",
            "2025-04-10 16:34:40,956 - livekit - INFO - /usr/local/lib/python3.11/dist-packages/livekit/rtc/_ffi_client.py:164 - ffi_event_callback - livekit_ffi::cabi:36:livekit_ffi::cabi - initializing ffi server v0.12.1\n",
            "2025-04-10 16:34:40,958 - chat-bot - WARNING - /content/achatbot/src/services/livekit_client.py:194 - join - LivekitDescribeVisionBot has connected chat-bot\n",
            "2025-04-10 16:34:41,037 - livekit - INFO - /usr/local/lib/python3.11/dist-packages/livekit/rtc/_ffi_client.py:164 - ffi_event_callback - livekit_api::signal_client::signal_stream:96:livekit_api::signal_client::signal_stream - connecting to wss://chat-bot-3siyyeda.livekit.cloud/rtc?sdk=python&protocol=15&auto_subscribe=1&adaptive_stream=0&version=0.17.6&access_token=...\n",
            "2025-04-10 16:34:41,215 - chat-bot - INFO - /content/achatbot/src/services/livekit_client.py:182 - _join - local_participant:rtc.LocalParticipant(sid=PA_SyvLbnrLLkgP, identity=f3415408-3987-4b62-9548-17a57c8b165b, name=LivekitDescribeVisionBot) joined room: chat-bot connection_state: 1\n",
            "2025-04-10 16:34:41,220 - chat-bot - INFO - /content/achatbot/src/services/livekit_client.py:216 - join - audio track local publication rtc.LocalTrackPublication(sid=TR_AMr5Bw5FNpWt82, name=achatbot-out-audio, kind=1, source=2)\n",
            "2025-04-10 16:34:41,220 - chat-bot - INFO - /content/achatbot/src/services/livekit_client.py:246 - join - u can access sandbox url: https://ultra-terminal-re8nmd.sandbox.livekit.io/rooms/chat-bot\n",
            "2025-04-10 16:34:41,221 - chat-bot - INFO - /content/achatbot/src/processors/livekit_input_transport_processor.py:78 - _audio_in_task_handler - Start sub room in audio stream task\n",
            "2025-04-10 16:35:21,312 - chat-bot - INFO - /content/achatbot/src/services/livekit_client.py:390 - _async_on_participant_connected - Participant:rtc.RemoteParticipant(sid=PA_6BKXqVAHJU6d, identity=weedge, name=) connected\n",
            "2025-04-10 16:35:21,312 - chat-bot - WARNING - /content/achatbot/src/services/livekit_client.py:700 - capture_participant_video - participant_id PA_6BKXqVAHJU6d no video track\n",
            "2025-04-10 16:35:21,312 - chat-bot - INFO - /content/achatbot/src/processors/speech/tts/tts_processor.py:51 - run_tts - Generating TTS: [你好,weedge。欢迎使用 Vision Bot. 我是一名虚拟助手，可以结合视频进行提问。]\n",
            "2025-04-10 16:35:21,634 - chat-bot - INFO - /content/achatbot/src/modules/speech/tts/edge_tts.py:46 - _inference - tts_edge voice: zh-CN-YunjianNeural\n",
            "2025-04-10 16:35:22,825 - chat-bot - INFO - /content/achatbot/src/services/livekit_client.py:408 - _async_on_track_subscribed - track subscribed: rtc.RemoteAudioTrack(sid=TR_AM5Eq3opzWyNBH, name=) from participant rtc.RemoteParticipant(sid=PA_6BKXqVAHJU6d, identity=weedge, name=)\n",
            "2025-04-10 16:35:22,825 - chat-bot - INFO - /content/achatbot/src/services/livekit_client.py:408 - _async_on_track_subscribed - track subscribed: rtc.RemoteVideoTrack(sid=TR_VCXkvjPdiHDk2z, name=) from participant rtc.RemoteParticipant(sid=PA_6BKXqVAHJU6d, identity=weedge, name=)\n",
            "2025-04-10 16:35:22,826 - chat-bot - INFO - /content/achatbot/src/services/livekit_client.py:431 - _process_audio_stream - Started processing audio stream for participant PA_6BKXqVAHJU6d\n",
            "2025-04-10 16:35:22,826 - chat-bot - INFO - /content/achatbot/src/services/livekit_client.py:740 - _async_on_participant_video_frame - Started capture participant_id:PA_6BKXqVAHJU6d from video stream <livekit.rtc.video_stream.VideoStream object at 0x790347bf7990>\n",
            "2025-04-10 16:35:34,235 - chat-bot - INFO - /content/achatbot/src/processors/speech/asr/asr_processor.py:69 - run_asr - sense_voice_asr Transcription: [描述一下你看到的]\n",
            "2025-04-10 16:35:34,800 - chat-bot - INFO - /content/achatbot/src/processors/speech/asr/asr_processor.py:69 - run_asr - sense_voice_asr Transcription: [内容]\n",
            "2025-04-10 16:35:38,311 - chat-bot - INFO - /content/achatbot/src/processors/speech/tts/tts_processor.py:51 - run_tts - Generating TTS: [这是一张低光环境下拍摄的照片，照片中的人物位于画面中央，面部特征较为模糊，背景中可以看到天花板上的灯光和一些家具。]\n",
            "2025-04-10 16:35:40,388 - chat-bot - INFO - /content/achatbot/src/processors/speech/tts/tts_processor.py:51 - run_tts - Generating TTS: [整体氛围较为昏暗，光线集中在人物脸部，使得面部细节难以辨认。]\n",
            "2025-04-10 16:36:09,312 - chat-bot - INFO - /content/achatbot/src/processors/speech/asr/asr_processor.py:69 - run_asr - sense_voice_asr Transcription: [我手里拿的是什么]\n",
            "2025-04-10 16:36:11,961 - chat-bot - INFO - /content/achatbot/src/processors/speech/tts/tts_processor.py:51 - run_tts - Generating TTS: [从图片来看，你手里拿的是一个白色的物体，可能是手机、遥控器或其他电子设备。]\n",
            "2025-04-10 16:36:13,337 - chat-bot - INFO - /content/achatbot/src/processors/speech/tts/tts_processor.py:51 - run_tts - Generating TTS: [由于光线较暗，具体是什么物体不太容易辨认。]\n",
            "2025-04-10 16:36:14,974 - chat-bot - INFO - /content/achatbot/src/processors/speech/tts/tts_processor.py:51 - run_tts - Generating TTS: [如果你能提供更多的背景信息或光线条件，可能会更容易识别。]\n",
            "2025-04-10 16:36:37,398 - chat-bot - INFO - /content/achatbot/src/processors/speech/asr/asr_processor.py:69 - run_asr - sense_voice_asr Transcription: [y]\n",
            "2025-04-10 16:36:46,042 - chat-bot - INFO - /content/achatbot/src/processors/speech/asr/asr_processor.py:69 - run_asr - sense_voice_asr Transcription: [我手里拿的是什么]\n",
            "Exception in thread Thread-8 (generate):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 2326, in generate\n",
            "    result = self._sample(\n",
            "             ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 3286, in _sample\n",
            "    outputs = self(**model_inputs, return_dict=True)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/Kimi-VL-A3B-Instruct/modeling_kimi_vl.py\", line 2530, in forward\n",
            "    image_features: torch.Tensor = self._extract_image_features(\n",
            "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/Kimi-VL-A3B-Instruct/modeling_kimi_vl.py\", line 2471, in _extract_image_features\n",
            "    image_features: list[torch.Tensor] = self.vision_tower(\n",
            "                                         ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/Kimi-VL-A3B-Instruct/modeling_kimi_vl.py\", line 2304, in forward\n",
            "    hidden_states = self.encoder(hidden_states, grid_hw)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/Kimi-VL-A3B-Instruct/modeling_kimi_vl.py\", line 553, in forward\n",
            "    hidden_states = block(\n",
            "                    ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/Kimi-VL-A3B-Instruct/modeling_kimi_vl.py\", line 510, in forward\n",
            "    attn_out = self.attention_qkvpacked(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/Kimi-VL-A3B-Instruct/modeling_kimi_vl.py\", line 488, in attention_qkvpacked\n",
            "    attn_out = attn_func(\n",
            "               ^^^^^^^^^^\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/Kimi-VL-A3B-Instruct/modeling_kimi_vl.py\", line 178, in sdpa_attention\n",
            "    attn_output = F.scaled_dot_product_attention(q, k, v, attention_mask, dropout_p=0.0)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 566.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 370.88 MiB is free. Process 108804 has 39.19 GiB memory in use. Of the allocated memory 32.38 GiB is allocated by PyTorch, and 6.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Exception ignored in sys.unraisablehook: <built-in function unraisablehook>\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-db9e8512b30b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LIVEKIT_URL=$LIVEKIT_URL LIVEKIT_API_KEY=$LIVEKIT_API_KEY LIVEKIT_API_SECRET=$LIVEKIT_API_SECRET    python -m src.cmd.bots.main -f /content/livekit_describe_transformers_vision_bot.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    202\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    280\u001b[0m   \u001b[0moutput_available\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m   \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m   \u001b[0minput_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "!LIVEKIT_URL=$LIVEKIT_URL LIVEKIT_API_KEY=$LIVEKIT_API_KEY LIVEKIT_API_SECRET=$LIVEKIT_API_SECRET \\\n",
        "  python -m src.cmd.bots.main -f /content/livekit_describe_transformers_vision_bot.json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 总结"
      ],
      "metadata": {
        "id": "dmu5oJ50Czp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 128k的输入上下文文本长度，Instruct 很适合多模态文本图片视频聊天, 不过需要占用更多的显存\n",
        "- 生成文本质量评估结果不错\n",
        "- 对于一般的多模态感知与理解、OCR、长视频和长文档、视频感知以及代理用途，推荐使用 Kimi-VL-A3B-Instruct 进行高效推理；\n",
        "- 对于高级文本和多模态推理（例如数学），请考虑使用 Kimi-VL-A3B-Thinking 。"
      ],
      "metadata": {
        "id": "MEMNf6YWC1qU"
      }
    }
  ]
}