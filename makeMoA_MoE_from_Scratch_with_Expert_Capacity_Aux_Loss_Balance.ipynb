{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weedge/doraemon-nb/blob/main/makeMoA_MoE_from_Scratch_with_Expert_Capacity_Aux_Loss_Balance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "8e9b80fc-12cf-41a9-a0de-354f678b412b",
          "showTitle": false,
          "title": ""
        },
        "id": "90vgVgmDkRJQ"
      },
      "source": [
        "#### 从头开始的稀疏专家混合语言模型，灵感来源于（并在很大程度上基于）[Andrej Karpathy的makemore项目](https://github.com/karpathy/makemore) :)\n",
        "\n",
        "这是一个从头开始实现的稀疏专家混合语言模型。这受到了Andrej Karpathy项目'makemore'的启发，并且大部分重用的组件都来自于该实现。就像makemore一样，makeMoE也是一个自回归的字符级语言模型，但是使用了上述稀疏专家的架构。\n",
        "\n",
        "与makemore体系结构相比，有显着的变化\n",
        "\n",
        "- 稀疏专家混合而不是孤立的前馈神经网络。\n",
        "- 使用了top-k门控和嘈杂的top-k门控实现。\n",
        "- 初始化 - 这里使用了Kaiming He初始化，但这个笔记本的重点是可hack性，所以你可以替换为Xavier Glorot等，并进行尝试。\n",
        "\n",
        "与makemore不变的部分\n",
        "\n",
        "- Andrej最初选择的数据集、预处理（tokenizer）和语言建模任务 - 生成类似莎士比亚的文本\n",
        "- 自注意力因果实现\n",
        "- 训练循环\n",
        "- 推断逻辑\n",
        "\n",
        "在此实现中大量引用的论文：\n",
        "\n",
        "- Mixtral of Experts：https://arxiv.org/pdf/2401.04088.pdf\n",
        "- Outrageosly Large Neural Networks: The Sparsely-Gated Mixture-Of-Experts layer：https://arxiv.org/pdf/1701.06538.pdf\n",
        "- Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity: https://arxiv.org/pdf/2101.03961.pdf\n",
        "\n",
        "这个笔记本演示了整个模型架构的直觉以及所有内容是如何相互关联的。\n",
        "\n",
        "\n",
        "请注意，该实现强调易读性和可hack性而不是性能，因此有许多方法可以改进此实现。请尝试并告诉我。\n",
        "\n",
        "如果在colab中运行，训练时加速，选择t4 GPU即可。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2f4a58a8-bd4c-40de-a4a9-95457842db0b",
          "showTitle": false,
          "title": ""
        },
        "id": "hywLNfb0kRJT"
      },
      "source": [
        "![](https://raw.githubusercontent.com/weedge/baby-llm/main/docs/simple-moa-moe.drawio.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5e1a3e38-8717-42ec-9bbc-71d3712c1c68",
          "showTitle": false,
          "title": ""
        },
        "id": "V521QQ_qkRJV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dc3d76f-b244-478a-f80b-4d070d84aff1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7d4f7cf2a110>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#Import the necessary packages and set seed for reproducibility. For this notebook, pytorch is all you need\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 数据集处理"
      ],
      "metadata": {
        "id": "DrIfTvwZmIv2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "faf99ef2-39bb-46fc-b772-05d6d0482bbc",
          "showTitle": false,
          "title": ""
        },
        "id": "-4r_QNRRkRJV"
      },
      "source": [
        "接下来的几个部分，下载数据、预处理数据和自注意力直接来自makemore。我稍微详细说明了自注意力，并添加了一些可视化辅助，以便更好地理解这个过程。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "45143d84-28c7-463d-9fb5-e21122842600",
          "showTitle": false,
          "title": ""
        },
        "id": "2GhDw0yWkRJV",
        "outputId": "6cd7b0e8-6f0c-4542-b893-252d5912bfe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-10 09:50:34--  https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  6.05MB/s    in 0.2s    \n",
            "\n",
            "2024-04-10 09:50:34 (6.05 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Downloading the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "192e830a-762d-4573-9484-70a58deb1fec",
          "showTitle": false,
          "title": ""
        },
        "id": "3sPAL1AKkRJW"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2d7181b7-f5e5-4ab5-bdd8-74c507c798ad",
          "showTitle": false,
          "title": ""
        },
        "id": "wNkF3RYLkRJX",
        "outputId": "fc5e4aa1-df0c-46a9-c016-638934639e10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "68032e07-8625-4750-a340-bc8f4eed2458",
          "showTitle": false,
          "title": ""
        },
        "id": "AHIwr-yxkRJX",
        "outputId": "5103d6f4-c49f-43cd-b400-7a2dbd207750",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b6995ad6-c9ac-4a21-9da0-ebbd3273c991",
          "showTitle": false,
          "title": ""
        },
        "id": "DHGayz7mkRJY",
        "outputId": "c4f174f7-a637-4ff5-b8f4-e2658ffa6354",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ],
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "43002fa3-ffd3-416c-9aaf-0a03b19c7bc1",
          "showTitle": false,
          "title": ""
        },
        "id": "pzn11WJckRJY",
        "outputId": "140693f2-ce17-4920-eac5-f669eb5ea3b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ],
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b4609fc4-09c7-4a39-8367-e9ee39d440ed",
          "showTitle": false,
          "title": ""
        },
        "id": "YbBGz0O2kRJY",
        "outputId": "899842fb-9a22-4d1b-98ca-a5f51b55fb99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ],
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "88f7cb0d-02ff-42b0-92a5-a505dc3f8f25",
          "showTitle": false,
          "title": ""
        },
        "id": "hoLIeA7YkRJZ"
      },
      "outputs": [],
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6b554ddf-50f4-441b-8acf-10b81a508b7e",
          "showTitle": false,
          "title": ""
        },
        "id": "VY55nr6EkRJZ",
        "outputId": "5ed689c3-68c8-4c8a-89bf-95e1448c8da6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "22ba4512-309d-4895-a908-2ef3efa317bc",
          "showTitle": false,
          "title": ""
        },
        "id": "5YbgrB9HkRJZ",
        "outputId": "061ac3f7-5fab-4017-a0da-2e4b543f37ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bf386bff-0f63-4358-82fc-6c7d02c37321",
          "showTitle": false,
          "title": ""
        },
        "id": "Oaxhage8kRJZ"
      },
      "outputs": [],
      "source": [
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "99acd85c-233f-4f2d-a062-028dbcde9960",
          "showTitle": false,
          "title": ""
        },
        "id": "HfpkIUNdkRJZ",
        "outputId": "fa59d70b-38d4-4731-f2c6-8ca80787e1ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([250930, 237205, 974116, 383898])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "ix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e46dc826-9f39-4aed-b2d2-f9ea401136de",
          "showTitle": false,
          "title": ""
        },
        "id": "faoGVPG3kRJa",
        "outputId": "c8bc9b82-4e14-4d1a-bf83-82e5acd2c318",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[42,  1, 58, 46, 59, 57,  1, 21],\n",
              "        [54, 56, 47, 43, 57, 58, 11,  0],\n",
              "        [49, 47, 52, 45, 12,  1, 58, 46],\n",
              "        [58, 46, 53, 59, 58,  1, 56, 43]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2886aedf-200e-40bd-9a9d-4658cf6c509b",
          "showTitle": false,
          "title": ""
        },
        "id": "hkllYFCPkRJa",
        "outputId": "c88e773a-bd9d-455b-eaef-d8f00bbf5aa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1, 58, 46, 59, 57,  1, 21,  1],\n",
              "        [56, 47, 43, 57, 58, 11,  0, 37],\n",
              "        [47, 52, 45, 12,  1, 58, 46, 53],\n",
              "        [46, 53, 59, 58,  1, 56, 43, 42]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "a486fc04-ed29-456f-918b-5f8395e455cb",
          "showTitle": false,
          "title": ""
        },
        "id": "tWrajECBkRJa"
      },
      "source": [
        "以下代码块清楚地展示了预测的自回归性质，以及上下文是对token（在本例中是字符）的一维排列的滚动窗口。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "49a86e10-ac37-4b92-8f18-775cd4853fdc",
          "showTitle": false,
          "title": ""
        },
        "id": "xjgtxxztkRJa",
        "outputId": "96f73a26-e809-4e81-898b-2f9e1917aa6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 6,  0, 14, 43, 44, 53, 56, 43],\n",
            "        [39,  1, 42, 59, 43,  1, 39, 52],\n",
            "        [47, 41, 43,  1, 39, 52, 42,  1],\n",
            "        [53, 44,  1, 50, 43, 58,  1, 58]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 0, 14, 43, 44, 53, 56, 43,  1],\n",
            "        [ 1, 42, 59, 43,  1, 39, 52, 42],\n",
            "        [41, 43,  1, 39, 52, 42,  1, 42],\n",
            "        [44,  1, 50, 43, 58,  1, 58, 46]])\n",
            "----\n",
            "when input is [6] the target: 0\n",
            "when input is [6, 0] the target: 14\n",
            "when input is [6, 0, 14] the target: 43\n",
            "when input is [6, 0, 14, 43] the target: 44\n",
            "when input is [6, 0, 14, 43, 44] the target: 53\n",
            "when input is [6, 0, 14, 43, 44, 53] the target: 56\n",
            "when input is [6, 0, 14, 43, 44, 53, 56] the target: 43\n",
            "when input is [6, 0, 14, 43, 44, 53, 56, 43] the target: 1\n",
            "when input is [39] the target: 1\n",
            "when input is [39, 1] the target: 42\n",
            "when input is [39, 1, 42] the target: 59\n",
            "when input is [39, 1, 42, 59] the target: 43\n",
            "when input is [39, 1, 42, 59, 43] the target: 1\n",
            "when input is [39, 1, 42, 59, 43, 1] the target: 39\n",
            "when input is [39, 1, 42, 59, 43, 1, 39] the target: 52\n",
            "when input is [39, 1, 42, 59, 43, 1, 39, 52] the target: 42\n",
            "when input is [47] the target: 41\n",
            "when input is [47, 41] the target: 43\n",
            "when input is [47, 41, 43] the target: 1\n",
            "when input is [47, 41, 43, 1] the target: 39\n",
            "when input is [47, 41, 43, 1, 39] the target: 52\n",
            "when input is [47, 41, 43, 1, 39, 52] the target: 42\n",
            "when input is [47, 41, 43, 1, 39, 52, 42] the target: 1\n",
            "when input is [47, 41, 43, 1, 39, 52, 42, 1] the target: 42\n",
            "when input is [53] the target: 44\n",
            "when input is [53, 44] the target: 1\n",
            "when input is [53, 44, 1] the target: 50\n",
            "when input is [53, 44, 1, 50] the target: 43\n",
            "when input is [53, 44, 1, 50, 43] the target: 58\n",
            "when input is [53, 44, 1, 50, 43, 58] the target: 1\n",
            "when input is [53, 44, 1, 50, 43, 58, 1] the target: 58\n",
            "when input is [53, 44, 1, 50, 43, 58, 1, 58] the target: 46\n"
          ]
        }
      ],
      "source": [
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "dde3273f-0519-4108-ba84-dfd99e020722",
          "showTitle": false,
          "title": ""
        },
        "id": "RlON_gNikRJa"
      },
      "source": [
        "### 理解 Causal Scaled Dot Product Self Attention (SDPA)\n",
        "\n",
        "这段代码来自于Andrej Karpathy出色的makemore代码库，链接在仓库中。\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e435d0cf-1383-446a-9026-cd80b4266019",
          "showTitle": false,
          "title": ""
        },
        "id": "uBVWP40SkRJa"
      },
      "source": [
        "![scaled dot product self attention](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/self_attention.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "97660589-1719-48c0-ad6f-4f7c2888348a",
          "showTitle": false,
          "title": ""
        },
        "id": "i-jFgMntkRJa"
      },
      "source": [
        "提供的代码演示了自注意力的机制和基本概念，特别是关注经典的缩放点积自注意力。在这个变体中，查询、键和值矩阵都来自同一个输入序列。为了确保自回归语言生成过程的完整性，特别是在仅包含解码器的模型中，代码实现了掩码。这种掩码技术至关重要，因为它隐藏了当前标记位置后面的任何信息，从而将模型的注意力引导到序列的前面部分。这样的注意力机制称为因果自注意力。值得注意的是，稀疏专家混合模型并不局限于仅包含解码器的Transformer架构。事实上，在这个领域的许多重要工作，特别是由Shazeer等人完成的工作，都围绕着T5架构展开，该架构包含了Transformer模型中的编码器和解码器组件。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6f82ca41-a301-4a92-aed9-ba7ac3a2bf88",
          "showTitle": false,
          "title": ""
        },
        "id": "lxMSgZWGkRJb",
        "outputId": "2594f3db-422b-45ea-9d47-deab98521609",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch(batch_size), time(block_size seq_len), channels(n_embd)\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1) #B,T,T\n",
        "\n",
        "v = value(x) #B,T,H\n",
        "out = wei @ v # (B,T,T) @ (B,T,H) -> (B,T,H)\n",
        "#The output from this final matrix product is subsequently passsed through a linear layer as shown in the diagram above\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "49c278ec-19db-4c5d-b4a3-3bdc45c5a443",
          "showTitle": false,
          "title": ""
        },
        "id": "cA3iXggEkRJb"
      },
      "source": [
        "对因果自注意力和多头因果自注意力的代码进行泛化和模块化。多头自注意力将多个注意力头并行应用，每个注意力头专注于通道的不同部分（嵌入维度）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "608c6c9f-fb93-43ed-9580-5e782fd90d61",
          "showTitle": false,
          "title": ""
        },
        "id": "909nX3PHkRJb"
      },
      "outputs": [],
      "source": [
        "#Causal scaled dot product self-Attention Head\n",
        "\n",
        "n_embd = 64 # hidden_size\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "head_size:int = 16 # n_embd/n_head\n",
        "dropout = 0.1\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6e8b31af-f45a-4066-8288-fb0d9c8e2aff",
          "showTitle": false,
          "title": ""
        },
        "id": "T3MoVK_WkRJb"
      },
      "outputs": [],
      "source": [
        "#Multi-Headed Self Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "16267e9a-008b-46e3-82ce-2ae41396a1a1",
          "showTitle": false,
          "title": ""
        },
        "id": "T-w53_mSkRJb",
        "outputId": "624f8678-6cdd-4843-ab54-83115db14896",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 64])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "#Confirming that what's output from multi head attention is the original embedding size\n",
        "B,T,C = 4,block_size,n_embd # batch(batch_size), time(block_size seq_len), channels(n_embd)\n",
        "x = torch.randn(B,T,C)\n",
        "print(x.shape)\n",
        "mha = MultiHeadAttention(n_head,head_size)\n",
        "mha(x).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "5f7ff128-7fe5-4a91-b9f2-208e2132e505",
          "showTitle": false,
          "title": ""
        },
        "id": "wNlJTtfhkRJb"
      },
      "source": [
        "### 创建专家模型： 简单的多层感知器（Multi Layer Perceptron - MLP）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "f6e422a5-57c1-4b2f-b7b9-2757e109848a",
          "showTitle": false,
          "title": ""
        },
        "id": "zv3fGRpbkRJb"
      },
      "source": [
        "在稀疏专家（MoE）架构中，每个Transformer块内部的自注意力机制保持不变。然而，每个块的结构发生了显著变化：标准的前馈神经网络被替换为几个稀疏激活的前馈网络，称为专家。 \"稀疏激活\" 指的是序列中的每个标记仅被路由到总池中的有限数量的这些专家之一或两个 - 通常是一个或两个。这种修改允许对输入数据的不同部分进行专门处理，使模型能够有效地处理更广泛的复杂性。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "efe9fdcc-82eb-4047-9233-ad3cfe8759b1",
          "showTitle": false,
          "title": ""
        },
        "id": "7Kz0Y_P0kRJc"
      },
      "source": [
        "![experts](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/experts.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a2f0f382-ab4a-45e1-9dce-27ae0d3da641",
          "showTitle": false,
          "title": ""
        },
        "id": "a-9CYWXgkRJc"
      },
      "outputs": [],
      "source": [
        "#Expert module\n",
        "class Expert(nn.Module):\n",
        "    \"\"\" An MLP is a simple linear layer followed by a non-linearity i.e. each Expert \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "a7764385-26e9-4d75-9aa7-ce011023e24e",
          "showTitle": false,
          "title": ""
        },
        "id": "qderdEuykRJc"
      },
      "source": [
        "### Top-k Gating Router"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "d3fca4df-4c47-4e9a-98cd-08cf8ccf7726",
          "showTitle": false,
          "title": ""
        },
        "id": "VxJv5y44kRJc"
      },
      "source": [
        "![top k gating](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/topk.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "8e494b86-cdb2-4f2a-8824-5fa2ef4b2606",
          "showTitle": false,
          "title": ""
        },
        "id": "n6DuhY0DkRJc"
      },
      "source": [
        "门控网络，也称为路由器，确定每个token从多头注意力中由哪个专家网络接收输出。让我们考虑一个简单的例子：假设有4个专家，并且要将标记路由到前2个专家。最初，我们通过一个线性层将token输入到门控网络中。这个层将输入张量从形状为（2，4，32）——表示（批量大小，tokens，n_embed，其中n_embed是输入的通道维度）——投影到一个新形状为（2，4，4）的张量，对应于（批量大小，tokens，num_experts），其中num_experts是专家网络的数量。接下来，我们确定最后一维中前k=2个最高值及其相应的索引。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "621916ff-2290-4e2f-9fd7-5181ed98d540",
          "showTitle": false,
          "title": ""
        },
        "id": "pNAuFDDvkRJc",
        "outputId": "1ec2998f-4504-4695-a348-77b81c010bc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 0.9558,  0.1610],\n",
              "          [ 0.8659, -0.1494],\n",
              "          [ 0.8765,  0.7202],\n",
              "          [ 0.9496, -0.6609]],\n",
              " \n",
              "         [[ 0.4419, -0.2500],\n",
              "          [ 1.2602,  0.8430],\n",
              "          [ 0.8570,  0.7822],\n",
              "          [ 0.7376,  0.2561]]], grad_fn=<TopkBackward0>),\n",
              " tensor([[[2, 0],\n",
              "          [3, 2],\n",
              "          [3, 0],\n",
              "          [1, 2]],\n",
              " \n",
              "         [[1, 3],\n",
              "          [1, 2],\n",
              "          [1, 2],\n",
              "          [0, 1]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "#Understanding how gating works\n",
        "num_experts = 4\n",
        "top_k=2\n",
        "n_embed=32\n",
        "\n",
        "\n",
        "#Example multi-head attention output for a simple illustrative example, consider n_embed=32, context_length=4 and batch_size=2\n",
        "mh_output = torch.randn(2, 4, n_embed)\n",
        "\n",
        "topkgate_linear = nn.Linear(n_embed, num_experts) # nn.Linear(32, 4)\n",
        "\n",
        "logits = topkgate_linear(mh_output)\n",
        "top_k_logits, top_k_indices = logits.topk(top_k, dim=-1)  # Get top-k experts\n",
        "top_k_logits, top_k_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "0f135ff7-0aa3-4b6d-ab5e-42399c48427b",
          "showTitle": false,
          "title": ""
        },
        "id": "EKwAyJxrkRJd"
      },
      "source": [
        "通过仅保留沿着最后一个维度的各自索引处的前k个值，获取稀疏门控输出。用'-inf'填充其余部分，并通过softmax激活函数传递。这将'-inf'值推向零，使前两个值更加突出，并且总和为1。这种总和为1有助于对专家输出进行加权。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "735e160a-ef1e-424d-b6d9-09f63ea99ec1",
          "showTitle": false,
          "title": ""
        },
        "id": "IiVejzOpkRJd",
        "outputId": "547b579b-eb17-42ae-f1d1-cf56f93debba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1610,    -inf,  0.9558,    -inf],\n",
              "         [   -inf,    -inf, -0.1494,  0.8659],\n",
              "         [ 0.7202,    -inf,    -inf,  0.8765],\n",
              "         [   -inf,  0.9496, -0.6609,    -inf]],\n",
              "\n",
              "        [[   -inf,  0.4419,    -inf, -0.2500],\n",
              "         [   -inf,  1.2602,  0.8430,    -inf],\n",
              "         [   -inf,  0.8570,  0.7822,    -inf],\n",
              "         [ 0.7376,  0.2561,    -inf,    -inf]]], grad_fn=<ScatterBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "zeros = torch.full_like(logits, float('-inf')) #full_like clones a tensor and fills it with a specified value (like infinity) for masking or calculations.\n",
        "sparse_logits = zeros.scatter(-1, top_k_indices, top_k_logits)\n",
        "sparse_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "9146e6f9-4eee-4a8b-8338-55072719ed59",
          "showTitle": false,
          "title": ""
        },
        "id": "HFgRxDF4kRJh",
        "outputId": "8e5e9c74-9738-4c3d-a288-d0a4a4c23c7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.3111, 0.0000, 0.6889, 0.0000],\n",
              "         [0.0000, 0.0000, 0.2660, 0.7340],\n",
              "         [0.4610, 0.0000, 0.0000, 0.5390],\n",
              "         [0.0000, 0.8335, 0.1665, 0.0000]],\n",
              "\n",
              "        [[0.0000, 0.6664, 0.0000, 0.3336],\n",
              "         [0.0000, 0.6028, 0.3972, 0.0000],\n",
              "         [0.0000, 0.5187, 0.4813, 0.0000],\n",
              "         [0.6181, 0.3819, 0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "gating_output= F.softmax(sparse_logits, dim=-1)\n",
        "gating_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "fe558b12-e443-4b62-9a85-c59120456352",
          "showTitle": false,
          "title": ""
        },
        "id": "dGJrq2uqkRJh"
      },
      "source": [
        "### Noisy Top-k Gating Router (adding noisy top-k Gating for load balancing)\n",
        "\n",
        "泛化和模块化上述代码，并添加嘈杂的top-k门控以实现负载平衡。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "45516b59-d814-4853-a34e-d36aae9f04eb",
          "showTitle": false,
          "title": ""
        },
        "id": "TKp4DqwYkRJh"
      },
      "outputs": [],
      "source": [
        "# First define the top k router module\n",
        "class TopkRouter(nn.Module):\n",
        "    def __init__(self, n_embed, num_experts, top_k):\n",
        "        super(TopkRouter, self).__init__()\n",
        "        self.top_k = top_k\n",
        "        self.linear =nn.Linear(n_embed, num_experts)\n",
        "\n",
        "    def forward(self, mh_ouput):\n",
        "        # mh_ouput is the output tensor from multihead self attention block\n",
        "        logits = self.linear(mh_output)\n",
        "        top_k_logits, indices = logits.topk(self.top_k, dim=-1)\n",
        "        zeros = torch.full_like(logits, float('-inf'))\n",
        "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
        "        router_output = F.softmax(sparse_logits, dim=-1)\n",
        "        return router_output, indices\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c500844f-0866-4bbf-acef-c0c1d4979721",
          "showTitle": false,
          "title": ""
        },
        "id": "KjkouzwkkRJh",
        "outputId": "02009cfd-f439-4409-ea64-f4ce57703e88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 4, 4]),\n",
              " tensor([[[0.0000, 0.0000, 0.6237, 0.3763],\n",
              "          [0.8167, 0.0000, 0.1833, 0.0000],\n",
              "          [0.2440, 0.7560, 0.0000, 0.0000],\n",
              "          [0.4934, 0.0000, 0.0000, 0.5066]],\n",
              " \n",
              "         [[0.0000, 0.0000, 0.5009, 0.4991],\n",
              "          [0.0000, 0.0000, 0.4645, 0.5355],\n",
              "          [0.0000, 0.7588, 0.2412, 0.0000],\n",
              "          [0.6103, 0.0000, 0.3897, 0.0000]]], grad_fn=<SoftmaxBackward0>),\n",
              " tensor([[[2, 3],\n",
              "          [0, 2],\n",
              "          [1, 0],\n",
              "          [3, 0]],\n",
              " \n",
              "         [[2, 3],\n",
              "          [3, 2],\n",
              "          [1, 2],\n",
              "          [0, 2]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "#Testing this out:\n",
        "num_experts = 4\n",
        "top_k = 2\n",
        "n_embd = 32\n",
        "\n",
        "mh_output = torch.randn(2, 4, n_embd)  # Example input\n",
        "top_k_gate = TopkRouter(n_embd, num_experts, top_k)\n",
        "gating_output, indices = top_k_gate(mh_output)\n",
        "gating_output.shape, gating_output, indices\n",
        "#And it works!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "9fa02d0c-3688-4d01-811d-d0b2b851ab33",
          "showTitle": false,
          "title": ""
        },
        "id": "tAouN-GwkRJi"
      },
      "source": [
        "虽然最近发布的Mixtral论文没有提到，但我认为嘈杂的top-k门控是训练MoE模型的重要工具。基本上，您不希望所有的token都被发送到同一组“偏爱”的专家中。您希望在开发和探索之间达到良好的平衡。为此，为门控线性层的logits添加标准正态噪声有助于负载平衡，使训练更加高效。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "e05b3306-b89f-4ebc-901b-f16398a925c2",
          "showTitle": false,
          "title": ""
        },
        "id": "aWeueE83kRJi"
      },
      "source": [
        "![noisy top-k gating](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/noisytopkgating.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "dda4d805-373c-48f7-9037-da08fbc06e64",
          "showTitle": false,
          "title": ""
        },
        "id": "ZBrN-w3JkRJi"
      },
      "outputs": [],
      "source": [
        "#Changing the above to accomodate noisy top-k gating\n",
        "class NoisyTopkRouter(nn.Module):\n",
        "    def __init__(self, n_embed, num_experts, top_k):\n",
        "        super(NoisyTopkRouter, self).__init__()\n",
        "        self.top_k = top_k\n",
        "        #layer for router logits\n",
        "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
        "        self.noise_linear =nn.Linear(n_embed, num_experts)\n",
        "\n",
        "\n",
        "    def forward(self, mh_output):\n",
        "        # mh_ouput is the output tensor from multihead self attention block\n",
        "        logits = self.topkroute_linear(mh_output)\n",
        "\n",
        "        #Noise logits\n",
        "        noise_logits = self.noise_linear(mh_output)\n",
        "\n",
        "        #Adding scaled unit gaussian noise to the logits\n",
        "        noise = torch.randn_like(logits)*F.softplus(noise_logits)\n",
        "        noisy_logits = logits + noise\n",
        "\n",
        "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
        "        zeros = torch.full_like(noisy_logits, float('-inf'))\n",
        "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
        "        router_output = F.softmax(sparse_logits, dim=-1)\n",
        "        return router_output, indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a01a9d6b-fedb-427d-b0da-c3b2a75a8643",
          "showTitle": false,
          "title": ""
        },
        "id": "7Q6KcH9AkRJi",
        "outputId": "3cffb18d-3387-4a06-a01e-03ff40c6557a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 4, 8]),\n",
              " tensor([[[0.8108, 0.0000, 0.0000, 0.0000, 0.1892, 0.0000, 0.0000, 0.0000],\n",
              "          [0.5673, 0.0000, 0.0000, 0.0000, 0.4327, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.3452, 0.6548, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.6709, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3291]],\n",
              " \n",
              "         [[0.5440, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4560],\n",
              "          [0.9737, 0.0000, 0.0000, 0.0000, 0.0000, 0.0263, 0.0000, 0.0000],\n",
              "          [0.4904, 0.5096, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.5366, 0.0000, 0.0000, 0.0000, 0.0000, 0.4634, 0.0000, 0.0000]]],\n",
              "        grad_fn=<SoftmaxBackward0>),\n",
              " tensor([[[0, 4],\n",
              "          [0, 4],\n",
              "          [4, 3],\n",
              "          [1, 7]],\n",
              " \n",
              "         [[0, 7],\n",
              "          [0, 5],\n",
              "          [1, 0],\n",
              "          [0, 5]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "#Testing this out, again:\n",
        "num_experts = 8\n",
        "top_k = 2\n",
        "n_embd = 16\n",
        "\n",
        "mh_output = torch.randn(2, 4, n_embd)  # Example input\n",
        "noisy_top_k_gate = NoisyTopkRouter(n_embd, num_experts, top_k)\n",
        "gating_output, indices = noisy_top_k_gate(mh_output)\n",
        "gating_output.shape, gating_output, indices\n",
        "#It works!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "076fa004-a165-42a7-b729-0bca8ad39418",
          "showTitle": false,
          "title": ""
        },
        "id": "XyKjpR-dkRJi"
      },
      "source": [
        "\n",
        "### 创建一个稀疏专家模型 (Sparse MoE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "6747b8de-0086-4cb0-8fbd-46ee95457eb9",
          "showTitle": false,
          "title": ""
        },
        "id": "UsRCy7i3kRJi"
      },
      "source": [
        "这个过程的主要方面涉及门控网络的输出。在获得这些结果后，会选择性地将前k个值与相应的前k个专家的输出相乘，以获得给定token的结果。这种选择性的乘法形成了加权求和，构成了SparseMoe块的输出。这个过程中的关键和具有挑战性的部分是避免不必要的乘法。只对前k个专家进行前向传播，然后计算这个加权和是至关重要的。对每个专家都进行前向传播会违背使用稀疏MoE的初衷，因为它将不再是稀疏的。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d6809b3f-4be9-4859-b39e-24fcdd6c8d86",
          "showTitle": false,
          "title": ""
        },
        "id": "7dDUHU_IkRJi"
      },
      "outputs": [],
      "source": [
        "class SparseMoE(nn.Module):\n",
        "    def __init__(self, n_embed, num_experts, top_k):\n",
        "        super(SparseMoE, self).__init__()\n",
        "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
        "        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def forward(self, x):\n",
        "        gating_output, indices = self.router(x)\n",
        "        final_output = torch.zeros_like(x)\n",
        "\n",
        "        # Reshape inputs for batch processing\n",
        "        flat_x = x.view(-1, x.size(-1))\n",
        "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
        "\n",
        "        # Process each expert in parallel\n",
        "        for i, expert in enumerate(self.experts):\n",
        "            # Create a mask for the inputs where the current expert is in top-k\n",
        "            expert_mask = (indices == i).any(dim=-1)\n",
        "            flat_mask = expert_mask.view(-1)\n",
        "\n",
        "            if flat_mask.any():\n",
        "                expert_input = flat_x[flat_mask]\n",
        "                expert_output = expert(expert_input)\n",
        "\n",
        "                # Extract and apply gating scores\n",
        "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
        "                weighted_output = expert_output * gating_scores\n",
        "\n",
        "                # Update final output\n",
        "                # We need to scatter_add the weighted outputs to their original positions in the batch\n",
        "                final_output.masked_scatter_(expert_mask.unsqueeze(-1), weighted_output)\n",
        "\n",
        "        return final_output.view_as(x)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "06239630-0a1c-47c9-976c-7770f3d82e18",
          "showTitle": false,
          "title": ""
        },
        "id": "q8kDLI1ukRJj",
        "outputId": "05ded114-399a-4e55-81a0-9c8332a3429f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the final output: torch.Size([4, 8, 16])\n",
            "tensor([[[ 7.0902e-03, -3.9785e-01,  2.5950e-01, -2.7855e-01,  1.0532e-01,\n",
            "           2.1916e-02, -3.9044e-01, -2.9938e-01, -1.6038e-01,  8.4919e-03,\n",
            "          -5.1904e-01, -2.6361e-01, -8.4711e-02,  3.5468e-02,  2.9799e-02,\n",
            "          -6.0143e-03],\n",
            "         [ 1.0294e-01,  4.1642e-01, -4.5188e-01,  3.9192e-03, -1.1848e-02,\n",
            "           5.1312e-02,  1.7714e-01, -1.9039e-01, -0.0000e+00,  1.9526e-01,\n",
            "           3.2151e-01, -3.0537e-01,  2.8723e-02,  2.9380e-01, -2.0152e-02,\n",
            "          -3.2795e-01],\n",
            "         [ 4.4361e-03,  0.0000e+00,  1.0039e-01, -2.2677e-02,  8.2390e-02,\n",
            "           5.1152e-02, -0.0000e+00, -1.0229e-01, -6.3003e-02, -7.9471e-03,\n",
            "          -2.7593e-01, -1.4275e-01,  1.6774e-02,  1.4232e-02,  1.9370e-02,\n",
            "          -3.2314e-02],\n",
            "         [-1.8278e-01, -3.1073e-01, -2.5140e-02,  1.5505e-01,  6.3831e-02,\n",
            "           7.3816e-02, -6.7672e-02, -8.8567e-02,  1.9844e-03,  9.7871e-02,\n",
            "          -3.4875e-01, -3.8098e-01, -1.6249e-01, -1.6895e-01, -3.3104e-02,\n",
            "           1.2102e-01],\n",
            "         [ 1.5222e-02, -6.9461e-03, -2.5421e-03, -4.1493e-02,  9.3050e-03,\n",
            "           8.3706e-03, -4.7768e-02, -1.8708e-02, -0.0000e+00,  5.8577e-02,\n",
            "          -7.7071e-02, -4.5046e-03,  3.7677e-02,  1.1022e-02,  2.7242e-02,\n",
            "           1.3886e-02],\n",
            "         [ 8.3666e-02,  4.3192e-03, -8.0300e-03,  1.9376e-01,  9.9431e-02,\n",
            "          -0.0000e+00,  6.2849e-03,  4.8056e-02,  6.2209e-02,  2.6626e-02,\n",
            "           1.0765e-01, -0.0000e+00,  2.0613e-02, -5.3575e-02, -3.6060e-02,\n",
            "          -7.1929e-04],\n",
            "         [-1.1137e-01,  4.8109e-01, -3.2158e-03,  7.6186e-02, -1.8514e-02,\n",
            "           1.4600e-02,  1.5851e-01, -9.9021e-02,  1.3835e-02,  3.1484e-01,\n",
            "          -1.5153e-01, -4.3381e-01,  5.1379e-02,  0.0000e+00,  4.3070e-02,\n",
            "          -8.2125e-02],\n",
            "         [ 3.0789e-02,  4.8443e-02,  1.6036e-01, -0.0000e+00,  1.4389e-01,\n",
            "           2.3659e-01, -1.3213e-02,  9.3633e-02, -3.3868e-02,  6.9731e-02,\n",
            "          -2.5851e-02, -2.5800e-02,  8.7120e-02, -5.5730e-02, -7.8549e-03,\n",
            "          -5.5365e-02]],\n",
            "\n",
            "        [[-9.3232e-02, -0.0000e+00,  1.3517e-01,  2.3967e-01,  8.5823e-02,\n",
            "          -1.7276e-01,  2.3944e-02, -1.4631e-01,  1.9627e-01,  1.5427e-01,\n",
            "          -2.5793e-01, -5.5459e-02,  4.7775e-02,  9.5862e-02, -0.0000e+00,\n",
            "          -2.0606e-01],\n",
            "         [ 7.3696e-02,  0.0000e+00,  1.2270e-01,  1.8900e-01,  8.0504e-02,\n",
            "          -1.0684e-01,  4.6204e-02,  9.9532e-02,  1.9922e-01, -2.3386e-02,\n",
            "          -5.0311e-02, -9.5034e-02,  1.1598e-01, -9.1335e-02,  4.8913e-02,\n",
            "          -1.9782e-01],\n",
            "         [ 4.1296e-02,  7.5910e-02, -1.6587e-01,  5.5780e-02,  6.1256e-02,\n",
            "          -1.8636e-01,  5.1717e-02, -9.3097e-02,  2.6238e-02,  2.5437e-01,\n",
            "           1.4697e-01, -6.9759e-02, -1.2682e-01,  1.5238e-01, -0.0000e+00,\n",
            "          -2.7709e-02],\n",
            "         [ 2.2114e-02,  5.7594e-02,  3.4403e-02, -7.7731e-02, -1.2270e-03,\n",
            "           4.8158e-02, -7.2131e-02,  2.5254e-01, -1.1306e-02, -7.3560e-02,\n",
            "          -6.9742e-02, -1.6971e-01, -5.0751e-02, -1.0020e-01, -4.6444e-02,\n",
            "          -1.2498e-01],\n",
            "         [ 4.2070e-02, -4.9295e-02,  1.3061e-02,  0.0000e+00, -7.0366e-02,\n",
            "           0.0000e+00, -6.9172e-02, -8.6724e-03,  1.5560e-01,  3.8947e-02,\n",
            "          -9.1059e-02, -1.3738e-02, -1.1989e-01,  9.1656e-02,  2.4767e-02,\n",
            "          -2.4643e-01],\n",
            "         [-1.1866e-01, -1.4329e-01, -1.4611e-02,  3.1168e-02,  5.2803e-02,\n",
            "          -7.6062e-02, -1.4015e-01, -3.7243e-02,  1.8793e-01,  1.5845e-01,\n",
            "          -5.8051e-02,  8.5040e-03, -1.0284e-01, -0.0000e+00,  4.9136e-02,\n",
            "          -1.7113e-05],\n",
            "         [-2.7025e-02, -0.0000e+00, -6.3316e-02, -0.0000e+00,  1.3463e-01,\n",
            "           1.4033e-01,  3.5989e-02,  3.3835e-03, -1.6649e-01, -4.2596e-02,\n",
            "           6.2817e-02, -1.8058e-01, -0.0000e+00, -9.4880e-02, -1.4716e-01,\n",
            "          -6.3473e-02],\n",
            "         [ 1.9757e-01, -3.9286e-02,  6.2043e-02,  5.9956e-02,  4.3724e-02,\n",
            "          -2.0910e-01,  0.0000e+00, -4.9258e-02,  5.9059e-02,  0.0000e+00,\n",
            "          -7.9359e-02, -6.7765e-02,  9.5178e-03, -1.6579e-01, -2.0318e-02,\n",
            "          -1.9235e-01]],\n",
            "\n",
            "        [[-9.9821e-03,  2.6563e-02,  3.9064e-02,  9.3178e-02, -1.5811e-01,\n",
            "          -1.7236e-01,  2.1204e-01,  2.3047e-01,  2.3720e-01, -1.6691e-01,\n",
            "          -0.0000e+00, -1.8347e-02,  1.7207e-01,  7.4248e-02,  1.1800e-02,\n",
            "          -3.1290e-01],\n",
            "         [ 1.4778e-02, -1.1592e-02,  4.6017e-02, -5.6714e-02,  6.2374e-02,\n",
            "           1.4087e-01, -7.4995e-02,  0.0000e+00, -0.0000e+00, -1.1922e-01,\n",
            "          -4.0507e-03,  1.6449e-01,  2.1372e-01, -0.0000e+00,  6.1882e-03,\n",
            "           3.3349e-02],\n",
            "         [ 0.0000e+00,  6.4599e-02, -3.3384e-02, -3.1901e-03, -1.1457e-02,\n",
            "          -0.0000e+00,  5.6654e-02, -1.3900e-01, -1.7182e-01,  1.4746e-01,\n",
            "          -2.2588e-02,  9.0085e-02,  1.8273e-01, -6.3535e-02, -9.3764e-02,\n",
            "           8.4647e-02],\n",
            "         [-8.1504e-03,  1.9732e-01, -1.0614e-01,  1.0559e-01, -3.0312e-02,\n",
            "           3.4910e-03,  8.9322e-02,  0.0000e+00, -2.8848e-02,  1.2396e-01,\n",
            "          -3.0295e-02, -1.6208e-01, -1.5634e-02,  1.7477e-01, -7.9974e-03,\n",
            "          -1.1316e-01],\n",
            "         [ 0.0000e+00, -9.1557e-01,  3.2491e-01, -5.5598e-01, -1.9007e-02,\n",
            "          -2.5471e-01, -1.7088e-01, -1.7087e-01, -1.0254e+00,  1.8221e-01,\n",
            "          -7.5076e-01, -2.9540e-01, -2.4143e-01,  9.5046e-02, -0.0000e+00,\n",
            "           2.0736e-01],\n",
            "         [ 1.0492e-01,  1.1123e-01,  4.2675e-02,  5.8447e-02, -1.7306e-01,\n",
            "           7.7420e-02, -5.6667e-02, -1.7969e-01, -9.8093e-02,  1.2518e-01,\n",
            "           0.0000e+00,  2.2737e-01,  2.4283e-01, -1.1951e-02, -2.0135e-01,\n",
            "          -8.7676e-02],\n",
            "         [-4.4439e-02, -1.1490e-01,  8.4873e-02, -4.0468e-02,  8.7659e-03,\n",
            "           1.3284e-01, -1.6354e-02, -0.0000e+00,  2.8038e-02,  2.3875e-02,\n",
            "          -2.0810e-01, -0.0000e+00, -1.1966e-02, -1.6517e-03,  8.9021e-03,\n",
            "           4.9589e-02],\n",
            "         [ 1.9238e-03, -1.1183e-01,  4.7651e-02, -4.4799e-02,  5.1195e-02,\n",
            "           9.7753e-02, -1.2341e-03, -2.1751e-02, -1.0706e-01, -6.4348e-02,\n",
            "          -6.5725e-03, -9.1290e-02,  2.6163e-03, -8.2028e-02, -1.2193e-02,\n",
            "          -4.8648e-02]],\n",
            "\n",
            "        [[-1.3687e-02, -3.4894e-01,  5.0357e-02, -1.9800e-01, -1.4568e-01,\n",
            "          -3.5514e-01,  4.1675e-02,  1.5287e-01,  4.2042e-01,  0.0000e+00,\n",
            "          -2.8177e-01, -1.7952e-01, -2.5762e-01, -3.0732e-01,  3.4048e-01,\n",
            "          -1.2918e-01],\n",
            "         [ 4.6953e-02,  6.0696e-02, -2.2946e-01, -1.4007e-01,  0.0000e+00,\n",
            "          -7.7821e-02,  1.1684e-01, -0.0000e+00, -1.4261e-01,  5.9779e-02,\n",
            "           1.5856e-01, -2.8272e-01,  6.2096e-02,  0.0000e+00, -1.4887e-03,\n",
            "          -2.0243e-01],\n",
            "         [ 1.1504e-02, -5.0405e-02, -1.3080e-02, -3.1717e-02,  7.9938e-02,\n",
            "           6.3951e-02, -0.0000e+00, -7.7068e-03, -2.9505e-02,  3.1871e-03,\n",
            "          -0.0000e+00, -9.4176e-02,  9.4114e-03, -2.4377e-02,  2.2030e-02,\n",
            "           0.0000e+00],\n",
            "         [-2.5270e-03,  1.2607e-01, -3.0448e-02, -2.4758e-01,  1.4040e-01,\n",
            "          -1.1162e-01,  6.9018e-02,  0.0000e+00,  9.3282e-02,  7.4819e-02,\n",
            "          -0.0000e+00,  0.0000e+00, -1.0497e-01, -7.9180e-02, -6.0220e-02,\n",
            "          -2.5417e-02],\n",
            "         [-9.2432e-03,  1.5398e-03,  1.4280e-01, -1.5391e-01,  3.7791e-02,\n",
            "           1.0939e-01, -0.0000e+00,  8.1758e-02, -1.7039e-01,  2.7449e-01,\n",
            "          -0.0000e+00, -1.4731e-02,  0.0000e+00, -1.6098e-01,  0.0000e+00,\n",
            "          -4.0818e-02],\n",
            "         [-2.7414e-02, -2.6008e-01, -2.8111e-01, -2.1479e-01,  1.1990e-02,\n",
            "          -1.1928e-01,  1.9846e-02,  3.0727e-01, -3.4547e-01, -4.4691e-02,\n",
            "          -2.2189e-01, -1.6102e-01,  2.2424e-01, -8.8759e-02,  2.9172e-01,\n",
            "           1.2923e-01],\n",
            "         [-9.8048e-02, -5.1701e-02, -1.6556e-01, -2.2874e-01,  5.3575e-01,\n",
            "           5.5958e-01,  3.8906e-01,  5.0318e-01,  0.0000e+00, -3.4652e-01,\n",
            "          -3.8751e-02, -1.7142e-01, -4.8565e-01,  1.1984e-01, -4.2107e-01,\n",
            "           1.7790e-01],\n",
            "         [-2.9049e-02,  7.9774e-02,  8.7329e-02,  2.9701e-02,  0.0000e+00,\n",
            "           1.1888e-01, -1.5980e-01,  0.0000e+00,  0.0000e+00,  1.4211e-02,\n",
            "          -0.0000e+00,  0.0000e+00,  1.0245e-01,  7.1208e-03,  1.4510e-01,\n",
            "           1.2389e-01]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#Let's test this out\n",
        "num_experts = 8\n",
        "top_k = 2\n",
        "n_embd = 16\n",
        "dropout=0.1\n",
        "\n",
        "mh_output = torch.randn(4, 8, n_embd)  # Example multi-head attention output\n",
        "sparse_moe = SparseMoE(n_embd, num_experts, top_k)\n",
        "final_output = sparse_moe(mh_output)\n",
        "print(\"Shape of the final output:\", final_output.shape)\n",
        "print(final_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "7476ca07-a315-4108-aa77-46173a703ca2",
          "showTitle": false,
          "title": ""
        },
        "id": "l7GoxCi2kRJj"
      },
      "source": [
        "强调一下，需要认识到路由器/门控网络输出的前k个专家的幅值，正如上面的代码所示，也是非常重要的。这些前k个索引确定了被激活的专家，而在这些前k个维度中数值的大小决定了它们各自的权重。这种加权求和的概念在下面的图示中进一步强调了。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "d99b5dce-301e-4380-8263-b5cfb4136ab2",
          "showTitle": false,
          "title": ""
        },
        "id": "e9a_oQ2akRJj"
      },
      "source": [
        "![sparse MoE](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/sparseMoEfinal.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Balancing Auxiliary Loss\n",
        "\n",
        "from:\n",
        "\n",
        "switch transformers: https://arxiv.org/pdf/2101.03961.pdf  \n",
        "\n",
        "A. Differentiable Load Balancing Loss"
      ],
      "metadata": {
        "id": "rJJ50AAa5E0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@torch.jit.script\n",
        "def compute_aux_loss(num_experts: int,\n",
        "                     top_k_gates: torch.Tensor,\n",
        "                     top_k_indices: torch.Tensor,\n",
        "                     logits: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Calculate and return the auxiliary loss based on the accumulated statistics.\n",
        "    switch transformers: https://arxiv.org/pdf/2101.03961.pdf\n",
        "    A. Differentiable Load Balancing Loss\n",
        "\n",
        "    Args:\n",
        "        num_experts (int): The number of experts.\n",
        "        top_k_gates (tensor): k个最大值的对应logits, 其每个元素表示对应logit概率值。\n",
        "        top_k_indices (tensor): k个最大值的对应logits索引, 其每个元素表示logit对应索引值。\n",
        "        logits (tensor): 其每个元素表示对应logit概率值。\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The calculated auxiliary loss.\n",
        "    \"\"\"\n",
        "    # 对logits进行softmax操作，得到每个类别的概率分布\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    zeros = torch.zeros_like(probs)\n",
        "    # Convert zeros to match top_k_gates dtype\n",
        "    zeros = zeros.to(top_k_gates.dtype)\n",
        "    gates = zeros.scatter(-1, top_k_indices, top_k_gates)\n",
        "\n",
        "    # 获取 logits 张量的批次大小，即样本数量\n",
        "    count = logits.size(0)\n",
        "    # 计算每个专家被选中的概率之和，即将概率沿着批次维度求和。\n",
        "    probs = probs.sum(0)\n",
        "    # 计算每个专家被选中的频率，即计算门控值大于0的次数（即专家被选中的次数），\n",
        "    # 然后将其沿着批次维度求和。\n",
        "    freq = (gates > 0).float().sum(0)\n",
        "    # 计算 logits 张量经过 softmax 处理后的平方和的对数。\n",
        "    # 这里首先使用 softmax 函数将 logits 转换为概率分布，\n",
        "    # 然后计算概率分布的每个样本的平方和，并取对数，最后将结果沿着批次维度求和。\n",
        "    lsesq = (torch.log(torch.exp(logits).sum(dim=-1)) ** 2).sum()\n",
        "\n",
        "    # 计算专家选择损失，其计算方式为对每个专家的概率和频率进行归一化，然后计算它们的点积，最后将结果乘以专家数量。\n",
        "    switchloss = num_experts * \\\n",
        "        (F.normalize(probs, p=1, dim=0) * F.normalize(freq, p=1, dim=0)).sum()\n",
        "    # 计算 z 损失，即 logits 的对数平方和除以样本数量\n",
        "    zloss = lsesq / count\n",
        "    # 将专家选择损失和 z 损失加权相加得到最终的辅助损失\n",
        "    loss = switchloss + 0.1 * zloss\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "PK1uuohR5ELc"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#noisy top-k gating\n",
        "class NoisyTopkRouter(nn.Module):\n",
        "    def __init__(self, n_embed, num_experts, top_k):\n",
        "        super(NoisyTopkRouter, self).__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.top_k = top_k\n",
        "        #layer for router logits\n",
        "        print(n_embed,num_experts,top_k)\n",
        "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
        "        self.noise_linear = nn.Linear(n_embed, num_experts)\n",
        "        self.aux_loss = 0.0\n",
        "\n",
        "\n",
        "    def forward(self, mh_output):\n",
        "        print(f\"mh_output.shape:{mh_output.shape}\")\n",
        "\n",
        "        # mh_ouput is the output tensor from multihead self attention block\n",
        "        logits = self.topkroute_linear(mh_output)\n",
        "        print(f\"logits.shape:{logits.shape}\")\n",
        "\n",
        "\n",
        "        #Noise logits\n",
        "        noise_logits = self.noise_linear(mh_output)\n",
        "\n",
        "\n",
        "        #Adding scaled unit gaussian noise to the logits\n",
        "        noise = torch.randn_like(logits)*F.softplus(noise_logits)\n",
        "        noisy_logits = logits + noise\n",
        "\n",
        "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
        "        zeros = torch.full_like(noisy_logits, float('-inf'))\n",
        "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
        "        router_output = F.softmax(sparse_logits, dim=-1)\n",
        "        # 训练时才计算辅助loss值, 为了专家之间的负载平衡\n",
        "        if self.training:\n",
        "          self.aux_loss = compute_aux_loss(self.num_experts, router_output, indices, noisy_logits)\n",
        "\n",
        "        return router_output, indices\n"
      ],
      "metadata": {
        "id": "dnTTSz4dfJ09"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing this out, again:\n",
        "num_experts = 8\n",
        "top_k = 2\n",
        "n_embd = 16\n",
        "\n",
        "input = torch.randn(2, 4, n_embd)  # Example input\n",
        "input = input.reshape(-1,n_embd)\n",
        "print(f\"input.shape:{input.shape}\")\n",
        "noisy_top_k_gate = NoisyTopkRouter(n_embd, num_experts, top_k)\n",
        "noisy_top_k_gate.training = True\n",
        "\n",
        "gating_output, indices = noisy_top_k_gate(input)\n",
        "print(noisy_top_k_gate.aux_loss.shape, noisy_top_k_gate.aux_loss)\n",
        "gating_output.shape, gating_output, indices\n",
        "#It works!!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUiTr1pOfU_-",
        "outputId": "bae03f0b-a94b-463a-946f-3acb3f3753be"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input.shape:torch.Size([8, 16])\n",
            "noise_logits.shape:torch.Size([8, 8])\n",
            "torch.Size([]) tensor(1.7808, grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([8, 8]),\n",
              " tensor([[0.0000, 0.5588, 0.0000, 0.0000, 0.0000, 0.4412, 0.0000, 0.0000],\n",
              "         [0.0000, 0.3876, 0.0000, 0.6124, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.3627, 0.0000, 0.6373, 0.0000, 0.0000],\n",
              "         [0.4443, 0.0000, 0.0000, 0.0000, 0.0000, 0.5557, 0.0000, 0.0000],\n",
              "         [0.5004, 0.0000, 0.0000, 0.0000, 0.0000, 0.4996, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.3144, 0.0000, 0.0000, 0.0000, 0.6856, 0.0000],\n",
              "         [0.0000, 0.0000, 0.2767, 0.0000, 0.7233, 0.0000, 0.0000, 0.0000],\n",
              "         [0.3451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6549, 0.0000]],\n",
              "        grad_fn=<SoftmaxBackward0>),\n",
              " tensor([[1, 5],\n",
              "         [3, 1],\n",
              "         [5, 3],\n",
              "         [5, 0],\n",
              "         [0, 5],\n",
              "         [6, 2],\n",
              "         [4, 2],\n",
              "         [6, 0]]))"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 引入专家容量 （Expert Capacity factor）\n",
        "\n",
        "from: https://huggingface.co/blog/AviSoori1x/makemoe2\n",
        "\n",
        "\n",
        "\n",
        "在预训练混合专家语言模型或任何大型语言模型时，该过程通常跨越多个GPU，并且通常涉及许多机器。跨这些硬件资源并行训练的方式对于平衡计算负载至关重要。然而，如果某些专家或一组专家过度受到偏爱——反映出对开发的偏好超过探索——它不仅可能导致模型中的性能问题，还可能导致集群中的计算负载不平衡。\n",
        "\n",
        "[Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) 实现使用专家容量来规避这个问题。专家容量确定每个专家在训练或推理过程中负责处理多少个标记。它是基于批次中的标记数和可用专家的数量定义的，通常通过容量因子进行调整。该因子允许在分配中灵活性，提供缓冲区以考虑数据分布的变化，并确保没有单个专家由于过载而成为瓶颈。在训练这些大型模型时，硬件故障是很常见的，可能持续数周甚至数月，因此这一点非常重要。\n",
        "\n",
        "以下是专家容量通常计算的方式：\n",
        "\n",
        "专家容量 = （每批标记数 / 专家数量）× 容量因子 其中：\n",
        "```python\n",
        "expert_capacity = int((tokens_per_batch / self.num_experts) * self.capacity_factor)\n",
        "```\n",
        "\n",
        "- 每批标记数`tokens_per_batch`是需要处理的批次中存在的总标记数。\n",
        "- 专家数量`num_experts`是MoE层中可用于处理数据的专家总数。\n",
        "- 容量因子`capacity_factor`是用于调整基础容量（每批标记数除以专家数量）的乘数。大于1的容量因子允许每个专家处理超出均匀分配份额的缓冲区，适应标记分配的不平衡。该值的一般范围为1-1.25。\n",
        "\n",
        "以下代码块进行了轻微调整，以实现专家容量的简单版本。"
      ],
      "metadata": {
        "id": "Xzd2KhXDnHMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SparseMoE(nn.Module):\n",
        "    def __init__(self, n_embed, num_experts, top_k, capacity_factor=1.0):\n",
        "        super(SparseMoE, self).__init__()\n",
        "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
        "        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
        "        self.top_k = top_k\n",
        "        # add capacity_factor\n",
        "        self.capacity_factor = capacity_factor\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "    def forward(self, x):\n",
        "    # Assuming x has shape [batch_size, seq_len, n_embd]\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        gating_output, indices = self.router(x)\n",
        "        final_output = torch.zeros_like(x)\n",
        "\n",
        "        # Flatten the batch and sequence dimensions to treat each token independently\n",
        "        flat_x = x.view(-1, x.size(-1))  # Now shape [batch_size * seq_len, n_embd]\n",
        "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
        "\n",
        "        tokens_per_batch = batch_size * seq_len * self.top_k\n",
        "        expert_capacity = int((tokens_per_batch / self.num_experts) * self.capacity_factor)\n",
        "\n",
        "        updates = torch.zeros_like(flat_x)\n",
        "\n",
        "        for i, expert in enumerate(self.experts):\n",
        "            expert_mask = (indices == i).any(dim=-1)\n",
        "            flat_mask = expert_mask.view(-1)\n",
        "            selected_indices = torch.nonzero(flat_mask).squeeze(-1)\n",
        "\n",
        "            limited_indices = selected_indices[:expert_capacity] if selected_indices.numel() > expert_capacity else selected_indices\n",
        "            if limited_indices.numel() > 0:\n",
        "                expert_input = flat_x[limited_indices]\n",
        "                expert_output = expert(expert_input)\n",
        "\n",
        "                gating_scores = flat_gating_output[limited_indices, i].unsqueeze(1)\n",
        "                weighted_output = expert_output * gating_scores\n",
        "\n",
        "                updates.index_add_(0, limited_indices, weighted_output)\n",
        "\n",
        "        # Reshape updates to match the original dimensions of x\n",
        "        final_output += updates.view(batch_size, seq_len, -1)\n",
        "\n",
        "        return final_output\n"
      ],
      "metadata": {
        "id": "HjoVac7Rood3"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#Let's test this out\n",
        "num_experts = 8\n",
        "top_k = 2\n",
        "n_embd = 16\n",
        "dropout=0.1\n",
        "\n",
        "mh_output = torch.randn(4, 8, n_embd)  # Example multi-head attention output\n",
        "sparse_moe = SparseMoE(n_embd, num_experts, top_k)\n",
        "final_output = sparse_moe(mh_output)\n",
        "print(\"Shape of the final output:\", final_output.shape)\n",
        "print(final_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HelPhgxrosLK",
        "outputId": "acf64c3a-2dd9-4a90-b845-9e5c57c46859"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the final output: torch.Size([4, 8, 16])\n",
            "tensor([[[-1.3410e-01,  2.8907e-01, -1.8396e-01,  2.1652e-01,  3.1087e-01,\n",
            "           1.5474e-01,  1.2759e-01,  2.1254e-01,  6.7634e-02, -1.8771e-01,\n",
            "           2.2336e-02, -2.4315e-01, -2.3298e-01, -2.9945e-02,  9.7308e-02,\n",
            "          -3.2861e-02],\n",
            "         [-3.1417e-01,  1.1285e-01,  1.9766e-02,  5.2405e-02,  1.3877e-01,\n",
            "           4.7633e-01,  1.2400e-01,  8.7759e-02,  1.1468e-01,  2.7183e-01,\n",
            "           1.4489e-01, -2.7205e-02,  3.2547e-01, -2.2932e-01, -5.1435e-01,\n",
            "           2.6505e-01],\n",
            "         [-2.4992e-01,  3.7111e-02,  2.6726e-01,  1.9991e-01,  1.3635e-01,\n",
            "           9.2536e-02,  2.1399e-01,  2.9728e-03, -1.0015e-01, -1.3600e-02,\n",
            "          -1.4013e-01, -4.1275e-02,  1.7718e-01, -1.4034e-01,  2.3375e-02,\n",
            "          -2.1945e-01],\n",
            "         [-1.5340e-01, -6.3935e-02, -1.4276e-01, -5.8719e-02,  2.7454e-01,\n",
            "          -5.0834e-02, -1.9131e-01,  1.7582e-01, -1.0842e-01, -1.0077e-01,\n",
            "           3.6268e-01,  3.8537e-02, -1.8663e-02,  2.1565e-01, -1.0356e-01,\n",
            "           7.7622e-02],\n",
            "         [-5.0352e-02,  6.0169e-02,  7.8225e-02,  2.0460e-01,  3.4104e-01,\n",
            "           4.1185e-03, -2.1693e-01,  9.1511e-03,  8.1720e-03,  1.8634e-01,\n",
            "          -4.9743e-03, -2.8880e-03,  6.6464e-02, -3.1658e-02,  1.1827e-01,\n",
            "           1.0420e-01],\n",
            "         [-4.7088e-02,  1.7321e-01, -1.2078e-01, -1.7532e-01,  3.4246e-01,\n",
            "           2.3853e-01, -2.1185e-01,  4.4237e-01,  2.5381e-02, -7.6518e-02,\n",
            "          -7.8987e-02,  6.8506e-03, -1.1590e-01,  3.2015e-01,  2.0830e-01,\n",
            "          -1.7926e-01],\n",
            "         [ 3.0142e-01,  8.2914e-02, -2.1717e-01, -4.2297e-02, -3.8633e-01,\n",
            "           1.6955e-01, -7.9301e-02, -2.1649e-01,  3.1913e-01,  6.6859e-02,\n",
            "          -8.6531e-02, -3.9852e-01, -1.6634e-01,  1.2309e-01,  1.3710e-01,\n",
            "          -1.5373e-01],\n",
            "         [-1.9217e-01, -2.7490e-01,  1.8501e-01,  2.4887e-01, -1.0565e-02,\n",
            "           2.2368e-01, -5.9924e-02,  1.8418e-01,  1.6392e-01,  1.6242e-01,\n",
            "          -8.5408e-02, -2.2260e-01,  1.2772e-01,  6.0125e-02, -5.6113e-02,\n",
            "           3.1684e-02]],\n",
            "\n",
            "        [[-9.4301e-03,  1.5918e-01,  6.4045e-02,  4.1158e-02,  1.8167e-01,\n",
            "           1.9097e-01, -1.3001e-01,  3.7462e-01,  4.5334e-02, -5.8969e-03,\n",
            "           1.0341e-01, -1.6892e-01, -7.7029e-02,  3.7404e-01, -4.3199e-02,\n",
            "           5.2625e-02],\n",
            "         [ 1.8439e-01,  2.3962e-01,  1.0462e-01,  3.0301e-02, -2.2004e-02,\n",
            "           2.2059e-01, -4.1119e-01,  1.0181e-01, -7.6934e-02,  4.3944e-02,\n",
            "           1.0500e-01,  4.8057e-02,  1.5766e-01, -1.1989e-01,  5.4049e-02,\n",
            "           6.9930e-03],\n",
            "         [ 3.3896e-01,  2.2190e-02,  1.9521e-01, -1.2843e-01,  5.4501e-01,\n",
            "           2.1085e-01, -2.9176e-01,  1.1123e-01,  5.4783e-02,  2.9412e-01,\n",
            "          -3.7728e-02, -9.3408e-02, -4.6159e-02,  7.9447e-02,  1.4152e-01,\n",
            "          -1.1979e-01],\n",
            "         [ 3.3686e-01, -2.7736e-01, -2.4709e-01,  8.2157e-02, -5.4695e-02,\n",
            "           1.5280e-01, -5.5295e-01,  8.5540e-02, -1.6448e-01, -8.2048e-03,\n",
            "          -2.8817e-01,  0.0000e+00,  6.0296e-02,  2.4894e-01,  1.8697e-01,\n",
            "           2.1587e-02],\n",
            "         [ 1.9337e-01,  2.0013e-01,  2.4542e-01,  7.3035e-02, -3.4774e-01,\n",
            "           2.6481e-01, -3.3470e-01,  1.7345e-01,  5.8708e-02, -4.0225e-02,\n",
            "           8.9422e-02,  4.1319e-01,  2.4074e-01, -1.0974e-01,  2.0958e-01,\n",
            "           2.1434e-01],\n",
            "         [ 1.8438e-01,  2.0815e-01,  1.2915e-01,  8.5475e-02,  2.6930e-01,\n",
            "           4.9557e-02, -2.9052e-01,  4.5419e-02,  3.7469e-02,  1.7770e-01,\n",
            "          -9.7940e-02, -1.2538e-01,  2.7985e-02,  2.6696e-01, -8.5278e-02,\n",
            "          -1.2567e-01],\n",
            "         [-1.6449e-01,  1.2690e-01, -5.6953e-02,  3.7100e-01,  1.9878e-01,\n",
            "           1.2669e-01, -1.8251e-01,  0.0000e+00, -2.1571e-01, -4.7101e-02,\n",
            "           1.5946e-01,  8.6213e-02,  1.5723e-01,  3.2024e-01, -1.1792e-01,\n",
            "          -4.4065e-01],\n",
            "         [ 3.7716e-01, -1.4516e-02,  4.3226e-02,  1.6090e-01, -6.5945e-03,\n",
            "          -4.0860e-01, -1.9911e-02,  2.5446e-02, -1.3693e-02, -5.2364e-02,\n",
            "           1.9383e-01,  2.3351e-01,  4.1979e-03, -2.1036e-01,  8.8263e-02,\n",
            "           7.5782e-02]],\n",
            "\n",
            "        [[-5.3736e-02,  2.0238e-01, -2.9431e-01, -2.6186e-01, -1.9307e-01,\n",
            "           1.8282e-02, -6.9584e-02,  9.0636e-02, -3.9961e-02,  7.2620e-02,\n",
            "          -1.5766e-01, -2.2679e-01,  3.5495e-01,  1.1732e-01,  6.4645e-02,\n",
            "          -3.1251e-02],\n",
            "         [-3.9688e-01,  2.5370e-01, -8.6413e-02, -5.5715e-01,  1.4442e-02,\n",
            "          -1.4634e-02, -2.5775e-02,  2.2946e-02,  1.2569e-01, -2.0594e-01,\n",
            "           3.0484e-01, -1.6864e-01,  3.5975e-01, -2.1907e-01, -5.4054e-01,\n",
            "          -1.9202e-01],\n",
            "         [-3.2868e-01,  1.7991e-01,  7.2260e-03, -1.6218e-01,  2.2013e-02,\n",
            "          -9.5035e-02, -2.7777e-01,  2.5525e-01,  9.7336e-02, -5.3021e-02,\n",
            "           5.2865e-01, -1.0348e-01, -5.6022e-02,  4.8166e-01, -5.4941e-02,\n",
            "          -1.9605e-01],\n",
            "         [-4.1414e-02,  1.0621e-02,  4.2473e-02, -1.9420e-01, -5.1727e-01,\n",
            "          -3.8886e-02, -1.7191e-01, -2.8249e-01,  3.0287e-01, -1.9519e-01,\n",
            "          -8.6597e-02,  1.3813e-01, -4.3964e-03,  2.2656e-01,  2.2894e-01,\n",
            "           4.2158e-02],\n",
            "         [ 5.0274e-02, -1.1375e-01, -2.7043e-01,  5.5355e-02,  1.0636e-01,\n",
            "           1.0611e-01,  0.0000e+00,  2.4668e-03,  0.0000e+00,  8.0896e-02,\n",
            "           2.5208e-02, -1.4498e-01, -6.2049e-02,  1.8749e-01,  2.2208e-01,\n",
            "          -4.8997e-02],\n",
            "         [-3.8162e-02, -4.7725e-02, -2.0747e-01, -8.5360e-02, -3.9670e-02,\n",
            "          -1.7424e-01, -4.2868e-01,  5.6050e-02,  0.0000e+00,  1.2087e-01,\n",
            "          -4.0732e-01, -2.9207e-01,  5.7558e-02,  2.5554e-01,  4.8109e-01,\n",
            "           2.6606e-01],\n",
            "         [-3.4323e-02,  1.6226e-01, -1.2476e-01,  4.6971e-01,  6.8972e-01,\n",
            "           1.3114e-01, -3.9554e-01,  1.8104e-01, -2.1592e-01,  7.1357e-02,\n",
            "           1.9578e-01,  1.9719e-02,  7.4772e-02,  2.9017e-01, -2.8594e-01,\n",
            "          -2.4104e-01],\n",
            "         [-1.1360e-01, -2.0419e-02, -1.0321e-01,  5.3823e-02, -1.3721e-01,\n",
            "           1.3663e-01, -6.8524e-02,  5.3273e-02,  1.2275e-01,  6.7142e-02,\n",
            "           0.0000e+00, -8.7126e-02,  2.8593e-01,  4.6370e-02,  1.6372e-01,\n",
            "           4.8040e-02]],\n",
            "\n",
            "        [[ 2.9000e-01,  2.9466e-01, -3.0345e-01, -6.0219e-02, -4.8090e-02,\n",
            "           1.3469e-01, -3.2794e-01, -7.6255e-02, -7.6589e-02,  2.0703e-01,\n",
            "           7.3841e-02, -2.8582e-01, -1.2940e-01,  9.5127e-02, -4.5211e-02,\n",
            "           5.2583e-02],\n",
            "         [-3.2366e-01,  2.7232e-01, -1.6774e-01,  7.8584e-02,  4.3643e-02,\n",
            "           1.1226e-01,  1.7752e-01,  5.5479e-02, -2.1350e-01, -3.1688e-01,\n",
            "          -4.2162e-03, -1.8445e-01,  1.5494e-01, -1.2115e-01, -2.2479e-02,\n",
            "          -1.5588e-01],\n",
            "         [ 2.1204e-01, -1.6785e-01, -2.0891e-01,  1.0881e-01, -1.7117e-01,\n",
            "           1.5555e-01, -1.4997e-01,  4.7959e-02, -5.2230e-02,  4.5708e-02,\n",
            "           5.6549e-02, -3.4988e-01, -4.7442e-02,  1.5066e-01, -5.0137e-02,\n",
            "           2.3971e-01],\n",
            "         [-2.5501e-01, -1.5644e-01,  9.2236e-02,  0.0000e+00,  3.0505e-04,\n",
            "           2.1975e-01,  5.1071e-02,  1.2339e-01,  6.2528e-02,  1.9150e-01,\n",
            "           0.0000e+00, -1.7600e-02, -2.0606e-02, -6.7591e-02, -8.6253e-02,\n",
            "           1.0710e-01],\n",
            "         [ 5.9930e-04, -4.3342e-02, -3.0107e-02,  4.9899e-02,  1.0368e-02,\n",
            "           4.4343e-02,  7.0780e-03,  1.1566e-01, -7.4839e-02, -6.9409e-02,\n",
            "          -6.4577e-02, -1.6975e-01,  6.5395e-02, -3.1015e-02, -7.1853e-02,\n",
            "           9.4818e-02],\n",
            "         [-1.7321e-02,  2.9427e-01, -3.1935e-01, -1.6506e-01, -1.6678e-01,\n",
            "           2.0909e-01, -2.5040e-01,  1.8513e-02,  1.1068e-01,  3.1135e-03,\n",
            "           6.9986e-02, -4.6955e-01,  1.6784e-01, -3.4844e-02,  6.7012e-02,\n",
            "          -7.0242e-02],\n",
            "         [-2.0253e-01,  6.5154e-02,  1.8120e-01,  6.2756e-02,  4.9540e-01,\n",
            "           5.6949e-02, -1.6761e-01, -2.0920e-01,  9.8035e-02,  3.4018e-01,\n",
            "          -2.5613e-01, -2.2230e-01,  5.7037e-02,  0.0000e+00, -3.0817e-02,\n",
            "          -2.7898e-02],\n",
            "         [ 3.8017e-02,  8.7414e-03,  4.5645e-02,  3.0935e-01,  2.2506e-01,\n",
            "          -4.1234e-02, -1.6929e-01,  1.7545e-01,  5.4638e-02,  1.6876e-01,\n",
            "          -1.5020e-01,  8.5737e-02, -7.0104e-02, -1.1632e-01,  2.2392e-01,\n",
            "          -1.5056e-01]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMoE+MultiHeadAttention\n",
        "\n",
        "![](https://raw.githubusercontent.com/weedge/baby-llm/main/docs/moe-self-attention.jpg)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZSogXMmcezoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "根据 router 返回的 top-k 门控值（gating values），计算批量级别的门控值(batch_gates)"
      ],
      "metadata": {
        "id": "Sbq09zNhAeZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "block_size = 4\n",
        "n_embd = 16 # hidden_size\n",
        "\n",
        "# fake hidden states randn tensor B(batch_size),T(block_size),C(n_embd)\n",
        "fake_hidden_states = torch.randn(batch_size, block_size, n_embd)\n",
        "print(fake_hidden_states.shape)\n",
        "\n",
        "#Causal scaled dot product self-Attention Head\n",
        "n_head = 4\n",
        "n_layer = 2\n",
        "head_size:int = 4 # n_embd/n_head\n",
        "dropout = 0.1\n",
        "\n",
        "# Sparse Top-K gating router + Experts (SMoE)\n",
        "num_experts = 8\n",
        "top_k = 2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyvizzbG9PSq",
        "outputId": "9b643e88-c39e-4761-d14d-b4dd45da2206"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.jit.script\n",
        "def compute_gating(k: int, num_experts: int, top_k_gates: torch.Tensor, top_k_indices: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Compute gating values for the mixture of experts based on probabilities and top-k indices.\n",
        "\n",
        "    Args:\n",
        "        k (int): Number of experts to select.\n",
        "        num_experts (int): Total number of experts.\n",
        "        top_k_gates (torch.Tensor): Gating values for top-k experts (batch_size x k).\n",
        "        top_k_indices (torch.Tensor): Indices of top-k experts (batch_size x k).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Batch-level gating values.\n",
        "        torch.Tensor: Batch-level expert indices.\n",
        "        torch.Tensor: Expert size for each expert.\n",
        "        torch.Tensor: Sorted indices of top-k experts.\n",
        "    \"\"\"\n",
        "    zeros = torch.zeros([top_k_gates.size(0), num_experts],\n",
        "                        dtype=top_k_gates.dtype, device=top_k_gates.device)\n",
        "    gates = zeros.scatter(-1, top_k_indices, 1)\n",
        "    print(gates)\n",
        "    # 计算每个专家被选择的次数，即每列中值为 1 的数量，得到专家大小（expert_size）。\n",
        "    expert_size = gates.long().sum(0)\n",
        "    print(expert_size)\n",
        "    # 将顶部 k 个专家的门控值和索引展平为一维张量，并对专家索引进行排序。\n",
        "    top_k_gates = top_k_gates.flatten()\n",
        "    #print(top_k_gates)\n",
        "    top_k_experts = top_k_indices.flatten()\n",
        "    _, index_sorted_experts = top_k_experts.sort(0)\n",
        "\n",
        "    # 根据专家索引的排序结果，确定每个样本所属的批次索引（batch_index）。\n",
        "    # 将排序后的索引张量 index_sorted_experts 中的每个元素除以一个标量 k，\n",
        "    # 并指定舍入模式为“截断”（truncation）。这意味着将索引除以 k 后取整数部分，舍去小数部分\n",
        "    batch_index = index_sorted_experts.div(k, rounding_mode=\"trunc\")\n",
        "    # 提取排序后的专家门控值，得到批次级别的门控值（batch_gates）。\n",
        "    batch_gates = top_k_gates[index_sorted_experts]\n",
        "\n",
        "    return batch_gates, batch_index, expert_size, index_sorted_experts"
      ],
      "metadata": {
        "id": "EeKOPuHsh7bt"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test noisy top-k gate router compute_gating method\n",
        "\n",
        "input=fake_hidden_states.reshape(-1, n_embd)# B*T, C\n",
        "print(f\"input.shape:{input.shape}\")\n",
        "\n",
        "noisy_top_k_gate = NoisyTopkRouter(n_embd, num_experts, top_k)\n",
        "#noisy_top_k_gate.eval()\n",
        "noisy_top_k_gate.training = True #default True\n",
        "\n",
        "top_k_gates, top_k_indices = noisy_top_k_gate(input)\n",
        "print(f\"aux_loss:{noisy_top_k_gate.aux_loss}\")\n",
        "print(top_k_gates.shape, top_k_gates)\n",
        "print(top_k_indices.shape, top_k_indices)\n",
        "\n",
        "\n",
        "batch_gates, batch_index, expert_size, index_sorted_experts = compute_gating(\n",
        "  top_k, num_experts, top_k_gates, top_k_indices\n",
        ")\n",
        "print(f\"batch_gates:{batch_gates}\")\n",
        "print(f\"batch_index:{batch_index}\")\n",
        "print(f\"expert_size:{expert_size}\")\n",
        "print(f\"index_sorted_experts:{index_sorted_experts}\")\n",
        "expert_size = expert_size.tolist()\n",
        "print(f\"expert_size:{expert_size}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slw_ESHvBEDI",
        "outputId": "62798fde-53a9-4864-e6cd-ff5abdfa8860"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input.shape:torch.Size([8, 16])\n",
            "16 8 2\n",
            "mh_output.shape:torch.Size([8, 16])\n",
            "logits.shape:torch.Size([8, 8])\n",
            "aux_loss:1.65873122215271\n",
            "torch.Size([8, 8]) tensor([[0.6291, 0.0000, 0.0000, 0.0000, 0.3709, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5793, 0.0000, 0.4207],\n",
            "        [0.0000, 0.0000, 0.0000, 0.4442, 0.0000, 0.0000, 0.5558, 0.0000],\n",
            "        [0.0000, 0.0000, 0.4570, 0.5430, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.2763, 0.0000, 0.7237, 0.0000],\n",
            "        [0.0000, 0.0000, 0.7778, 0.0000, 0.0000, 0.2222, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.6454, 0.0000, 0.3546, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.5659, 0.0000, 0.0000, 0.4341, 0.0000]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "torch.Size([8, 2]) tensor([[0, 4],\n",
            "        [5, 7],\n",
            "        [6, 3],\n",
            "        [3, 2],\n",
            "        [6, 4],\n",
            "        [2, 5],\n",
            "        [4, 6],\n",
            "        [3, 6]])\n",
            " 1  0  0  0  1  0  0  0\n",
            " 0  0  0  0  0  1  0  1\n",
            " 0  0  0  1  0  0  1  0\n",
            " 0  0  1  1  0  0  0  0\n",
            " 0  0  0  0  1  0  1  0\n",
            " 0  0  1  0  0  1  0  0\n",
            " 0  0  0  0  1  0  1  0\n",
            " 0  0  0  1  0  0  1  0\n",
            "[ CPUFloatType{8,8} ]\n",
            " 1\n",
            " 0\n",
            " 2\n",
            " 3\n",
            " 3\n",
            " 2\n",
            " 4\n",
            " 1\n",
            "[ CPULongType{8} ]\n",
            "batch_gates:tensor([0.6291, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.3709, 0.0000, 0.5793, 0.4207, 0.0000],\n",
            "       grad_fn=<IndexBackward0>)\n",
            "batch_index:tensor([0, 3, 5, 2, 3, 7, 0, 4, 6, 1, 5, 2, 4, 6, 7, 1])\n",
            "expert_size:tensor([1, 0, 2, 3, 3, 2, 4, 1])\n",
            "index_sorted_experts:tensor([ 0,  7, 10,  5,  6, 14,  1,  9, 12,  2, 11,  4,  8, 13, 15,  3])\n",
            "expert_size:[1, 0, 2, 3, 3, 2, 4, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用并行专家处理，分为输入并行专家(专家数目num_experts)和输出并行专家(专家数目num_experts)，\n",
        "\n",
        "初始化专家的参数权重W B,T,C（num_experts, output_size, input_size）； 并初始化为在 [-1/output_size, 1/output_size] 范围内的均匀分布的随机值"
      ],
      "metadata": {
        "id": "jwqGtYwlvuBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# init weight\n",
        "assert n_head % top_k == 0\n",
        "num_key_value_heads = int(n_head/top_k)\n",
        "print(f\"num_key_value_heads:{num_key_value_heads}\")\n",
        "kv_proj_size=num_key_value_heads*head_size\n",
        "print(f\"kv_proj_size:{kv_proj_size}\")\n",
        "weight = nn.Parameter(torch.empty(num_experts, kv_proj_size, n_embd))\n",
        "print(f\"weight.shape:{weight.shape}\")\n",
        "#print(weight)\n",
        "nn.init.uniform_(weight, -1.0 / weight.size(1), 1.0 / weight.size(1))\n",
        "print(weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsJ6vZrUw5Ey",
        "outputId": "f6f9953f-65cb-4cbb-c289-37c26c9cef3e"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_key_value_heads:2\n",
            "kv_proj_size:8\n",
            "weight.shape:torch.Size([8, 8, 16])\n",
            "Parameter containing:\n",
            "tensor([[[ 0.1162, -0.0343,  0.0675,  ..., -0.0964,  0.0922,  0.0681],\n",
            "         [ 0.0016, -0.0472, -0.0787,  ...,  0.0549, -0.0476, -0.0060],\n",
            "         [-0.0668,  0.1190,  0.0771,  ..., -0.0331,  0.1091, -0.1133],\n",
            "         ...,\n",
            "         [-0.0979,  0.1120,  0.0739,  ..., -0.1052, -0.0721,  0.1052],\n",
            "         [-0.0765, -0.0952,  0.0303,  ..., -0.1188, -0.1188,  0.0708],\n",
            "         [ 0.0260,  0.0751, -0.1059,  ...,  0.1188,  0.0343, -0.0411]],\n",
            "\n",
            "        [[-0.0151, -0.1161, -0.1086,  ...,  0.0219,  0.0250, -0.0181],\n",
            "         [-0.0239, -0.1244,  0.0143,  ...,  0.0865, -0.0011,  0.1112],\n",
            "         [ 0.0210, -0.0955, -0.0176,  ...,  0.1119, -0.1043,  0.0331],\n",
            "         ...,\n",
            "         [-0.1228, -0.1043,  0.0418,  ..., -0.0954, -0.0621,  0.1240],\n",
            "         [-0.0826, -0.0012,  0.0303,  ...,  0.0408, -0.0891, -0.0392],\n",
            "         [-0.0761, -0.0749,  0.1163,  ...,  0.0658,  0.0944, -0.0178]],\n",
            "\n",
            "        [[ 0.0954, -0.1104,  0.1174,  ...,  0.0237, -0.0864, -0.1239],\n",
            "         [-0.0571,  0.0281, -0.0889,  ...,  0.0934,  0.0859, -0.1216],\n",
            "         [-0.0883, -0.0994, -0.1247,  ...,  0.0243,  0.0102, -0.0725],\n",
            "         ...,\n",
            "         [ 0.0546, -0.1190, -0.0390,  ...,  0.1087, -0.0714,  0.0769],\n",
            "         [-0.0390,  0.0362,  0.1235,  ..., -0.0436, -0.0118,  0.0388],\n",
            "         [ 0.0300,  0.1177, -0.1141,  ...,  0.0378,  0.0477,  0.0373]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0989, -0.1153,  0.1166,  ...,  0.0142, -0.1132,  0.1234],\n",
            "         [-0.0709,  0.1068, -0.0023,  ...,  0.0332, -0.0234,  0.1178],\n",
            "         [-0.0479, -0.0153,  0.0320,  ...,  0.0636, -0.1231,  0.0137],\n",
            "         ...,\n",
            "         [-0.0417, -0.0202, -0.1012,  ...,  0.0083, -0.0902,  0.0771],\n",
            "         [ 0.0387, -0.0927, -0.0428,  ...,  0.0834, -0.0937, -0.1200],\n",
            "         [-0.0413, -0.0906, -0.0895,  ..., -0.0667,  0.0577,  0.0619]],\n",
            "\n",
            "        [[ 0.1052,  0.0827,  0.0584,  ...,  0.0359, -0.1051,  0.0805],\n",
            "         [ 0.0455, -0.0898, -0.0226,  ..., -0.0444, -0.1102, -0.1033],\n",
            "         [ 0.0208, -0.0763, -0.1003,  ...,  0.1059,  0.1056, -0.1112],\n",
            "         ...,\n",
            "         [ 0.0714, -0.0809,  0.0038,  ...,  0.0380,  0.0378, -0.0408],\n",
            "         [-0.0668, -0.0464,  0.1207,  ..., -0.1211, -0.0656,  0.0384],\n",
            "         [-0.0420,  0.0507, -0.0068,  ...,  0.0216, -0.0922,  0.0512]],\n",
            "\n",
            "        [[-0.0796, -0.0419, -0.0675,  ..., -0.0493, -0.0461, -0.0596],\n",
            "         [-0.1214, -0.0725, -0.0829,  ..., -0.0287,  0.1167, -0.0774],\n",
            "         [ 0.0242, -0.0929, -0.0981,  ...,  0.0901, -0.0608,  0.0617],\n",
            "         ...,\n",
            "         [-0.0015,  0.1188, -0.0166,  ..., -0.1099, -0.0282,  0.1117],\n",
            "         [-0.0144,  0.0614, -0.0536,  ...,  0.0143,  0.1001,  0.1050],\n",
            "         [ 0.0350, -0.0242, -0.1117,  ..., -0.0748, -0.0916,  0.0978]]],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "expert_inputs = input[batch_index]\n",
        "print(expert_inputs.shape)\n",
        "print(expert_inputs)\n",
        "\n",
        "print(f\"expert_size:{expert_size}\")\n",
        "#在指定维度 dim=0 上将输入张量 expert_inputs 按照给定的尺寸 expert_size 进行分割，然后返回分割后的子张量列表\n",
        "input_list = expert_inputs.split(expert_size, dim=0)# return tuple\n",
        "print(input_list)\n",
        "\n",
        "output_list = []\n",
        "for i in range(num_experts):\n",
        "    print(f\"input_list[{i}].shape:{input_list[i].shape}\")\n",
        "    print(f\"weight[{i}].shape:{weight[i].shape}\")\n",
        "    # B = A*W^T A:(N, in_features) W:(out_features, in_features) => B:(N,out_features)\n",
        "    output=F.linear(input_list[i], weight[i])\n",
        "    print(f\"output.shape:{output.shape}\")\n",
        "    output_list.append(output)\n",
        "\n",
        "# 张量的形状在除了连接维度 dim=0 外都是一致的，那么这个操作会将这些张量沿着第一个维度 进行连接，形成一个新的张量。\n",
        "# 如果连接的张量在其他维度的大小不同，那么连接操作会失败\n",
        "results = torch.cat(output_list, dim=0)\n",
        "print(results.shape)\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjLTmFUR7CZZ",
        "outputId": "99c6931f-727f-494a-e881-0c0e22445047"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 16])\n",
            "tensor([[ 0.6275, -0.2494, -0.9234, -0.1392, -0.6819, -1.2787,  0.4516, -1.2711,\n",
            "         -1.0155, -1.0718,  1.7033,  0.3927, -0.4351, -0.7506,  0.6580,  0.1792],\n",
            "        [-1.0995,  0.4704,  0.3262, -0.4887, -0.5590,  1.0151, -1.4383,  1.2668,\n",
            "         -0.5727, -0.5563, -0.0041,  0.6063, -0.3828, -0.6839,  0.1887, -2.0636],\n",
            "        [-0.2277, -0.2768, -0.9280,  0.1068, -1.1972, -0.8575, -0.4312,  0.1373,\n",
            "         -1.0293, -0.1143, -0.7472,  1.0169,  1.0057,  1.7110,  0.5539, -0.2593],\n",
            "        [-2.7094,  1.2737, -0.4098,  0.8823, -1.4135,  1.4640,  0.1716, -0.9132,\n",
            "         -0.5465,  0.8197,  0.2410,  1.5406, -1.2586, -0.4886, -0.7066,  0.4511],\n",
            "        [-1.5670,  1.0781, -1.2979, -1.1760, -0.3268, -0.5293, -0.7467,  0.5153,\n",
            "         -1.6209,  0.9633, -1.0558, -0.6734, -0.1359, -0.2297, -1.8250,  0.0945],\n",
            "        [-0.2609,  0.5300, -0.3991,  1.2225, -1.1739, -0.7944,  2.2792, -0.0253,\n",
            "          0.4196, -1.0681,  2.0502, -1.3337,  0.2271, -1.4689,  0.7892,  0.3918],\n",
            "        [-1.0995,  0.4704,  0.3262, -0.4887, -0.5590,  1.0151, -1.4383,  1.2668,\n",
            "         -0.5727, -0.5563, -0.0041,  0.6063, -0.3828, -0.6839,  0.1887, -2.0636],\n",
            "        [-1.5670,  1.0781, -1.2979, -1.1760, -0.3268, -0.5293, -0.7467,  0.5153,\n",
            "         -1.6209,  0.9633, -1.0558, -0.6734, -0.1359, -0.2297, -1.8250,  0.0945],\n",
            "        [-2.7094,  1.2737, -0.4098,  0.8823, -1.4135,  1.4640,  0.1716, -0.9132,\n",
            "         -0.5465,  0.8197,  0.2410,  1.5406, -1.2586, -0.4886, -0.7066,  0.4511],\n",
            "        [ 0.4133, -0.1904, -0.2118,  0.3013, -0.8913, -0.6195,  0.1404,  0.3688,\n",
            "          0.7055,  0.4657, -0.4813, -1.0145, -0.2254, -0.6617, -1.0694,  0.8874],\n",
            "        [ 0.6275, -0.2494, -0.9234, -0.1392, -0.6819, -1.2787,  0.4516, -1.2711,\n",
            "         -1.0155, -1.0718,  1.7033,  0.3927, -0.4351, -0.7506,  0.6580,  0.1792],\n",
            "        [-0.5095,  0.1873, -0.8453, -0.5449, -0.4811, -2.4356, -0.6019,  1.2318,\n",
            "          0.3009, -0.3978, -1.3151, -0.1928,  0.1329,  0.1632, -1.2297,  0.3681],\n",
            "        [ 0.4133, -0.1904, -0.2118,  0.3013, -0.8913, -0.6195,  0.1404,  0.3688,\n",
            "          0.7055,  0.4657, -0.4813, -1.0145, -0.2254, -0.6617, -1.0694,  0.8874],\n",
            "        [-0.2277, -0.2768, -0.9280,  0.1068, -1.1972, -0.8575, -0.4312,  0.1373,\n",
            "         -1.0293, -0.1143, -0.7472,  1.0169,  1.0057,  1.7110,  0.5539, -0.2593],\n",
            "        [-0.2609,  0.5300, -0.3991,  1.2225, -1.1739, -0.7944,  2.2792, -0.0253,\n",
            "          0.4196, -1.0681,  2.0502, -1.3337,  0.2271, -1.4689,  0.7892,  0.3918],\n",
            "        [-0.5095,  0.1873, -0.8453, -0.5449, -0.4811, -2.4356, -0.6019,  1.2318,\n",
            "          0.3009, -0.3978, -1.3151, -0.1928,  0.1329,  0.1632, -1.2297,  0.3681]])\n",
            "expert_size:[5, 1, 2, 1, 1, 2, 1, 3]\n",
            "(tensor([[ 0.6275, -0.2494, -0.9234, -0.1392, -0.6819, -1.2787,  0.4516, -1.2711,\n",
            "         -1.0155, -1.0718,  1.7033,  0.3927, -0.4351, -0.7506,  0.6580,  0.1792],\n",
            "        [-1.0995,  0.4704,  0.3262, -0.4887, -0.5590,  1.0151, -1.4383,  1.2668,\n",
            "         -0.5727, -0.5563, -0.0041,  0.6063, -0.3828, -0.6839,  0.1887, -2.0636],\n",
            "        [-0.2277, -0.2768, -0.9280,  0.1068, -1.1972, -0.8575, -0.4312,  0.1373,\n",
            "         -1.0293, -0.1143, -0.7472,  1.0169,  1.0057,  1.7110,  0.5539, -0.2593],\n",
            "        [-2.7094,  1.2737, -0.4098,  0.8823, -1.4135,  1.4640,  0.1716, -0.9132,\n",
            "         -0.5465,  0.8197,  0.2410,  1.5406, -1.2586, -0.4886, -0.7066,  0.4511],\n",
            "        [-1.5670,  1.0781, -1.2979, -1.1760, -0.3268, -0.5293, -0.7467,  0.5153,\n",
            "         -1.6209,  0.9633, -1.0558, -0.6734, -0.1359, -0.2297, -1.8250,  0.0945]]), tensor([[-0.2609,  0.5300, -0.3991,  1.2225, -1.1739, -0.7944,  2.2792, -0.0253,\n",
            "          0.4196, -1.0681,  2.0502, -1.3337,  0.2271, -1.4689,  0.7892,  0.3918]]), tensor([[-1.0995,  0.4704,  0.3262, -0.4887, -0.5590,  1.0151, -1.4383,  1.2668,\n",
            "         -0.5727, -0.5563, -0.0041,  0.6063, -0.3828, -0.6839,  0.1887, -2.0636],\n",
            "        [-1.5670,  1.0781, -1.2979, -1.1760, -0.3268, -0.5293, -0.7467,  0.5153,\n",
            "         -1.6209,  0.9633, -1.0558, -0.6734, -0.1359, -0.2297, -1.8250,  0.0945]]), tensor([[-2.7094,  1.2737, -0.4098,  0.8823, -1.4135,  1.4640,  0.1716, -0.9132,\n",
            "         -0.5465,  0.8197,  0.2410,  1.5406, -1.2586, -0.4886, -0.7066,  0.4511]]), tensor([[ 0.4133, -0.1904, -0.2118,  0.3013, -0.8913, -0.6195,  0.1404,  0.3688,\n",
            "          0.7055,  0.4657, -0.4813, -1.0145, -0.2254, -0.6617, -1.0694,  0.8874]]), tensor([[ 0.6275, -0.2494, -0.9234, -0.1392, -0.6819, -1.2787,  0.4516, -1.2711,\n",
            "         -1.0155, -1.0718,  1.7033,  0.3927, -0.4351, -0.7506,  0.6580,  0.1792],\n",
            "        [-0.5095,  0.1873, -0.8453, -0.5449, -0.4811, -2.4356, -0.6019,  1.2318,\n",
            "          0.3009, -0.3978, -1.3151, -0.1928,  0.1329,  0.1632, -1.2297,  0.3681]]), tensor([[ 0.4133, -0.1904, -0.2118,  0.3013, -0.8913, -0.6195,  0.1404,  0.3688,\n",
            "          0.7055,  0.4657, -0.4813, -1.0145, -0.2254, -0.6617, -1.0694,  0.8874]]), tensor([[-0.2277, -0.2768, -0.9280,  0.1068, -1.1972, -0.8575, -0.4312,  0.1373,\n",
            "         -1.0293, -0.1143, -0.7472,  1.0169,  1.0057,  1.7110,  0.5539, -0.2593],\n",
            "        [-0.2609,  0.5300, -0.3991,  1.2225, -1.1739, -0.7944,  2.2792, -0.0253,\n",
            "          0.4196, -1.0681,  2.0502, -1.3337,  0.2271, -1.4689,  0.7892,  0.3918],\n",
            "        [-0.5095,  0.1873, -0.8453, -0.5449, -0.4811, -2.4356, -0.6019,  1.2318,\n",
            "          0.3009, -0.3978, -1.3151, -0.1928,  0.1329,  0.1632, -1.2297,  0.3681]]))\n",
            "input_list[0].shape:torch.Size([5, 16])\n",
            "weight[0].shape:torch.Size([8, 16])\n",
            "output.shape:torch.Size([5, 8])\n",
            "input_list[1].shape:torch.Size([1, 16])\n",
            "weight[1].shape:torch.Size([8, 16])\n",
            "output.shape:torch.Size([1, 8])\n",
            "input_list[2].shape:torch.Size([2, 16])\n",
            "weight[2].shape:torch.Size([8, 16])\n",
            "output.shape:torch.Size([2, 8])\n",
            "input_list[3].shape:torch.Size([1, 16])\n",
            "weight[3].shape:torch.Size([8, 16])\n",
            "output.shape:torch.Size([1, 8])\n",
            "input_list[4].shape:torch.Size([1, 16])\n",
            "weight[4].shape:torch.Size([8, 16])\n",
            "output.shape:torch.Size([1, 8])\n",
            "input_list[5].shape:torch.Size([2, 16])\n",
            "weight[5].shape:torch.Size([8, 16])\n",
            "output.shape:torch.Size([2, 8])\n",
            "input_list[6].shape:torch.Size([1, 16])\n",
            "weight[6].shape:torch.Size([8, 16])\n",
            "output.shape:torch.Size([1, 8])\n",
            "input_list[7].shape:torch.Size([3, 16])\n",
            "weight[7].shape:torch.Size([8, 16])\n",
            "output.shape:torch.Size([3, 8])\n",
            "torch.Size([16, 8])\n",
            "tensor([[ 0.1069, -0.3511, -0.0357,  0.1286,  0.1875,  0.3863,  0.3417,  0.0422],\n",
            "        [-0.3201, -0.2135,  0.4693,  0.0424, -0.2133, -0.0631, -0.0633,  0.0364],\n",
            "        [-0.0504, -0.1192,  0.0372,  0.0736,  0.2469, -0.1159, -0.3175,  0.2811],\n",
            "        [-0.3447, -0.1431,  0.1691, -0.0238,  0.2409,  0.5962,  0.4827, -0.0619],\n",
            "        [-0.4495,  0.0237,  0.3056,  0.0013,  0.0838,  0.2926,  0.3174, -0.1023],\n",
            "        [ 0.3788,  0.1854,  0.3682, -0.2299,  0.3040,  0.3442,  0.0297, -0.3773],\n",
            "        [ 0.2940,  0.5804, -0.1251, -0.2825,  0.1748, -0.0135,  0.3603,  0.0763],\n",
            "        [-0.2987,  0.1153,  0.0879, -0.0341,  0.3732, -0.0834,  0.2425,  0.3176],\n",
            "        [ 0.2384, -0.2355,  0.4666,  0.0551,  0.3072, -0.0710,  0.1343,  0.3821],\n",
            "        [-0.1740,  0.0123,  0.0500, -0.3279,  0.0456, -0.1562, -0.0819, -0.2364],\n",
            "        [-0.0225,  0.0210,  0.0476, -0.3928, -0.0232,  0.0877, -0.1903, -0.2502],\n",
            "        [ 0.1496, -0.1022,  0.0766, -0.3484,  0.5833,  0.3489, -0.2548, -0.2059],\n",
            "        [ 0.3251,  0.0970, -0.3982,  0.0760,  0.1558, -0.0914,  0.0118, -0.0675],\n",
            "        [-0.0054,  0.3648, -0.1261,  0.1781, -0.0621, -0.3738,  0.1548, -0.0525],\n",
            "        [-0.6939,  0.4805,  0.0132,  0.1731,  0.1252,  0.4971, -0.0988, -0.2368],\n",
            "        [ 0.0730, -0.3661, -0.1599, -0.1040,  0.0325,  0.1117, -0.4064,  0.0418]],\n",
            "       grad_fn=<CatBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "封装成ParallelExperts"
      ],
      "metadata": {
        "id": "abujKmLoLNus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ParallelExperts(nn.Module):\n",
        "    def __init__(self, num_experts, input_size, output_size) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the ParallelExperts module.\n",
        "        like a Expert pool\n",
        "        maybe manager diff export pool for feature to load :)\n",
        "\n",
        "        Args:\n",
        "            num_experts (int): Number of experts.\n",
        "            input_size (int): Size of the input.\n",
        "            output_size (int): Size of the output.\n",
        "            bias (bool): Whether to include bias terms.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.empty(\n",
        "            num_experts, output_size, input_size))\n",
        "        self.reset_parameters()\n",
        "        self.num_experts = num_experts\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return \"num_experts={}, input_size={}, output_size={}\".format(\n",
        "            self.num_experts, self.input_size, self.output_size\n",
        "        )\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        \"\"\"\n",
        "        Reset the parameters of the model.\n",
        "        \"\"\"\n",
        "        nn.init.uniform_(self.weight, -1.0 / self.weight.size(1),\n",
        "                         1.0 / self.weight.size(1))\n",
        "\n",
        "    def forward(self, inputs, expert_size):\n",
        "        \"\"\"\n",
        "        Forward pass of the ParallelExperts module.\n",
        "\n",
        "        Args:\n",
        "            inputs (Tensor): Input tensor.\n",
        "            expert_size: Expert size information.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Output tensor.\n",
        "        \"\"\"\n",
        "        input_list = inputs.split(expert_size, dim=0)# return tuple\n",
        "        output_list = []\n",
        "        for i in range(self.num_experts):\n",
        "            output_list.append(F.linear(input_list[i], self.weight[i]))\n",
        "        results = torch.cat(output_list, dim=0)\n",
        "        return results"
      ],
      "metadata": {
        "id": "uxhWvVveh2wp"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert n_head % top_k == 0\n",
        "#num_key_value_heads = int(n_head/top_k)\n",
        "print(f\"num_key_value_heads:{num_key_value_heads}\")\n",
        "#kv_proj_size=num_key_value_heads*head_size\n",
        "print(f\"kv_proj_size:{kv_proj_size}\")\n",
        "\n",
        "parallel_experts = ParallelExperts(num_experts, input_size=n_embd, output_size=kv_proj_size)\n",
        "#expert_inputs = input[batch_index]\n",
        "#print(expert_inputs.shape)\n",
        "#print(expert_inputs)\n",
        "\n",
        "results = parallel_experts(expert_inputs, expert_size)\n",
        "print(results.shape)\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hFlf0IU0kQt",
        "outputId": "acd5152e-75e8-4672-f496-10246f3a2566"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_key_value_heads:2\n",
            "kv_proj_size:8\n",
            "torch.Size([16, 8])\n",
            "tensor([[ 5.7369e-02, -2.7929e-01, -6.2102e-01,  1.4588e-01,  1.5471e-01,\n",
            "         -2.1183e-01, -6.1586e-01,  2.2009e-01],\n",
            "        [ 3.6758e-02,  4.7301e-02,  2.0505e-01, -1.5489e-01, -9.4189e-02,\n",
            "         -4.6538e-02,  2.6035e-02,  1.4479e-01],\n",
            "        [-4.8403e-01, -1.3046e-01, -2.6872e-01, -2.0892e-01, -2.1421e-01,\n",
            "         -1.2554e-04, -5.1007e-02,  4.3072e-01],\n",
            "        [-1.0765e-01,  2.7371e-01,  5.0974e-02, -1.0747e-01, -4.5881e-01,\n",
            "          1.7182e-01,  5.8526e-01, -1.9386e-01],\n",
            "        [-2.7319e-01, -4.8780e-01,  1.0511e-01, -8.3513e-01, -4.7752e-01,\n",
            "          1.5330e-01, -6.3043e-02,  2.7391e-01],\n",
            "        [ 6.4076e-01,  3.2031e-01,  4.3063e-01,  1.6182e-01,  1.4516e-01,\n",
            "         -2.2927e-01,  2.9131e-01, -7.2021e-03],\n",
            "        [-3.1115e-01, -5.2544e-02, -9.6139e-02, -2.6856e-01,  3.3494e-01,\n",
            "         -1.9269e-01, -1.9188e-01, -2.4091e-01],\n",
            "        [-2.4421e-01,  7.2130e-02,  7.0269e-02,  1.5036e-02, -4.0587e-01,\n",
            "         -3.4576e-01,  1.4677e-01, -2.8714e-01],\n",
            "        [-6.2935e-01,  3.8552e-01,  1.1533e-02, -1.6528e-01,  5.2873e-01,\n",
            "         -2.7161e-02, -5.4414e-01,  4.7407e-02],\n",
            "        [-9.4096e-02,  1.6853e-01, -2.0569e-01, -8.9769e-02,  1.3962e-01,\n",
            "         -3.3980e-01, -1.9630e-01, -1.0843e-01],\n",
            "        [-2.3510e-02,  6.7903e-01, -2.9926e-01,  4.0091e-02, -2.5442e-01,\n",
            "         -2.4079e-01,  3.5699e-01, -3.2560e-01],\n",
            "        [ 1.3892e-01, -6.6374e-01, -2.8034e-01,  2.7732e-01,  2.4238e-01,\n",
            "          4.9999e-02,  1.1367e-01,  9.0833e-02],\n",
            "        [-4.5918e-01,  5.7487e-02, -3.1154e-02,  9.6205e-02,  4.4895e-02,\n",
            "         -8.7207e-02,  1.6011e-01, -3.8711e-01],\n",
            "        [ 2.1407e-01, -1.4165e-01, -1.3006e-01, -3.9975e-01,  1.9815e-01,\n",
            "          1.4084e-01, -3.0251e-01,  4.8415e-02],\n",
            "        [ 5.2414e-01, -1.2837e-02,  1.3457e-01,  7.5807e-01, -1.4042e-01,\n",
            "         -6.0593e-02,  7.8260e-02,  2.3274e-01],\n",
            "        [ 4.7140e-01, -2.2729e-02, -4.9671e-01, -1.2778e-01,  1.9008e-01,\n",
            "          1.6902e-01, -5.5219e-01,  2.4748e-01]], grad_fn=<CatBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "输入 hidden states 对其 map操作 输出 Q\n",
        "\n",
        "QKV 进行 SDPA 输出 对其 reduce操作 输出 attetion_output\n",
        "\n",
        "整体封装成 SparseMoEMultiHeadAttention， 相关map/reduce 操作见代码注释:"
      ],
      "metadata": {
        "id": "8lqyPBvfPwjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SparseMoEMultiHeadAttention(nn.Module):\n",
        "    \"\"\" spare moe + multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size, n_embed, block_size, dropout, num_experts=8, top_k=2, reduce_bias=True):\n",
        "        super(SparseMoEMultiHeadAttention, self).__init__()\n",
        "\n",
        "        # 偏置是可学习的参数，通常用于线性层（如全连接层）和卷积层中。\n",
        "        # 模型中引入偏置项，有助于模型更好地拟合训练数据和提高模型的表达能力\n",
        "        # 在训练过程中，模型会通过梯度下降等优化算法自动学习到合适的偏置值，从而使模型的预测更准确。\n",
        "        self.p_reduce_bias = None\n",
        "        if reduce_bias:\n",
        "            self.p_reduce_bias = torch.nn.Parameter(torch.empty(n_embed))\n",
        "            #一定要记住初始化zeros\n",
        "            torch.nn.init.zeros_(self.p_reduce_bias)\n",
        "\n",
        "        self.n_embed = n_embed\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = head_size\n",
        "        self.num_experts = num_experts\n",
        "        self.top_k = min(top_k, self.num_experts)\n",
        "\n",
        "        assert self.top_k > 0, f\"topk must > 0\"\n",
        "        assert self.num_heads > 0, f\"num_heads must > 0\"\n",
        "        assert num_heads % \\\n",
        "            self.top_k == 0, f\"need num_heads:{num_heads}%top_k:{self.top_k} == 0\"\n",
        "\n",
        "        self.num_key_val_heads = int(num_heads/top_k)\n",
        "        self.kv_proj_size = self.num_key_val_heads*head_size\n",
        "\n",
        "        self.input_linear = ParallelExperts(\n",
        "            num_experts, n_embed, self.kv_proj_size)\n",
        "        self.output_linear = ParallelExperts(\n",
        "            num_experts, self.kv_proj_size, n_embed)\n",
        "\n",
        "        self.router = NoisyTopkRouter(n_embed, num_experts, self.top_k)\n",
        "\n",
        "        self.k_proj = torch.nn.Linear(\n",
        "            n_embed, self.kv_proj_size, bias=False)\n",
        "        self.v_proj = torch.nn.Linear(\n",
        "            n_embed, self.kv_proj_size, bias=False)\n",
        "\n",
        "        self.register_buffer('tril', torch.tril(\n",
        "            torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        bsz, seq_len, feat_dim = x.size()\n",
        "\n",
        "        query_states = self.map(x)\n",
        "        key_states = self.k_proj(x)\n",
        "        value_states = self.v_proj(x)\n",
        "\n",
        "        query_states = query_states.view(\n",
        "            bsz, seq_len, self.num_heads, self.head_size\n",
        "        ).transpose(1, 2)\n",
        "        key_states = key_states.view(\n",
        "            bsz, seq_len, self.num_key_val_heads, self.head_size\n",
        "        ).transpose(1, 2)\n",
        "        value_states = value_states.view(\n",
        "            bsz, seq_len, self.num_key_val_heads, self.head_size\n",
        "        ).transpose(1, 2)\n",
        "\n",
        "        kv_seq_len = key_states.shape[2]  # seq_len\n",
        "\n",
        "        # repeat k/v heads if num_key_val_heads < num_heads, it's true\n",
        "        key_states = key_states.repeat(1, self.top_k, 1, 1)\n",
        "        value_states = value_states.repeat(1, self.top_k, 1, 1)\n",
        "\n",
        "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(\n",
        "            self.head_size\n",
        "        )\n",
        "        if attn_weights.size() != (bsz, self.num_heads, seq_len, kv_seq_len):\n",
        "            raise ValueError(\n",
        "                f\"Attention weights should be of size {(bsz, self.num_heads, seq_len, kv_seq_len)}, but is\"\n",
        "                f\" {attn_weights.size()}\"\n",
        "            )\n",
        "\n",
        "        # upcast attention to fp32\n",
        "        attn_weights = nn.functional.softmax(\n",
        "            attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
        "        # attn_weights = self.dropout(attn_weights)\n",
        "        # attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
        "\n",
        "        attn_output = torch.matmul(attn_weights, value_states)\n",
        "        if attn_output.size() != (bsz, self.num_heads, seq_len, self.head_size):\n",
        "            raise ValueError(\n",
        "                f\"`attn_output` should be of size {(bsz, self.num_heads, seq_len, self.head_size)}, but is\"\n",
        "                f\" {attn_output.size()}\"\n",
        "            )\n",
        "\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        attn_output = attn_output.reshape(\n",
        "            bsz, seq_len, self.top_k, self.kv_proj_size)\n",
        "\n",
        "        attn_output = self.reduce(attn_output)\n",
        "        attn_output = attn_output.view(bsz, seq_len, -1)\n",
        "\n",
        "        attn_output = self.dropout(attn_output)\n",
        "\n",
        "        return attn_output\n",
        "        # return attn_output, attn_weights\n",
        "\n",
        "    def map(self, x):\n",
        "        # 解析输入张量的形状，获取批次大小（bsz）、序列长度（length）和输入特征维度（emb_size）。\n",
        "        bsz, length, emb_size = x.size()\n",
        "        # 将输入张量 x 重新整形为二维张量，形状为 (bsz * length, emb_size)，以便进行批次级别的处理。\n",
        "        x = x.reshape(-1, emb_size)\n",
        "\n",
        "        # 调用 compute_gate 方法计算门控损失。\n",
        "        self.compute_gate(x)\n",
        "\n",
        "        # 根据 batch_index 提取每个样本所属的专家输入，形状为 (num_experts, expert_size)。\n",
        "        expert_inputs = x[self.batch_index]\n",
        "        # 将专家输入传递给 input_linear 层，使用专家大小信息进行线性变换，得到专家输出。\n",
        "        expert_outputs = self.input_linear(expert_inputs, self.expert_size)\n",
        "\n",
        "        # 创建一个全零张量 zeros，形状为 (bsz * length * top_k, kv_proj_size)，数据类型和设备与 expert_outputs 相同。\n",
        "        zeros = torch.zeros(\n",
        "            (bsz * length * self.top_k, self.kv_proj_size), dtype=expert_outputs.dtype, device=expert_outputs.device\n",
        "        )\n",
        "        print(f\"expert_outputs.shape:{expert_outputs.shape}\")\n",
        "        print(f\"zeros.shape:{zeros.shape}\")\n",
        "        print(f\"index_sorted_experts.shape:{self.index_sorted_experts.shape}\")\n",
        "\n",
        "        # 使用 index_add 方法将专家输出根据 index_sorted_experts 分散到全零张量 zeros 中，得到混合输出张量 y。\n",
        "        y = zeros.index_add(0, self.index_sorted_experts, expert_outputs)\n",
        "        # 将混合输出张量 y 重新整形为四维张量，形状为(bsz, length, top_k, kv_proj_size)。\n",
        "        y = y.view(bsz, length, self.top_k, -1)\n",
        "        return y\n",
        "\n",
        "    def reduce(self, x: torch.Tensor):\n",
        "        # 解析输入张量的形状，获取批次大小（bsz）、序列长度（length）、专家数量（k）和嵌入维度（emb_size）。\n",
        "        bsz, length, k, emb_size = x.size()\n",
        "        # 将输入张量 x 重新整形为二维张量，形状为 (bsz * length * k, emb_size)。\n",
        "        x = x.reshape(-1, emb_size)\n",
        "\n",
        "        # 根据 index_sorted_experts 提取每个样本所属的专家输入，形状为 (num_experts, expert_size)。\n",
        "        expert_inputs = x[self.index_sorted_experts]\n",
        "        # 将专家输入传递给 output_linear 层，使用专家大小信息进行线性变换，得到专家输出。\n",
        "        expert_outputs = self.output_linear(expert_inputs, self.expert_size)\n",
        "\n",
        "        # 将专家输出乘以对应的门控值。\n",
        "        expert_outputs = expert_outputs * self.batch_gates[:, None]\n",
        "\n",
        "        # 创建一个全零张量 zeros，形状为 (bsz * length, n_embed)，数据类型和设备与 expert_outputs 相同。\n",
        "        zeros = torch.zeros((bsz * length, self.n_embed),\n",
        "                            dtype=expert_outputs.dtype, device=expert_outputs.device)\n",
        "        # 使用 index_add 方法将乘以门控值的专家输出张量根据 batch_index 分散到全零张量 zeros 中，得到降维后的输出张量 y。\n",
        "        y = zeros.index_add(0, self.batch_index, expert_outputs)\n",
        "        # 将降维后的输出张量 y 重新整形为三维张量，形状为 (bsz, length, n_embed)。\n",
        "        y = y.view(bsz, length, self.n_embed)\n",
        "        # 如果设置了偏置项，则将偏置项添加到输出张量 y 中。\n",
        "        if self.p_reduce_bias is not None:\n",
        "            y = y + self.p_reduce_bias\n",
        "        return y\n",
        "\n",
        "    def compute_gate(self, x):\n",
        "        self.top_k_gates, top_k_indices = self.router(x)\n",
        "\n",
        "\n",
        "        self.batch_gates, self.batch_index, expert_size, self.index_sorted_experts = compute_gating(\n",
        "            self.top_k, self.num_experts, self.top_k_gates, top_k_indices\n",
        "        )\n",
        "        self.expert_size = expert_size.tolist()\n"
      ],
      "metadata": {
        "id": "GCdalUZAezFg"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sparse_moe_mha = SparseMoEMultiHeadAttention(n_head, head_size, n_embd, block_size, dropout, num_experts, top_k)\n",
        "print(sparse_moe_mha)\n",
        "sparse_moe_mha_final_output = sparse_moe_mha(fake_hidden_states)\n",
        "print(\"sparse_moe_mha Shape of the final output:\", sparse_moe_mha_final_output.shape)\n",
        "print(sparse_moe_mha_final_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNCvLqVolQO6",
        "outputId": "f0bd4300-3c6c-4a29-8055-ef3d31a4605b"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16 8 2\n",
            "SparseMoEMultiHeadAttention(\n",
            "  (input_linear): ParallelExperts(num_experts=8, input_size=16, output_size=8)\n",
            "  (output_linear): ParallelExperts(num_experts=8, input_size=8, output_size=16)\n",
            "  (router): NoisyTopkRouter(\n",
            "    (topkroute_linear): Linear(in_features=16, out_features=8, bias=True)\n",
            "    (noise_linear): Linear(in_features=16, out_features=8, bias=True)\n",
            "  )\n",
            "  (k_proj): Linear(in_features=16, out_features=8, bias=False)\n",
            "  (v_proj): Linear(in_features=16, out_features=8, bias=False)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "mh_output.shape:torch.Size([8, 16])\n",
            "logits.shape:torch.Size([8, 8])\n",
            " 0  0  0  0  0  0  1  1\n",
            " 0  0  0  1  0  0  0  1\n",
            " 0  0  1  1  0  0  0  0\n",
            " 0  0  0  0  0  1  0  1\n",
            " 0  1  0  0  1  0  0  0\n",
            " 0  0  1  1  0  0  0  0\n",
            " 0  0  1  1  0  0  0  0\n",
            " 0  0  0  0  0  1  1  0\n",
            "[ CPUFloatType{8,8} ]\n",
            " 0\n",
            " 1\n",
            " 3\n",
            " 4\n",
            " 1\n",
            " 2\n",
            " 2\n",
            " 3\n",
            "[ CPULongType{8} ]\n",
            "expert_outputs.shape:torch.Size([16, 8])\n",
            "zeros.shape:torch.Size([16, 8])\n",
            "index_sorted_experts.shape:torch.Size([16])\n",
            "sparse_moe_mha Shape of the final output: torch.Size([2, 4, 16])\n",
            "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000],\n",
            "         [ 0.0110,  0.0081,  0.0262,  0.0299,  0.0000,  0.0183, -0.0204,\n",
            "           0.0000, -0.0161,  0.0208,  0.0498,  0.0000, -0.0440, -0.0142,\n",
            "          -0.0260, -0.0230]],\n",
            "\n",
            "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000],\n",
            "         [-0.0078, -0.0357,  0.0301, -0.0317,  0.0068, -0.0284, -0.0196,\n",
            "          -0.0313,  0.0050,  0.0108,  0.0047, -0.0055,  0.0480, -0.0050,\n",
            "          -0.0128, -0.0147],\n",
            "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000],\n",
            "         [ 0.0066, -0.0106, -0.0153, -0.0062,  0.0009, -0.0060, -0.0115,\n",
            "          -0.0039, -0.0044,  0.0063, -0.0089,  0.0049, -0.0082,  0.0127,\n",
            "           0.0147, -0.0103]]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "da5f3be4-f155-4d6c-bcbd-2f88a088261f",
          "showTitle": false,
          "title": ""
        },
        "id": "vMZCda7dkRJj"
      },
      "source": [
        "## Putting it all together to train and generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0eaf71cd-c77e-40c7-b5be-e364e91685cf",
          "showTitle": false,
          "title": ""
        },
        "id": "f8yczkFHkRJj"
      },
      "outputs": [],
      "source": [
        "#First defining hyperparameters and boiler plate code. Imports and data preparation code is repeated for convenience\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import init\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 400\n",
        "head_size = 16\n",
        "n_embed = 128\n",
        "n_head = 8\n",
        "n_layer = 8\n",
        "dropout = 0.1\n",
        "num_experts = 8\n",
        "top_k = 2\n",
        "aux_loss_coef=0.01\n",
        "moe_self_attention=False\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@torch.jit.script\n",
        "def compute_aux_loss(num_experts: int,\n",
        "                     top_k_gates: torch.Tensor,\n",
        "                     top_k_indices: torch.Tensor,\n",
        "                     logits: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Calculate and return the auxiliary loss based on the accumulated statistics.\n",
        "    switch transformers: https://arxiv.org/pdf/2101.03961.pdf\n",
        "    A. Differentiable Load Balancing Loss\n",
        "\n",
        "    Args:\n",
        "        num_experts (int): The number of experts.\n",
        "        top_k_gates (tensor): k个最大值的对应logits, 其每个元素表示对应logit概率值。\n",
        "        top_k_indices (tensor): k个最大值的对应logits索引, 其每个元素表示logit对应索引值。\n",
        "        logits (tensor): 其每个元素表示对应logit概率值。\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The calculated auxiliary loss.\n",
        "    \"\"\"\n",
        "    # 对logits进行softmax操作，得到每个类别的概率分布\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    zeros = torch.zeros_like(probs)\n",
        "    # Convert zeros to match top_k_gates dtype\n",
        "    zeros = zeros.to(top_k_gates.dtype)\n",
        "    gates = zeros.scatter(-1, top_k_indices, top_k_gates)\n",
        "\n",
        "    # 获取 logits 张量的批次大小，即样本数量\n",
        "    count = logits.size(0)\n",
        "    # 计算每个专家被选中的概率之和，即将概率沿着批次维度求和。\n",
        "    probs = probs.sum(0)\n",
        "    # 计算每个专家被选中的频率，即计算门控值大于0的次数（即专家被选中的次数），\n",
        "    # 然后将其沿着批次维度求和。\n",
        "    freq = (gates > 0).float().sum(0)\n",
        "    # 计算 logits 张量经过 softmax 处理后的平方和的对数。\n",
        "    # 这里首先使用 softmax 函数将 logits 转换为概率分布，\n",
        "    # 然后计算概率分布的每个样本的平方和，并取对数，最后将结果沿着批次维度求和。\n",
        "    lsesq = (torch.log(torch.exp(logits).sum(dim=-1)) ** 2).sum()\n",
        "\n",
        "    # 计算专家选择损失，其计算方式为对每个专家的概率和频率进行归一化，然后计算它们的点积，最后将结果乘以专家数量。\n",
        "    switchloss = num_experts * \\\n",
        "        (F.normalize(probs, p=1, dim=0) * F.normalize(freq, p=1, dim=0)).sum()\n",
        "    # 计算 z 损失，即 logits 的对数平方和除以样本数量\n",
        "    zloss = lsesq / count\n",
        "    # 将专家选择损失和 z 损失加权相加得到最终的辅助损失\n",
        "    loss = switchloss + 0.1 * zloss\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "9GXFv5iGpVPk"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.jit.script\n",
        "def compute_gating(k: int, num_experts: int, top_k_gates: torch.Tensor, top_k_indices: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Compute gating values for the mixture of experts based on probabilities and top-k indices.\n",
        "\n",
        "    Args:\n",
        "        k (int): Number of experts to select.\n",
        "        num_experts (int): Total number of experts.\n",
        "        top_k_gates (torch.Tensor): Gating values for top-k experts (batch_size x k).\n",
        "        top_k_indices (torch.Tensor): Indices of top-k experts (batch_size x k).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Batch-level gating values.\n",
        "        torch.Tensor: Batch-level expert indices.\n",
        "        torch.Tensor: Expert size for each expert.\n",
        "        torch.Tensor: Sorted indices of top-k experts.\n",
        "    \"\"\"\n",
        "    zeros = torch.zeros([top_k_gates.size(0), num_experts],\n",
        "                        dtype=top_k_gates.dtype, device=top_k_gates.device)\n",
        "    gates = zeros.scatter(-1, top_k_indices, 1)\n",
        "    #print(gates)\n",
        "    # 计算每个专家被选择的次数，即每列中值为 1 的数量，得到专家大小（expert_size）。\n",
        "    expert_size = gates.long().sum(0)\n",
        "    #print(expert_size)\n",
        "    # 将顶部 k 个专家的门控值和索引展平为一维张量，并对专家索引进行排序。\n",
        "    top_k_gates = top_k_gates.flatten()\n",
        "    #print(top_k_gates)\n",
        "    top_k_experts = top_k_indices.flatten()\n",
        "    _, index_sorted_experts = top_k_experts.sort(0)\n",
        "\n",
        "    # 根据专家索引的排序结果，确定每个样本所属的批次索引（batch_index）。\n",
        "    # 将排序后的索引张量 index_sorted_experts 中的每个元素除以一个标量 k，\n",
        "    # 并指定舍入模式为“截断”（truncation）。这意味着将索引除以 k 后取整数部分，舍去小数部分\n",
        "    batch_index = index_sorted_experts.div(k, rounding_mode=\"trunc\")\n",
        "    # 提取排序后的专家门控值，得到批次级别的门控值（batch_gates）。\n",
        "    batch_gates = top_k_gates[index_sorted_experts]\n",
        "\n",
        "    return batch_gates, batch_index, expert_size, index_sorted_experts"
      ],
      "metadata": {
        "id": "01XqNNpIpbNj"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ee1180f7-5004-4425-87fe-9a81a17b9024",
          "showTitle": false,
          "title": ""
        },
        "id": "QfxJ6B2fkRJj"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "#Multi-Headed Self Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embed, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "03611a92-aaa2-4e0d-9755-cba56f96c794",
          "showTitle": false,
          "title": ""
        },
        "id": "y35jVCZYkRJk"
      },
      "outputs": [],
      "source": [
        "#Expert module\n",
        "class Expert(nn.Module):\n",
        "    \"\"\" An MLP is a simple linear layer followed by a non-linearity i.e. each Expert \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "#noisy top-k gating\n",
        "class NoisyTopkRouter(nn.Module):\n",
        "    def __init__(self, n_embed, num_experts, top_k):\n",
        "        super(NoisyTopkRouter, self).__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.top_k = top_k\n",
        "        #layer for router logits\n",
        "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
        "        self.noise_linear =nn.Linear(n_embed, num_experts)\n",
        "        self.aux_loss = 0.0\n",
        "\n",
        "\n",
        "    def forward(self, mh_output):\n",
        "        # mh_ouput is the output tensor from multihead self attention block\n",
        "        logits = self.topkroute_linear(mh_output)\n",
        "\n",
        "        #Noise logits\n",
        "        noise_logits = self.noise_linear(mh_output)\n",
        "\n",
        "        #Adding scaled unit gaussian noise to the logits\n",
        "        noise = torch.randn_like(logits)*F.softplus(noise_logits)\n",
        "        noisy_logits = logits + noise\n",
        "\n",
        "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
        "        zeros = torch.full_like(noisy_logits, float('-inf'))\n",
        "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
        "        router_output = F.softmax(sparse_logits, dim=-1)\n",
        "        # 训练时才计算辅助loss值, 为了专家之间的负载平衡\n",
        "        if self.training:\n",
        "            self.aux_loss = compute_aux_loss(self.num_experts, router_output,\n",
        "                                             indices, noisy_logits)\n",
        "\n",
        "        return router_output, indices\n",
        "\n",
        "#Now create the sparse mixture of experts module\n",
        "class SparseMoE(nn.Module):\n",
        "    def __init__(self, n_embed, num_experts, top_k, capacity_factor=1.0):\n",
        "        super(SparseMoE, self).__init__()\n",
        "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
        "        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
        "        self.top_k = top_k\n",
        "        # add capacity_factor\n",
        "        self.capacity_factor = capacity_factor\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "    def forward(self, x):\n",
        "    # Assuming x has shape [batch_size, seq_len, n_embd]\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        gating_output, indices = self.router(x)\n",
        "        final_output = torch.zeros_like(x)\n",
        "\n",
        "        # Flatten the batch and sequence dimensions to treat each token independently\n",
        "        flat_x = x.view(-1, x.size(-1))  # Now shape [batch_size * seq_len, n_embd]\n",
        "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
        "\n",
        "        tokens_per_batch = batch_size * seq_len * self.top_k\n",
        "        expert_capacity = int((tokens_per_batch / self.num_experts) * self.capacity_factor)\n",
        "\n",
        "        updates = torch.zeros_like(flat_x)\n",
        "\n",
        "        for i, expert in enumerate(self.experts):\n",
        "            expert_mask = (indices == i).any(dim=-1)\n",
        "            flat_mask = expert_mask.view(-1)\n",
        "            selected_indices = torch.nonzero(flat_mask).squeeze(-1)\n",
        "\n",
        "            limited_indices = selected_indices[:expert_capacity] if selected_indices.numel() > expert_capacity else selected_indices\n",
        "            if limited_indices.numel() > 0:\n",
        "                expert_input = flat_x[limited_indices]\n",
        "                expert_output = expert(expert_input)\n",
        "\n",
        "                gating_scores = flat_gating_output[limited_indices, i].unsqueeze(1)\n",
        "                weighted_output = expert_output * gating_scores\n",
        "\n",
        "                updates.index_add_(0, limited_indices, weighted_output)\n",
        "\n",
        "        # Reshape updates to match the original dimensions of x\n",
        "        final_output += updates.view(batch_size, seq_len, -1)\n",
        "\n",
        "        return final_output\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ParallelExperts(nn.Module):\n",
        "    def __init__(self, num_experts, input_size, output_size) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the ParallelExperts module.\n",
        "        like a Expert pool\n",
        "        maybe manager diff export pool for feature to load :)\n",
        "\n",
        "        Args:\n",
        "            num_experts (int): Number of experts.\n",
        "            input_size (int): Size of the input.\n",
        "            output_size (int): Size of the output.\n",
        "            bias (bool): Whether to include bias terms.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.empty(\n",
        "            num_experts, output_size, input_size))\n",
        "        self.reset_parameters()\n",
        "        self.num_experts = num_experts\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return \"num_experts={}, input_size={}, output_size={}\".format(\n",
        "            self.num_experts, self.input_size, self.output_size\n",
        "        )\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        \"\"\"\n",
        "        Reset the parameters of the model.\n",
        "        \"\"\"\n",
        "        nn.init.uniform_(self.weight, -1.0 / self.weight.size(1),\n",
        "                         1.0 / self.weight.size(1))\n",
        "\n",
        "    def forward(self, inputs, expert_size):\n",
        "        \"\"\"\n",
        "        Forward pass of the ParallelExperts module.\n",
        "\n",
        "        Args:\n",
        "            inputs (Tensor): Input tensor.\n",
        "            expert_size: Expert size information.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Output tensor.\n",
        "        \"\"\"\n",
        "        input_list = inputs.split(expert_size, dim=0)# return tuple\n",
        "        output_list = []\n",
        "        for i in range(self.num_experts):\n",
        "            output_list.append(F.linear(input_list[i], self.weight[i]))\n",
        "        results = torch.cat(output_list, dim=0)\n",
        "        return results"
      ],
      "metadata": {
        "id": "VUyRlYCrppB_"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SparseMoEMultiHeadAttention(nn.Module):\n",
        "    \"\"\" spare moe + multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size, n_embed, block_size, dropout, num_experts=8, top_k=2, reduce_bias=True):\n",
        "        super(SparseMoEMultiHeadAttention, self).__init__()\n",
        "\n",
        "        # 偏置是可学习的参数，通常用于线性层（如全连接层）和卷积层中。\n",
        "        # 模型中引入偏置项，有助于模型更好地拟合训练数据和提高模型的表达能力\n",
        "        # 在训练过程中，模型会通过梯度下降等优化算法自动学习到合适的偏置值，从而使模型的预测更准确。\n",
        "        self.p_reduce_bias = None\n",
        "        if reduce_bias:\n",
        "            self.p_reduce_bias = torch.nn.Parameter(torch.empty(n_embed))\n",
        "            #一定要记住初始化zeros\n",
        "            torch.nn.init.zeros_(self.p_reduce_bias)\n",
        "\n",
        "        self.n_embed = n_embed\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = head_size\n",
        "        self.num_experts = num_experts\n",
        "        self.top_k = min(top_k, self.num_experts)\n",
        "\n",
        "        assert self.top_k > 0, f\"topk must > 0\"\n",
        "        assert self.num_heads > 0, f\"num_heads must > 0\"\n",
        "        assert num_heads % \\\n",
        "            self.top_k == 0, f\"need num_heads:{num_heads}%top_k:{self.top_k} == 0\"\n",
        "\n",
        "        self.num_key_val_heads = int(num_heads/top_k)\n",
        "        self.kv_proj_size = self.num_key_val_heads*head_size\n",
        "\n",
        "        self.input_linear = ParallelExperts(\n",
        "            num_experts, n_embed, self.kv_proj_size)\n",
        "        self.output_linear = ParallelExperts(\n",
        "            num_experts, self.kv_proj_size, n_embed)\n",
        "\n",
        "        self.router = NoisyTopkRouter(n_embed, num_experts, self.top_k)\n",
        "\n",
        "        self.k_proj = torch.nn.Linear(\n",
        "            n_embed, self.kv_proj_size, bias=False)\n",
        "        self.v_proj = torch.nn.Linear(\n",
        "            n_embed, self.kv_proj_size, bias=False)\n",
        "\n",
        "        self.register_buffer('tril', torch.tril(\n",
        "            torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        bsz, seq_len, feat_dim = x.size()\n",
        "\n",
        "        query_states = self.map(x)\n",
        "        key_states = self.k_proj(x)\n",
        "        value_states = self.v_proj(x)\n",
        "\n",
        "        query_states = query_states.view(\n",
        "            bsz, seq_len, self.num_heads, self.head_size\n",
        "        ).transpose(1, 2)\n",
        "        key_states = key_states.view(\n",
        "            bsz, seq_len, self.num_key_val_heads, self.head_size\n",
        "        ).transpose(1, 2)\n",
        "        value_states = value_states.view(\n",
        "            bsz, seq_len, self.num_key_val_heads, self.head_size\n",
        "        ).transpose(1, 2)\n",
        "\n",
        "        kv_seq_len = key_states.shape[2]  # seq_len\n",
        "\n",
        "        # repeat k/v heads if num_key_val_heads < num_heads, it's true\n",
        "        key_states = key_states.repeat(1, self.top_k, 1, 1)\n",
        "        value_states = value_states.repeat(1, self.top_k, 1, 1)\n",
        "\n",
        "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(\n",
        "            self.head_size\n",
        "        )\n",
        "        if attn_weights.size() != (bsz, self.num_heads, seq_len, kv_seq_len):\n",
        "            raise ValueError(\n",
        "                f\"Attention weights should be of size {(bsz, self.num_heads, seq_len, kv_seq_len)}, but is\"\n",
        "                f\" {attn_weights.size()}\"\n",
        "            )\n",
        "\n",
        "        # upcast attention to fp32\n",
        "        attn_weights = nn.functional.softmax(\n",
        "            attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
        "        # attn_weights = self.dropout(attn_weights)\n",
        "        # attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
        "\n",
        "        attn_output = torch.matmul(attn_weights, value_states)\n",
        "        if attn_output.size() != (bsz, self.num_heads, seq_len, self.head_size):\n",
        "            raise ValueError(\n",
        "                f\"`attn_output` should be of size {(bsz, self.num_heads, seq_len, self.head_size)}, but is\"\n",
        "                f\" {attn_output.size()}\"\n",
        "            )\n",
        "\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        attn_output = attn_output.reshape(\n",
        "            bsz, seq_len, self.top_k, self.kv_proj_size)\n",
        "\n",
        "        attn_output = self.reduce(attn_output)\n",
        "        attn_output = attn_output.view(bsz, seq_len, -1)\n",
        "\n",
        "        attn_output = self.dropout(attn_output)\n",
        "\n",
        "        return attn_output\n",
        "        # return attn_output, attn_weights\n",
        "\n",
        "    def map(self, x):\n",
        "        # 解析输入张量的形状，获取批次大小（bsz）、序列长度（length）和输入特征维度（emb_size）。\n",
        "        bsz, length, emb_size = x.size()\n",
        "        # 将输入张量 x 重新整形为二维张量，形状为 (bsz * length, emb_size)，以便进行批次级别的处理。\n",
        "        x = x.reshape(-1, emb_size)\n",
        "\n",
        "        # 调用 compute_gate 方法计算门控损失。\n",
        "        self.compute_gate(x)\n",
        "\n",
        "        # 根据 batch_index 提取每个样本所属的专家输入，形状为 (num_experts, expert_size)。\n",
        "        expert_inputs = x[self.batch_index]\n",
        "        # 将专家输入传递给 input_linear 层，使用专家大小信息进行线性变换，得到专家输出。\n",
        "        expert_outputs = self.input_linear(expert_inputs, self.expert_size)\n",
        "\n",
        "        # 创建一个全零张量 zeros，形状为 (bsz * length * top_k, kv_proj_size)，数据类型和设备与 expert_outputs 相同。\n",
        "        zeros = torch.zeros(\n",
        "            (bsz * length * self.top_k, self.kv_proj_size), dtype=expert_outputs.dtype, device=expert_outputs.device\n",
        "        )\n",
        "        #print(f\"expert_outputs.shape:{expert_outputs.shape}\")\n",
        "        #print(f\"zeros.shape:{zeros.shape}\")\n",
        "        #print(f\"index_sorted_experts.shape:{self.index_sorted_experts.shape}\")\n",
        "\n",
        "        # 使用 index_add 方法将专家输出根据 index_sorted_experts 分散到全零张量 zeros 中，得到混合输出张量 y。\n",
        "        y = zeros.index_add(0, self.index_sorted_experts, expert_outputs)\n",
        "        # 将混合输出张量 y 重新整形为四维张量，形状为(bsz, length, top_k, kv_proj_size)。\n",
        "        y = y.view(bsz, length, self.top_k, -1)\n",
        "        return y\n",
        "\n",
        "    def reduce(self, x: torch.Tensor):\n",
        "        # 解析输入张量的形状，获取批次大小（bsz）、序列长度（length）、专家数量（k）和嵌入维度（emb_size）。\n",
        "        bsz, length, k, emb_size = x.size()\n",
        "        # 将输入张量 x 重新整形为二维张量，形状为 (bsz * length * k, emb_size)。\n",
        "        x = x.reshape(-1, emb_size)\n",
        "\n",
        "        # 根据 index_sorted_experts 提取每个样本所属的专家输入，形状为 (num_experts, expert_size)。\n",
        "        expert_inputs = x[self.index_sorted_experts]\n",
        "        # 将专家输入传递给 output_linear 层，使用专家大小信息进行线性变换，得到专家输出。\n",
        "        expert_outputs = self.output_linear(expert_inputs, self.expert_size)\n",
        "\n",
        "        # 将专家输出乘以对应的门控值。\n",
        "        expert_outputs = expert_outputs * self.batch_gates[:, None]\n",
        "\n",
        "        # 创建一个全零张量 zeros，形状为 (bsz * length, n_embed)，数据类型和设备与 expert_outputs 相同。\n",
        "        zeros = torch.zeros((bsz * length, self.n_embed),\n",
        "                            dtype=expert_outputs.dtype, device=expert_outputs.device)\n",
        "        # 使用 index_add 方法将乘以门控值的专家输出张量根据 batch_index 分散到全零张量 zeros 中，得到降维后的输出张量 y。\n",
        "        y = zeros.index_add(0, self.batch_index, expert_outputs)\n",
        "        # 将降维后的输出张量 y 重新整形为三维张量，形状为 (bsz, length, n_embed)。\n",
        "        y = y.view(bsz, length, self.n_embed)\n",
        "        # 如果设置了偏置项，则将偏置项添加到输出张量 y 中。\n",
        "        if self.p_reduce_bias is not None:\n",
        "            y = y + self.p_reduce_bias\n",
        "        return y\n",
        "\n",
        "    def compute_gate(self, x):\n",
        "        self.top_k_gates, top_k_indices = self.router(x)\n",
        "\n",
        "\n",
        "        self.batch_gates, self.batch_index, expert_size, self.index_sorted_experts = compute_gating(\n",
        "            self.top_k, self.num_experts, self.top_k_gates, top_k_indices\n",
        "        )\n",
        "        self.expert_size = expert_size.tolist()\n"
      ],
      "metadata": {
        "id": "0noBR1YqoSuK"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bfdff2bb-092f-41c8-9a33-c84e6f8d6633",
          "showTitle": false,
          "title": ""
        },
        "id": "jGiuRsSgkRJk"
      },
      "outputs": [],
      "source": [
        "#First create a self attention + mixture of experts block, that may be repeated several number of times\n",
        "#Copy pasting key architecture variables for clarity\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Mixture of Experts Transformer block: communication followed by computation (multi-head self attention + SparseMoE) \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, n_head, num_experts, top_k):\n",
        "        # n_embed: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        if moe_self_attention:\n",
        "            # moe_self_attention, block_size, dropout is global var\n",
        "            self.sa = SparseMoEMultiHeadAttention(n_head, head_size, n_embed, block_size, dropout, num_experts, top_k)\n",
        "\n",
        "        self.smoe = SparseMoE(n_embed, num_experts, top_k)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.smoe(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2d32a276-d0cc-4808-90d7-62441771af44",
          "showTitle": false,
          "title": ""
        },
        "id": "RpyZBA71kRJk"
      },
      "outputs": [],
      "source": [
        "#Finally putting it all together to crease a sparse mixture of experts language model\n",
        "class SparseMoELanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.blocks = nn.ModuleList([Block(n_embed, n_head=n_head, num_experts=num_experts,top_k=top_k) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embed) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        # x = self.blocks(x)  # (B,T,C)\n",
        "        aux_loss = 0.0\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "            if self.training:\n",
        "              #print(block.smoe.router.aux_loss)\n",
        "              aux_loss += block.smoe.router.aux_loss\n",
        "              if moe_self_attention:\n",
        "                aux_loss += block.sa.router.aux_loss\n",
        "\n",
        "\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        if targets is not None and self.training:\n",
        "            loss += aux_loss_coef*aux_loss.to(loss.device)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "622ba7ce-3f20-4820-8982-93f3d3b7be09",
          "showTitle": false,
          "title": ""
        },
        "id": "bBzQXmfykRJk"
      },
      "source": [
        "这里使用Kaiming He初始化，因为专家中存在ReLU激活函数。可以随意尝试使用更常用于Transformer的Glorot初始化。Jeremy Howard的Fastai第2部分有一个非常出色的讲座，从零开始实现了这些初始化方法：https://course.fast.ai/Lessons/lesson17.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a6d3c057-08ee-4c1b-8013-6a88b2eadac5",
          "showTitle": false,
          "title": ""
        },
        "id": "guGaJqHbkRJk"
      },
      "outputs": [],
      "source": [
        "def kaiming_init_weights(m):\n",
        "    if isinstance (m, (nn.Linear)):\n",
        "        init.kaiming_normal_(m.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5b4d9525-8405-4a51-adda-661aba004e57",
          "showTitle": false,
          "title": ""
        },
        "id": "nJGGkXz4kRJl",
        "outputId": "8d391479-f9ff-4f94-82e7-8b556c604150",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SparseMoELanguageModel(\n",
              "  (token_embedding_table): Embedding(65, 128)\n",
              "  (position_embedding_table): Embedding(32, 128)\n",
              "  (blocks): ModuleList(\n",
              "    (0-7): 8 x Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopkRouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (3): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=128, out_features=65, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ],
      "source": [
        "moe_self_attention=False\n",
        "model = SparseMoELanguageModel()\n",
        "model.apply(kaiming_init_weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb5IcQqhtYSi",
        "outputId": "629d0fa0-344a-4d93-a9b9-9e3cdc8a32a1"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.996545 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "moe_self_attention=True\n",
        "model = SparseMoELanguageModel()\n",
        "model.apply(kaiming_init_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikHiprZxsJcJ",
        "outputId": "4db32d29-3042-45fe-c84b-8b81ba7a6336"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SparseMoELanguageModel(\n",
              "  (token_embedding_table): Embedding(65, 128)\n",
              "  (position_embedding_table): Embedding(32, 128)\n",
              "  (blocks): ModuleList(\n",
              "    (0-7): 8 x Block(\n",
              "      (sa): SparseMoEMultiHeadAttention(\n",
              "        (input_linear): ParallelExperts(num_experts=8, input_size=128, output_size=64)\n",
              "        (output_linear): ParallelExperts(num_experts=8, input_size=64, output_size=128)\n",
              "        (router): NoisyTopkRouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (k_proj): Linear(in_features=128, out_features=64, bias=False)\n",
              "        (v_proj): Linear(in_features=128, out_features=64, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopkRouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (3): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=128, out_features=65, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KoySskftfom",
        "outputId": "4ea6301a-7942-46d9-99dd-7fd948c91df8"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.668417 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "6360e1b7-94c4-4ef1-a850-9bc93f49a083",
          "showTitle": false,
          "title": ""
        },
        "id": "WP_2lcRUkRJl"
      },
      "outputs": [],
      "source": [
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "model.train()\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if (iter+1) % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "8aa6e4c4-c688-4985-a3b8-e2af1f771e54",
          "showTitle": false,
          "title": ""
        },
        "id": "W4yshpXMkRJl",
        "outputId": "5978bc96-6e98-4959-db22-2bd2c7312915",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "lopk yilging antoo lost if ther\n",
            "Ifhuf arn afroht.\n",
            "\n",
            "JEN,itio,y bield.\n",
            "\n",
            "  Ileaved my tigrol:\n",
            "My Iid kuantie.\n",
            "\n",
            "Hir!\n",
            "\n",
            "ou hash:t not cealfurnome dearth, he'sing,\n",
            "Els what : that hou you'll sints for inome\n",
            "Whix theing that swrveful of dihtumented case.\n",
            "\n",
            "RICLAREN:\n",
            ": thio, told should\n",
            "up the fould usalchs; when hand frames nimetield.\n",
            "\n",
            "ELERCZEO:\n",
            ",- bor\n",
            "while' thats, I chall mend live: they doubtSicenent\n",
            "Whither sew ere itholy haset, Wrieldy thoLed ip it\n",
            "Fits day ield ver torshiciin have! loves his anlith,\n",
            "Fiding wortyorl I that tone e tthem blood\n",
            "Sture when were to sure and gentlem dovene; mone ballincheed\n",
            "Agud poon thwont onvy thou perfell would it thr follace ghneyr spauH;\n",
            "That is byean men, eye facenthrom honce fless Mornemy ! Has hemnest dehs tsenate\n",
            "Warough wleen hichseral. Time.\n",
            "That, everth a of boreyen you agince!\n",
            "\n",
            "DOFMWARUS:\n",
            "Dchmest this monishing, I shall, that had fond firth:\n",
            "And yumer; but youtio, an this lot accestizfed your then me\n",
            "Wague frviston feep d of themn, ufir\n",
            "Ussters. yet you'groung thi sorr'd\n",
            "Of pere hance ownnest O' rurunting souds,\n",
            "Thu whato thou siling; father feed is\n",
            "I gaivitter fheerscniendly you, lybelt lork-fath myon so Kinf Banchidiful to plusing honcthem!\n",
            "Hhatnd, if this yied the men lits and trong's ant when war\n",
            "inds btonecher'd a deating, lett 'tness;\n",
            "Frews madang is alous slain gownrofinquaciond silvly\n",
            "she lWis litgomertyors to mak,\n",
            "The ingerr alleat the rast reighee\n",
            "Th consuer, talt they his hey doninch\n",
            "do resicions the freits cerly sbbungbs\n",
            "in the councearnigng; and wwill it the canest yoe blood\n",
            "And to to her heast with take come my mockion,\n",
            "Pray untedonges glivrnes thou histhert for mean;\n",
            "And inscondring ourds lady morry groudd.\n",
            "\n",
            "PrCYNAR:\n",
            "Ancause king them in that surcicq;\n",
            "Fry, so, bydnowg his scausancer ton be sourant! why saity  of day, sirge!\n",
            "\n",
            "DoldfERD CAMIOX!\n",
            "\n",
            "FRYREYI:$--MOHved CLArizlifn, ore DucH hads suciun the wife,''\n",
            "\n",
            "Pronce tind Hefinnds, dong teaam\n",
            "As a carriv, yean come heirthin?\n",
            "\n",
            "CiMENENOIO:\n",
            "Iure youuse that n thee steet?, \n"
          ]
        }
      ],
      "source": [
        "# generate from the model. Not great. Not too bad either\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 工程优化考虑\n",
        "\n",
        "以上是简单实现进训练;\n",
        "\n",
        "如果考虑硬件资源训练成本，需要对模型中的tensor操作进行优化， 比如：\n",
        "\n",
        "1. 稀疏tensor 尽量降维，变成紧密tensor, 然后进行+、*等算子操作，减低内存空间，和计算成本，利用SIMD指令集提高计算效率；\n",
        "2. 计算尽量批量处理，并且尽量并行化处理。\n"
      ],
      "metadata": {
        "id": "-pdIA3GlcnoA"
      }
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "makeMoE_from_Scratch",
      "widgets": {}
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}