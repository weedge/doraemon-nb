{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weedge/doraemon-nb/blob/main/langchain/rag/langgraph_self_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "919fe33c-0149-4f7d-b200-544a18986c9a",
      "metadata": {
        "id": "919fe33c-0149-4f7d-b200-544a18986c9a"
      },
      "source": [
        "# Self-RAG\n",
        "\n",
        "Self-RAG 是一种 RAG 策略，它融合了对检索到的文档和生成内容进行自我反思 / 自我评估（的功能）。\n",
        "\n",
        "在[论文](https://arxiv.org/abs/2310.11511)中，会做出一些决策：\n",
        "\n",
        "1.  是否应该从检索器 `R` 检索 -\n",
        "    * 输入: `x (问题)` 或 `x (问题)`, `y (生成内容)`\n",
        "    * 决定何时用 `R` 检索 `D` 个块\n",
        "    * 输出: `是, 否, 继续`\n",
        "\n",
        "2.  检索到的段落 `D` 是否与问题 `x` 相关 -\n",
        "    * 输入: (`x (问题)`, `d (块)`) 针对 `D` 中的每个 `d`\n",
        "    * `d` 提供了解决 `x` 的有用信息\n",
        "    * 输出: `相关, 不相关`\n",
        "\n",
        "3.  `D` 中每个块的 LLM 生成内容是否与该块相关 (幻觉等) -\n",
        "    * 输入: `x (问题)`, `d (块)`, `y (生成内容)` 针对 `D` 中的每个 `d`\n",
        "    * `y (生成内容)` 中所有值得核实的陈述都得到了 `d` 的支持\n",
        "    * 输出: `{完全支持, 部分支持, 不支持}`\n",
        "\n",
        "4.  `D` 中每个块的 LLM 生成内容是否是对 `x (问题)` 的有用响应 -\n",
        "    * 输入: `x (问题)`, `y (生成内容)`\n",
        "    * `y (生成内容)` 是对 `x (问题)` 的有用响应。\n",
        "    * 输出: `{5, 4, 3, 2, 1}`\n",
        "\n",
        "我们将使用 [LangGraph](https://langchain-ai.github.io/langgraph/) 从头开始实现其中的一些想法。\n",
        "\n",
        "![](https://github-production-user-asset-6210df.s3.amazonaws.com/1203957/510510224-32069937-cb01-4f12-b429-07dcf8edefdd.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20251106%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251106T021633Z&X-Amz-Expires=300&X-Amz-Signature=93fd8a5dae15b6cba5bd529d322cf46d21e8398f2063bf684b5b260f788f93c0&X-Amz-SignedHeaders=host)\n",
        "\n",
        "\n",
        "1. 发起查询，从向量数据库中召回相应文档(一般是采用混合检索，召回和准确率)；\n",
        "2. 通过LLM判断这些文档是不是相关文档；\n",
        "3. 如果不是相关文档，通过LLM对查询进行重写，进行第1步操作；\n",
        "4. 如果是相关文档，通过LLM判断是否有幻觉；\n",
        "5. 如果有幻觉，通过LLM对这些相关文档和查询继续生成回答内容；\n",
        "6. 如果没有幻觉，进行第2步操作；\n",
        "7. 如果不是相关文档，通过LLM对查询进行重写，进行第1步操作；\n",
        "8. 如果是相关文档， 返回生成结果；"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a44a72d6-7e0c-4478-9d20-4c09000420a8"
      },
      "source": [
        "## 设置 (Setup)\n",
        "\n",
        "首先，我们需要安装所需的包。"
      ],
      "id": "a44a72d6-7e0c-4478-9d20-4c09000420a8"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b451b58a-89bd-424f-8c06-0d9fe325e01b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd3f2095-72ab-4236-d69a-7f800e29b9fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.9/469.9 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.9.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --quiet -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb langchain langgraph  tavily-python langchain-google-genai langchain-text-splitters"
      ],
      "id": "b451b58a-89bd-424f-8c06-0d9fe325e01b"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep -E \"langchain|langgraph|chromadb\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoRFlZmMGi3W",
        "outputId": "ac388980-4b87-449c-9b29-cecf2a4828d9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chromadb                                 1.3.4\n",
            "langchain                                1.0.3\n",
            "langchain-classic                        1.0.0\n",
            "langchain-cohere                         0.5.0\n",
            "langchain-community                      0.4.1\n",
            "langchain-core                           1.0.3\n",
            "langchain-google-genai                   3.0.1\n",
            "langchain-openai                         1.0.2\n",
            "langchain-text-splitters                 1.0.0\n",
            "langchainhub                             0.1.21\n",
            "langgraph                                1.0.2\n",
            "langgraph-checkpoint                     3.0.1\n",
            "langgraph-prebuilt                       1.0.2\n",
            "langgraph-sdk                            0.2.9\n"
          ]
        }
      ],
      "id": "uoRFlZmMGi3W"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35f267b0-98db-4a59-8b2c-a23f795576ff"
      },
      "source": [
        "接下来，我们需要为 OpenAI（我们将使用的 LLM）和 Tavily（我们将使用的搜索工具）设置 API 密钥。"
      ],
      "id": "35f267b0-98db-4a59-8b2c-a23f795576ff"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "743c19df-6da9-4d1e-b2d2-ea40080b9fdc"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]=userdata.get(\"ZHIPU_API_KEY\")\n",
        "os.environ[\"TAVILY_API_KEY\"]=userdata.get(\"TAVILY_API_KEY\")\n",
        "os.environ[\"GOOGLE_API_KEY\"]=userdata.get(\"GOOGLE_API_KEY\")\n",
        "os.environ[\"GITEE_API_KEY\"]=userdata.get(\"GITEE_API_KEY\")\n",
        "\n"
      ],
      "id": "743c19df-6da9-4d1e-b2d2-ea40080b9fdc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab5cea6d"
      },
      "source": [
        "<div class=\"admonition tip\"> <p class=\"admonition-title\">设置 <a href=\"https://smith.langchain.com\">LangSmith</a> 以用于 LangGraph 开发</p> <p style=\"padding-top: 5px;\"> 注册 LangSmith 可以快速发现问题并提高您的 LangGraph 项目的性能。LangSmith 允许您使用跟踪数据来调试、测试和监控使用 LangGraph 构建的 LLM 应用程序——阅读更多关于如何开始的信息 <a href=\"https://docs.smith.langchain.com\">请点击此处</a>。 </p> </div>"
      ],
      "id": "ab5cea6d"
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LANGSMITH_API_KEY\"]=userdata.get(\"LANGSMITH_API_KEY\")\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n"
      ],
      "metadata": {
        "id": "IooCYxWjE5il"
      },
      "execution_count": 4,
      "outputs": [],
      "id": "IooCYxWjE5il"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ac1c2cd-81fb-40eb-8ba1-e9197800cba6"
      },
      "source": [
        "## 创建索引\n",
        "\n",
        "使用 OpenAI 嵌入（OpenAI Embeddings）和 Chroma 向量数据库来设置一个向量数据库。\n",
        "输入与智能体（agents）、提示工程（prompt engineering）和大型语言模型（LLMs）相关的博客文章的 URL。\n",
        "生成用于检索增强生成（RAG）的向量索引。"
      ],
      "id": "9ac1c2cd-81fb-40eb-8ba1-e9197800cba6"
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "\tbase_url=\"https://ai.gitee.com/v1\",\n",
        "\tapi_key=os.environ[\"GITEE_API_KEY\"],\n",
        "\tdefault_headers={\"X-Failover-Enabled\":\"true\"},\n",
        ")\n",
        "\n",
        "response = client.embeddings.create(\n",
        "\tinput=\"Today is a sunny day and I will get some ice cream.\",\n",
        "\tmodel=\"Qwen3-Embedding-8B\",\n",
        "\tdimensions=1024,\n",
        ")\n"
      ],
      "metadata": {
        "id": "XVD5LL-TjqFS"
      },
      "execution_count": null,
      "outputs": [],
      "id": "XVD5LL-TjqFS"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "### from langchain_cohere import CohereEmbeddings\n",
        "\n",
        "# Set embeddings\n",
        "# https://ai.gitee.com/serverless-api#embedding-rerank\n",
        "# 100 api calls per day, free tier ..................NO!!!! ❄️\n",
        "embd = OpenAIEmbeddings(\n",
        "    base_url=\"https://ai.gitee.com/v1\",\n",
        "    model=\"Qwen3-Embedding-8B\",#4096\n",
        "    api_key=os.environ[\"GITEE_API_KEY\"],\n",
        "    dimensions=1024,\n",
        "    check_embedding_ctx_length=False,\n",
        "    chunk_size=1000,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "ydSF1Bzhg8H5"
      },
      "execution_count": 5,
      "outputs": [],
      "id": "ydSF1Bzhg8H5"
    },
    {
      "cell_type": "code",
      "source": [
        "vector = embd.embed_query(\"Today is a sunny day and I will get some ice cream.\")\n",
        "print(vector)"
      ],
      "metadata": {
        "id": "yNsHUYzVFU9S"
      },
      "execution_count": null,
      "outputs": [],
      "id": "yNsHUYzVFU9S"
    },
    {
      "cell_type": "markdown",
      "source": [
        "定义批量大小"
      ],
      "metadata": {
        "id": "U5CigAnqDybh"
      },
      "id": "U5CigAnqDybh"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "from chromadb.api.client import Client as ClientCreator\n",
        "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE\n",
        "from chromadb.config import Settings, System\n",
        "\n",
        "class CustomClientCreator(ClientCreator):\n",
        "    # region Initialization\n",
        "    def __init__(\n",
        "        self,\n",
        "        tenant: Optional[str] = DEFAULT_TENANT,\n",
        "        database: Optional[str] = DEFAULT_DATABASE,\n",
        "        settings: Settings = Settings(),\n",
        "    ) -> None:\n",
        "        super().__init__(tenant=tenant, database=database, settings=settings)\n",
        "\n",
        "    def get_max_batch_size(self) -> int:\n",
        "        return 10  # some server api limit batch size"
      ],
      "metadata": {
        "id": "sGHKHtSTDxu1"
      },
      "execution_count": 6,
      "outputs": [],
      "id": "sGHKHtSTDxu1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Index"
      ],
      "metadata": {
        "id": "gHY7Kz5w5XU6"
      },
      "id": "gHY7Kz5w5XU6"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "b224e5ba-50ca-495a-a7fa-0f75a080e03c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "863dbff1-4bf1-4db2-d85b-4f9e14b37963"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document splits: 44\n"
          ]
        }
      ],
      "source": [
        "### Build Index\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "os.environ[\"USER_AGENT\"] = \"achatbot-demo\"\n",
        "# Docs to index\n",
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]\n",
        "\n",
        "# Load\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=1000, chunk_overlap=0\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "print(f\"Document splits: {len(doc_splits)}\")\n",
        "\n"
      ],
      "id": "b224e5ba-50ca-495a-a7fa-0f75a080e03c"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Add to vectorstore\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag-chroma\",\n",
        "    persist_directory=\"./rag_chroma_db\",# sqlite(row) file path or duckdb(column) file path\n",
        "    embedding=embd,\n",
        "    client=CustomClientCreator(),  # use custom client with batch utils\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "HXIqSRoVFiAA"
      },
      "execution_count": 8,
      "outputs": [],
      "id": "HXIqSRoVFiAA"
    },
    {
      "cell_type": "code",
      "source": [
        "retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHtd4yeVJNON",
        "outputId": "cb5b2839-9df8-43a5-c8a6-54a079f42efb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7cfe2834a9c0>, search_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "id": "AHtd4yeVJNON"
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke(\"agent memory\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4SfmqsWJJI6",
        "outputId": "8e9f725a-c314-48c8-c939-c6957b71299b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'language': 'en', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.'}, page_content='Commands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'),\n",
              " Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'language': 'en'}, page_content='They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\n\\nThe generative agent architecture. (Image source: Park et al. 2023)\\n\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes'),\n",
              " Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'language': 'en'}, page_content=\"LLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.\"),\n",
              " Document(metadata={'language': 'en', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\n\\nExamples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\n\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\n\\nIllustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\n\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\n\\nExperiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "id": "f4SfmqsWJJI6"
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore.persist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xPKosva7wS_",
        "outputId": "b899c4ad-c6c6-4130-be7f-11a10cd5d905"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-398866168.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  vectorstore.persist()\n"
          ]
        }
      ],
      "id": "9xPKosva7wS_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### use indexed docs"
      ],
      "metadata": {
        "id": "rWaiQ6eW5bcQ"
      },
      "id": "rWaiQ6eW5bcQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "retriveal_vectorstore = Chroma(\n",
        "    collection_name=\"rag-chroma\",\n",
        "    persist_directory=\"./rag_chroma_db\",# sqlite(row) file path or duckdb(column) file path\n",
        "    embedding_function=embd,\n",
        "    client=CustomClientCreator(),  # use custom client with batch utils\n",
        ")\n",
        "\n",
        "retriveal_retriever = retriveal_vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "KAN1EgBy5kDM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a100419b-47d4-44c0-a447-260a0b2d63c5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2217942785.py:3: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
            "  retriveal_vectorstore = Chroma(\n"
          ]
        }
      ],
      "id": "KAN1EgBy5kDM"
    },
    {
      "cell_type": "code",
      "source": [
        "retriveal_retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU-Uy9Ev7Ux3",
        "outputId": "385c669e-da08-4433-e680-7fe2c80b6cd5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7cfebf5e3080>, search_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "id": "FU-Uy9Ev7Ux3"
    },
    {
      "cell_type": "code",
      "source": [
        "retriveal_retriever.invoke(\"agent memory\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxPUoTo27WzY",
        "outputId": "bc4c0212-0a50-4a86-9358-60698f42fbc3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'language': 'en'}, page_content='Commands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'),\n",
              " Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\n\\nThe generative agent architecture. (Image source: Park et al. 2023)\\n\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content=\"LLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.\"),\n",
              " Document(metadata={'language': 'en', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\n\\nExamples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\n\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\n\\nIllustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\n\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\n\\nExperiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)')]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "id": "lxPUoTo27WzY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f52b427-750c-40f8-8893-e9caab3afd8d"
      },
      "source": [
        "## LLMs"
      ],
      "id": "0f52b427-750c-40f8-8893-e9caab3afd8d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d28baefd-a961-49b0-8394-c5478dadda1c"
      },
      "source": [
        "<div class=\"admonition note\"> <p class=\"admonition-title\">将 Pydantic 与 LangChain 配合使用</p> <p> 本笔记（notebook）使用 Pydantic v2 <code>BaseModel</code>，这需要 <code>langchain-core >= 0.3</code>。如果使用 <code>langchain-core < 0.3</code>，将会因 Pydantic v1 和 v2 <code>BaseModels</code> 混用而导致错误。 </p> </div>"
      ],
      "id": "d28baefd-a961-49b0-8394-c5478dadda1c"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7-WhWtnA3uY",
        "outputId": "30c44528-da05-42bb-b7f5-a5d01666e79c"
      },
      "id": "s7-WhWtnA3uY",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "langchain-core                           1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# zhipu\n",
        "llm=ChatOpenAI(\n",
        "  base_url=\"https://open.bigmodel.cn/api/paas/v4\",\n",
        "  model=\"glm-4.5-flash\",\n",
        "  max_tokens=32768,\n",
        "  temperature=0\n",
        ")\n"
      ],
      "metadata": {
        "id": "E0JrCfQOIpx_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "E0JrCfQOIpx_"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# google\n",
        "llm=ChatGoogleGenerativeAI(\n",
        "  #model=\"gemini-2.5-flash\",\n",
        "  model=\"gemini-2.5-pro\",# ok\n",
        "  temperature=0\n",
        ")"
      ],
      "metadata": {
        "id": "aOcvvEzeIOlH"
      },
      "execution_count": 20,
      "outputs": [],
      "id": "aOcvvEzeIOlH"
    },
    {
      "cell_type": "markdown",
      "id": "c27bebdc-be71-4130-ab9d-42f09f87658b",
      "metadata": {
        "id": "c27bebdc-be71-4130-ab9d-42f09f87658b"
      },
      "source": [
        "## Retriever\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "1fafad21-60cc-483e-92a3-6a7edb1838e3",
      "metadata": {
        "id": "1fafad21-60cc-483e-92a3-6a7edb1838e3",
        "outputId": "f146fbac-c9cb-4a62-f8f4-85a1a33378b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "binary_score='yes'\n"
          ]
        }
      ],
      "source": [
        "### Retrieval Grader\n",
        "\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "# Data model\n",
        "class GradeDocuments(BaseModel):\n",
        "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "\n",
        "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
        "grade_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "retrieval_grader = grade_prompt | structured_llm_grader\n",
        "question = \"agent memory\"\n",
        "docs = retriever.invoke(question)\n",
        "doc_txt = docs[1].page_content\n",
        "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Generate\n",
        "\n",
        "from langchain_classic import hub\n",
        "\n",
        "# Prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUBSRJcpTQV7",
        "outputId": "50f0c688-2fd1-445e-b416-42dee087f5df"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
          ]
        }
      ],
      "id": "iUBSRJcpTQV7"
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt.format(context=\"context\", question=\"question\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjuua2V_Xjpf",
        "outputId": "cf55e5e9-635d-49ea-a342-4f776a6e162f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
            "Question: question \n",
            "Context: context \n",
            "Answer:\n"
          ]
        }
      ],
      "id": "qjuua2V_Xjpf"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "dcd77cc1-4587-40ec-b633-5364eab9e1ec",
      "metadata": {
        "id": "dcd77cc1-4587-40ec-b633-5364eab9e1ec",
        "outputId": "7c92c7d3-9eeb-4998-88e0-da9b2136cb86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In an LLM-powered autonomous agent, memory consists of two key types: short-term and long-term. Short-term memory is used for in-context learning within the model's limited context window. Long-term memory allows the agent to retain and recall information over extended periods, often by leveraging an external vector store for fast retrieval.\n"
          ]
        }
      ],
      "source": [
        "### Generate\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    if not isinstance(docs, list):\n",
        "        docs = [docs]\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# Chain\n",
        "rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run\n",
        "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
        "print(generation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "e78931ec-940c-46ad-a0b2-f43f953f1fd7",
      "metadata": {
        "id": "e78931ec-940c-46ad-a0b2-f43f953f1fd7",
        "outputId": "4554a08b-c6f9-4355-e80e-bca7668db2ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradeHallucinations(binary_score='yes')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "### Hallucination Grader\n",
        "\n",
        "\n",
        "# Data model\n",
        "class GradeHallucinations(BaseModel):\n",
        "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "\n",
        "# LLM with function call\n",
        "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
        "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
        "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
        "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "bd62276f-bf26-40d0-8cff-e07b10e00321",
      "metadata": {
        "id": "bd62276f-bf26-40d0-8cff-e07b10e00321",
        "outputId": "845a6be8-d231-4c85-fb71-97bb64e2c41f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradeAnswer(binary_score='yes')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "### Answer Grader\n",
        "\n",
        "\n",
        "# Data model\n",
        "class GradeAnswer(BaseModel):\n",
        "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "\n",
        "# LLM with structured_output\n",
        "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n\n",
        "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
        "answer_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "answer_grader = answer_prompt | structured_llm_grader\n",
        "answer_grader.invoke({\"question\": question, \"generation\": generation})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "c6f4c70e-1660-4149-82c0-837f19fc9fb5",
      "metadata": {
        "id": "c6f4c70e-1660-4149-82c0-837f19fc9fb5",
        "outputId": "3290f599-01d7-477a-8863-d4db4b3e214f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What are the different types of memory for AI agents, such as short-term and long-term memory, and what are the common techniques for implementing them?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "### Question Re-writer\n",
        "\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n",
        "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\\n\n",
        "     Please provide the rewritten result directly.\"\"\"\n",
        "re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
        "question_rewriter.invoke({\"question\": question})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "276001c5-c079-4e5b-9f42-81a06704d200",
      "metadata": {
        "id": "276001c5-c079-4e5b-9f42-81a06704d200"
      },
      "source": [
        "# Graph\n",
        "\n",
        "Capture the flow in as a graph.\n",
        "\n",
        "## Graph state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "f1617e9e-66a8-4c1a-a1fe-cc936284c085",
      "metadata": {
        "id": "f1617e9e-66a8-4c1a-a1fe-cc936284c085"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        question: question\n",
        "        generation: LLM generation\n",
        "        documents: list of documents\n",
        "    \"\"\"\n",
        "\n",
        "    question: str\n",
        "    generation: str\n",
        "    documents: List[str]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "add509d8-6682-4127-8d95-13dd37d79702",
      "metadata": {
        "id": "add509d8-6682-4127-8d95-13dd37d79702"
      },
      "outputs": [],
      "source": [
        "### Nodes\n",
        "\n",
        "\n",
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVE---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Retrieval\n",
        "    documents = retriever.invoke(question)\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "\n",
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains LLM generation\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # RAG generation\n",
        "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "\n",
        "\n",
        "def grade_documents(state):\n",
        "    \"\"\"\n",
        "    Determines whether the retrieved documents are relevant to the question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with only filtered relevant documents\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Score each doc\n",
        "    filtered_docs = []\n",
        "    for d in documents:\n",
        "        score = retrieval_grader.invoke(\n",
        "            {\"question\": question, \"document\": d.page_content}\n",
        "        )\n",
        "        grade = score.binary_score\n",
        "        if grade == \"yes\":\n",
        "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "            filtered_docs.append(d)\n",
        "        else:\n",
        "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "            continue\n",
        "    return {\"documents\": filtered_docs, \"question\": question}\n",
        "\n",
        "\n",
        "def transform_query(state):\n",
        "    \"\"\"\n",
        "    Transform the query to produce a better question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates question key with a re-phrased question\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---TRANSFORM QUERY---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Re-write question\n",
        "    better_question = question_rewriter.invoke({\"question\": question})\n",
        "    return {\"documents\": documents, \"question\": better_question}\n",
        "\n",
        "\n",
        "### Edges\n",
        "\n",
        "\n",
        "def decide_to_generate(state):\n",
        "    \"\"\"\n",
        "    Determines whether to generate an answer, or re-generate a question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Binary decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
        "    state[\"question\"]\n",
        "    filtered_documents = state[\"documents\"]\n",
        "\n",
        "    if not filtered_documents:\n",
        "        # All documents have been filtered check_relevance\n",
        "        # We will re-generate a new query\n",
        "        print(\n",
        "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
        "        )\n",
        "        return \"transform_query\"\n",
        "    else:\n",
        "        # We have relevant documents, so generate answer\n",
        "        print(\"---DECISION: GENERATE---\")\n",
        "        return \"generate\"\n",
        "\n",
        "\n",
        "def grade_generation_v_documents_and_question(state):\n",
        "    \"\"\"\n",
        "    Determines whether the generation is grounded in the document and answers question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK HALLUCINATIONS---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    generation = state[\"generation\"]\n",
        "\n",
        "    score = hallucination_grader.invoke(\n",
        "        {\"documents\": documents, \"generation\": generation}\n",
        "    )\n",
        "    grade = score.binary_score\n",
        "\n",
        "    # Check hallucination\n",
        "    if grade == \"yes\":\n",
        "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
        "        # Check question-answering\n",
        "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
        "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
        "        grade = score.binary_score\n",
        "        if grade == \"yes\":\n",
        "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
        "            return \"useful\"\n",
        "        else:\n",
        "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
        "            return \"not useful\"\n",
        "    else:\n",
        "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
        "        return \"not supported\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61cd5797-1782-4d78-a277-8196d13f3e1b",
      "metadata": {
        "id": "61cd5797-1782-4d78-a277-8196d13f3e1b"
      },
      "source": [
        "## Build Graph\n",
        "\n",
        "The just follows the flow we outlined in the figure above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "0e09ca9f-e36d-4ef4-a0d5-79fdbada9fe0",
      "metadata": {
        "id": "0e09ca9f-e36d-4ef4-a0d5-79fdbada9fe0"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import END, StateGraph, START\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
        "workflow.add_node(\"generate\", generate)  # generate\n",
        "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
        "\n",
        "# Build graph\n",
        "workflow.add_edge(START, \"retrieve\")\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        \"transform_query\": \"transform_query\",\n",
        "        \"generate\": \"generate\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"generate\",\n",
        "    grade_generation_v_documents_and_question,\n",
        "    {\n",
        "        \"not supported\": \"generate\",\n",
        "        \"useful\": END,\n",
        "        \"not useful\": \"transform_query\",\n",
        "    },\n",
        ")\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(app.get_graph(xray=True).draw_mermaid_png()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "aACqK3-eC1A6",
        "outputId": "e5c14416-c41c-44c9-a66e-f7df352384aa"
      },
      "id": "aACqK3-eC1A6",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "fb69dbb9-91ee-4868-8c3c-93af3cd885be",
      "metadata": {
        "id": "fb69dbb9-91ee-4868-8c3c-93af3cd885be",
        "outputId": "29cfb9ab-4706-472e-ff14-da8c65e9dba2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---RETRIEVE---\n",
            "\"Node 'retrieve':\"\n",
            "'\\n---\\n'\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: GENERATE---\n",
            "\"Node 'grade_documents':\"\n",
            "'\\n---\\n'\n",
            "---GENERATE---\n",
            "---CHECK HALLUCINATIONS---\n",
            "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
            "---GRADE GENERATION vs QUESTION---\n",
            "---DECISION: GENERATION ADDRESSES QUESTION---\n",
            "\"Node 'generate':\"\n",
            "'\\n---\\n'\n",
            "('Agent memory works through two main systems. Short-term memory utilizes '\n",
            " \"in-context learning, where information is held within the model's finite \"\n",
            " 'context window for immediate use in tasks like reasoning. Long-term memory '\n",
            " 'allows the agent to retain and recall information over extended periods by '\n",
            " 'using an external vector store for fast retrieval.')\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Run\n",
        "inputs = {\"question\": \"Explain how the different types of agent memory work?\"}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint(value, indent=2, width=80, depth=None)\n",
        "    pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint(value[\"generation\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "trace: https://smith.langchain.com/public/5e17ae7d-1c96-43e8-9a19-c2885e8089f0/r"
      ],
      "metadata": {
        "id": "j4AYG92xDJCL"
      },
      "id": "j4AYG92xDJCL"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "4138bc51-8c84-4b8a-8d24-f7f470721f6f",
      "metadata": {
        "id": "4138bc51-8c84-4b8a-8d24-f7f470721f6f",
        "outputId": "592c6592-ae07-4bee-cd69-d0bbe0e8e296",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---RETRIEVE---\n",
            "\"Node 'retrieve':\"\n",
            "'\\n---\\n'\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: GENERATE---\n",
            "\"Node 'grade_documents':\"\n",
            "'\\n---\\n'\n",
            "---GENERATE---\n",
            "---CHECK HALLUCINATIONS---\n",
            "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
            "---GRADE GENERATION vs QUESTION---\n",
            "---DECISION: GENERATION ADDRESSES QUESTION---\n",
            "\"Node 'generate':\"\n",
            "'\\n---\\n'\n",
            "('Chain of thought (CoT) prompting is a technique that instructs a language '\n",
            " 'model to generate a step-by-step reasoning process before giving a final '\n",
            " 'answer. This method helps the model decompose complex tasks into smaller, '\n",
            " 'simpler steps, which improves performance on complicated reasoning tasks. '\n",
            " 'CoT can be implemented by providing a few examples with reasoning chains '\n",
            " '(few-shot) or by simply adding a phrase like \"Let\\'s think step by step\" to '\n",
            " 'the prompt (zero-shot).')\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"question\": \"Explain how chain of thought prompting works?\"}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        pprint(value, indent=2, width=80, depth=None)\n",
        "    pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint(value[\"generation\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "548f1c5b-4108-4aae-8abb-ec171b511b92",
      "metadata": {
        "id": "548f1c5b-4108-4aae-8abb-ec171b511b92"
      },
      "source": [
        "trace: https://smith.langchain.com/public/0a2f8728-a0a6-4dc7-80e4-0256a2605949/r"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}