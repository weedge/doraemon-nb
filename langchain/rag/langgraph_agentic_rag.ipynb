{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwe4dyLIeSseLuFkCNMEKJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weedge/doraemon-nb/blob/main/langchain/rag/langgraph_agentic_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ⭐️ https://arxiv.org/html/2501.09136\n",
        "\n",
        "# Agentic RAG（智能体 RAG）\n",
        "\n",
        "在本教程中，我们将构建一个[检索智能体](https://python.langchain.com/docs/tutorials/qa_chat_history)。当您希望 LLM（大型语言模型）来决定是从向量存储（vectorstore）中检索上下文，还是直接响应用户时，检索智能体会非常有用。\n",
        "\n",
        "学完本教程后，我们将完成以下工作：\n",
        "\n",
        "1.  获取并预处理将用于检索的文档。\n",
        "2.  将这些文档索引（index）以便进行语义搜索，并为智能体创建一个检索器工具（retriever tool）。\n",
        "3.  构建一个 agentic RAG 系统，使其能够决定何时使用该检索器工具。\n",
        "\n",
        "![](https://raw.githubusercontent.com/langchain-ai/langgraph/refs/heads/main/docs/docs/tutorials/rag/assets/screenshot_2024_02_14_3_43_58_pm.png)\n",
        "## 设置\n",
        "\n",
        "让我们下载所需的包并设置我们的 API 密钥："
      ],
      "metadata": {
        "id": "R5Se5FkN-aDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet -U langgraph langchain_community langchain-openai  langchain-google-genai langchain-text-splitters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pO-FssbO_ImN",
        "outputId": "0a9be026-c671-4a8c-c183-2e742a1c9470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/156.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/2.5 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.9/469.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.9.0 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.3 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep -E \"langchain|langgraph\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoRFlZmMGi3W",
        "outputId": "4a63c5b3-582e-4706-9e0f-a722c6e1de4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "langchain                                0.3.27\n",
            "langchain-classic                        1.0.0\n",
            "langchain-community                      0.4.1\n",
            "langchain-core                           1.0.3\n",
            "langchain-google-genai                   3.0.1\n",
            "langchain-openai                         1.0.2\n",
            "langchain-text-splitters                 1.0.0\n",
            "langgraph                                1.0.2\n",
            "langgraph-checkpoint                     3.0.1\n",
            "langgraph-prebuilt                       1.0.2\n",
            "langgraph-sdk                            0.2.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35f267b0-98db-4a59-8b2c-a23f795576ff"
      },
      "source": [
        "接下来，我们需要为 OpenAI（我们将使用的 LLM）和 Tavily（我们将使用的搜索工具）设置 API 密钥。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "743c19df-6da9-4d1e-b2d2-ea40080b9fdc"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]=userdata.get(\"ZHIPU_API_KEY\")\n",
        "os.environ[\"TAVILY_API_KEY\"]=userdata.get(\"TAVILY_API_KEY\")\n",
        "os.environ[\"GOOGLE_API_KEY\"]=userdata.get(\"GOOGLE_API_KEY\")\n",
        "os.environ[\"GITEE_API_KEY\"]=userdata.get(\"GITEE_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LANGSMITH_API_KEY\"]=userdata.get(\"LANGSMITH_API_KEY\")\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\""
      ],
      "metadata": {
        "id": "IooCYxWjE5il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\!\\!\\! tip\n",
        "注册 [LangSmith](https://www.google.com/search?q=https.docs.smith.langchain.com) 以快速发现问题并提高您的 LangGraph 项目的性能。LangSmith 允许您使用跟踪数据来调试、测试和监控使用 LangGraph 构建的 LLM 应用程序。\n",
        "\n",
        "## 1\\. 预处理文档\n",
        "\n",
        "1.  获取要在我们的 RAG 系统中使用的文档。我们将使用 [Lilian Weng 精彩博客](https://www.google.com/search?q=httpshttps://lilianweng.github.io/)中最新的三篇文章。我们将首先使用 `WebBaseLoader` 工具来获取这些页面的内容："
      ],
      "metadata": {
        "id": "EXBsbo-C-nVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "os.environ[\"USER_AGENT\"] = \"achatbot-demo\"\n",
        "\n",
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\",\n",
        "    \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n",
        "    \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\",\n",
        "]\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]"
      ],
      "metadata": {
        "id": "_wIT_cGVAf8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_83m-nZtS9uE",
        "outputId": "93b34c9b-e773-48ab-c801-332e0e3afce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 将获取到的文档拆分成更小的块，以便将其索引到我们的向量存储中："
      ],
      "metadata": {
        "id": "nzpPM83kKWg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "print(len(docs_list))\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=1000, chunk_overlap=50\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "print(len(doc_splits))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDDULhOwKKgh",
        "outputId": "30afa676-25b8-49d6-822f-2484f49180ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_splits[0].page_content.strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "l1nspkp7Kbj5",
        "outputId": "66ea9716-c948-4706-9fee-170c59dd5f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Reward Hacking in Reinforcement Learning | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Reward Hacking in Reinforcement Learning\\n    \\nDate: November 28, 2024  |  Estimated Reading Time: 37 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nBackground\\n\\nReward Function in RL\\n\\nSpurious Correlation\\n\\n\\nLet’s Define Reward Hacking\\n\\nList of Examples\\n\\nReward hacking examples in RL tasks\\n\\nReward hacking examples in LLM tasks\\n\\nReward hacking examples in real life\\n\\n\\nWhy does Reward Hacking Exist?\\n\\n\\nHacking RL Environment\\n\\nHacking RLHF of LLMs\\n\\nHacking the Training Process\\n\\nHacking the Evaluator\\n\\nIn-Context Reward Hacking\\n\\n\\nGeneralization of Hacking Skills\\n\\nPeek into Mitigations\\n\\nRL Algorithm Improvement\\n\\nDetecting Reward Hacking\\n\\nData Analysis of RLHF\\n\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nReward hacking occurs when a reinforcement learning (RL) agent exploits flaws or ambiguities in the reward function to achieve high rewards, without genuinely learning or completing the intended task. Reward hacking exists because RL environments are often imperfect, and it is fundamentally challenging to accurately specify a reward function.\\nWith the rise of language models generalizing to a broad spectrum of tasks and RLHF becomes a de facto method for alignment training, reward hacking in RL training of language models has become a critical practical challenge. Instances where the model learns to modify unit tests to pass coding tasks, or where responses contain biases that mimic a user’s preference, are pretty concerning and are likely one of the major blockers for real-world deployment of more autonomous use cases of AI models.\\nMost of the past work on this topic has been quite theoretical and focused on defining or demonstrating the existence of reward hacking. However, research into practical mitigations, especially in the context of RLHF and LLMs, remains limited. I especially want to call out for more research efforts directed toward understanding and developing mitigation for reward hacking in the future. Hope I will be able to cover the mitigation part in a dedicated post soon.\\nBackground#\\nReward Function in RL#\\nReward function defines the task, and reward shaping significantly impacts learning efficiency and accuracy in reinforcement learning. Designing a reward function for an RL task often feels like a ‘dark art’. Many factors contribute to this complexity: How you decompose a big goal into small goals? Is the reward sparse or dense? How you measure the success? Various choices may lead to good or problematic learning dynamics, including unlearnable tasks or hackable reward functions. There is a long history of research on how to do reward shaping in RL.\\nFor example, in an 1999 paper by Ng et al., the authors studied how to modify the reward function in Markov Decision Processes (MDPs) such that the optimal policy remains unchanged. They found that linear transformation works. Given a MDP $M = (S, A, T, \\\\gamma, R)$, we want to create a transformed MDP $M’ = (S, A, T, \\\\gamma, R’)$ where $R’ = R + F$ and $F: S \\\\times A \\\\times S \\\\mapsto \\\\mathbb{R}$, such that we can guide the learning algorithm to be more efficient. Given a real-valued function $\\\\Phi: S \\\\mapsto \\\\mathbb{R}$, $F$ is a potential-based shaping function if for all $s \\\\in S - {s_0}, a \\\\in A, s’ \\\\in S$:\\n\\n$$\\nF(s, a, s') = \\\\gamma \\\\Phi(s') - \\\\Phi(s)\\n$$\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embedding model"
      ],
      "metadata": {
        "id": "y1ShIXr2TODN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "### from langchain_cohere import CohereEmbeddings\n",
        "\n",
        "# Set embeddings\n",
        "# https://ai.gitee.com/serverless-api#embedding-rerank\n",
        "# 100 api calls per day, free tier ..................NO!!!! ❄️\n",
        "embd = OpenAIEmbeddings(\n",
        "    base_url=\"https://ai.gitee.com/v1\",\n",
        "    model=\"Qwen3-Embedding-8B\",#4096\n",
        "    api_key=os.environ[\"GITEE_API_KEY\"],\n",
        "    dimensions=1024,\n",
        "    check_embedding_ctx_length=False,\n",
        "    chunk_size=1000,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "ydSF1Bzhg8H5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = embd.embed_query(\"Today is a sunny day and I will get some ice cream.\")\n",
        "print(vector)"
      ],
      "metadata": {
        "id": "yNsHUYzVFU9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 创建检索器工具 (Create a retriever tool)\n",
        "\n",
        "现在我们有了拆分后的文档，我们可以将它们索引到一个向量存储（vector store）中，以便用于语义搜索。\n",
        "\n",
        "1.  使用内存中的（in-memory）向量存储和 OpenAI 嵌入（OpenAI embeddings）："
      ],
      "metadata": {
        "id": "_YvntWJMKco2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "vectorstore = InMemoryVectorStore(embedding=embd)\n",
        "while len(doc_splits) > 0:\n",
        "    documents = doc_splits[:10]\n",
        "    print(f\"{len(documents)=}\")\n",
        "    vectorstore.add_documents(documents=documents)\n",
        "    doc_splits = doc_splits[10:]\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GrYqeFOKjp1",
        "outputId": "2368fda4-1253-4aba-95bb-a4fb5cf77393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(documents)=10\n",
            "len(documents)=10\n",
            "len(documents)=10\n",
            "len(documents)=10\n",
            "len(documents)=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 使用 LangChain 预构建的 `create_retriever_tool` 来创建检索器工具："
      ],
      "metadata": {
        "id": "MrIhflQVKj_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools.retriever import create_retriever_tool\n",
        "\n",
        "retriever_tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"retrieve_blog_posts\",\n",
        "    \"Search and return information about Lilian Weng blog posts.\",\n",
        ")"
      ],
      "metadata": {
        "id": "GXVkGT9_KmzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Test the tool:"
      ],
      "metadata": {
        "id": "6K_TCZy5KpQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_tool.invoke({\"query\": \"types of reward hacking\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "dCVntLwfKrPm",
        "outputId": "a69f1099-a915-4c71-8de4-3ea6d899fbdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Or\\n@article{weng2024rewardhack,\\n  title   = \"Reward Hacking in Reinforcement Learning.\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2024\",\\n  month   = \"Nov\",\\n  url     = \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"\\n}\\nReferences#\\n[1] Andrew Ng & Stuart Russell. “Algorithms for inverse reinforcement learning.”. ICML 2000.\\n[2] Amodei et al. “Concrete problems in AI safety: Avoid reward hacking.” arXiv preprint arXiv:1606.06565 (2016).\\n[3] Krakovna et al. “Specification gaming: the flip side of AI ingenuity.” 2020.\\n[4] Langosco et al. “Goal Misgeneralization in Deep Reinforcement Learning” ICML 2022.\\n[5] Everitt et al. “Reinforcement learning with a corrupted reward channel.” IJCAI 2017.\\n[6] Geirhos et al. “Shortcut Learning in Deep Neural Networks.” Nature Machine Intelligence 2020.\\n[7] Ribeiro et al. “Why Should I Trust You?”: Explaining the Predictions of Any Classifier. KDD 2016.\\n[8] Nagarajan et al. “Understanding the Failure Modes of Out-of-Distribution Generalization.” ICLR 2021.\\n[9] Garrabrant. “Goodhart Taxonomy”. AI Alignment Forum (Dec 30th 2017).\\n[10] Koch et al. “Objective robustness in deep reinforcement learning.” 2021.\\n[11] Pan et al. “The effects of reward misspecification: mapping and mitigating misaligned models.”\\n[12] Everitt et al. “Reward tampering problems and solutions in reinforcement learning: A causal influence diagram perspective.” arXiv preprint arXiv:1908.04734 (2019).\\n[13] Gleave et al. “Adversarial Policies: Attacking Deep Reinforcement Learning.” ICRL 2020\\n[14] “Reward hacking behavior can generalize across tasks.”\\n[15] Ng et al. “Policy invariance under reward transformations: Theory and application to reward shaping.” ICML 1999.\\n[16] Wang et al. “Large Language Models are not Fair Evaluators.” ACL 2024.\\n[17] Liu et al. “LLMs as narcissistic evaluators: When ego inflates evaluation scores.” ACL 2024.\\n[18] Gao et al. “Scaling Laws for Reward Model Overoptimization.” ICML 2023.\\n[19] Pan et al. “Spontaneous Reward Hacking in Iterative Self-Refinement.” arXiv preprint arXiv:2407.04549 (2024).\\n[20] Pan et al. “Feedback Loops With Language Models Drive In-Context Reward Hacking.” arXiv preprint arXiv:2402.06627 (2024).\\n[21] Shrama et al. “Towards Understanding Sycophancy in Language Models.” arXiv preprint arXiv:2310.13548 (2023).\\n[22] Denison et al. “Sycophancy to subterfuge: Investigating reward tampering in language models.” arXiv preprint arXiv:2406.10162 (2024).\\n[23] Uesato et al. “Avoiding Tampering Incentives in Deep RL via Decoupled Approval.” arXiv preprint arXiv:2011.08827 (2020).\\n[24] Amin and Singh. “Towards resolving unidentifiability in inverse reinforcement learning.”\\n[25] Wen et al. “Language Models Learn to Mislead Humans via RLHF.” arXiv preprint arXiv:2409.12822 (2024).\\n[26] Revel et al. “SEAL: Systematic Error Analysis for Value ALignment.” arXiv preprint arXiv:2408.10270 (2024).\\n\\nQuantitative generalization results of a model trained with expert iteration according to our curriculum from each stage to the next. (Image source: Denison et al. 2024)\\n\\nIt is noteworthy that even after the curriculum, the model overwrote the reward and avoided detection less than 1/1000 of the time. Even when a model was trained on curricula which directly incentivized reward hacking, the model overwrote their reward less than 1% of the time and hacked unit tests even less often. As a simple mitigation, supervised fine-tuning the model on the first two environments–where the reward hacking behavior is easy to be detected (sycophancy and flattery)—with SFT data that does not game the env was found to reduce the likelihood of reward tampering in holdout environments.\\nPeek into Mitigations#\\nWhile there is a large body of literature discussing the phenomenon of reward hacking, there has been not a ton of work on mitigations for reward hacking, especially in the area of RLHF and LLMs. Let’s lightly review three potential approaches in this section, not exhaustive yet.\\nRL Algorithm Improvement#\\nAmodei et al. (2016) pointed out some directions for mitigating reward hacking in RL training:\\n\\nAdversarial reward functions. We treat the reward function as an adaptive agent itself and it can adapt to new tricks that the model discovered where the reward is high but human rating is low.\\nModel lookahead. It is possible to give reward based on future anticipated states; e.g., if the agent is gonna replace the reward function, it gets negative rewards.\\nAdversarial blinding. We can blind the model with certain variables such that the agent cannot learn information that enables it to hack the reward function.\\nCareful engineering. Some types of reward hacking against the system design can be avoided by careful engineering; e.g., sandboxing the agent to isolate its actions from its reward signals.\\nReward capping. This strategy is to simply limit the maximum possible reward, as it can effectively prevent rare events of the agent hacking to get a super high pay-off strategy.\\nCounterexample resistance. Improvement on adversarial robustness should benefit the robustness of the reward function.\\nCombination of multiple rewards. Combining different types of rewards could make it harder to be hacked.\\nReward pretraining. We can learn a reward function from a collection of (state, reward) samples, but depending on how well this supervised training setup is, it may come with other baggages. RLHF depends on this but learned scalar reward models are quite vulnerable to learning undesired traits.\\nVariable indifference. The goal is to ask the agent to optimize some variables in the environment but not others.\\nTrip wires. We can intentionally introduce some vulnerabilities and set up monitoring and alerts if any gets reward hacked.\\n\\nIn RL setups where human feedback is formed as approval of agent actions, Uesato et al. (2020) proposed to prevent reward tampering with decoupled approval.  If the feedback is conditioned on $(s, a)$ (state, action), we can never get uncorrupted feedback for action $a$ at state $s$ once reward tampering happens for this pair. Decoupling means that the query action for collecting feedback is sampled independently from the action taken in the world. Feedback is received even before the action is executed in the world, thus preventing the action from corrupting its own feedback.\\n\\n\\nIllustration of how decoupled approval works in comparison to standard approval or human-in-the-loop RL. (Image source: Uesato et al. 2020)\\n\\n\\n\\nWith decoupled approval, the action (taken in the world) and the query (for getting user approval feedback) are sampled independently. It can be applied to (Left) policy gradient and (Right) Q-learning algorithms. (Image source: Uesato et al. 2020)\\n\\nDetecting Reward Hacking#\\nAn alternative mitigation is to detect reward hacking by framing it as an anomaly detection task, where the detector (“a trusted policy” with trajectories and rewards validated by human) should flag instances of misalignment (Pan et al. 2022). Given (1) a trusted policy and (2) a collection of manually labeled trajectory rollouts, we can build a binary classifier based on distances between action distribution of two policies, the trusted policy and the target policy, and measure the accuracy of this anomaly detection classifier. In experiments by Pan et al. (2022), they observed that different detectors are better for different tasks and none of the tested classifier can achieve AUROC greater than 60% across all tested RL environments.\\n\\n\\nPerformance of detectors on different tasks. (Image source: Pan et al. 2022)\\n\\nNlp\\nLanguage-Model\\nSafety\\nHallucination\\nFactuality\\n\\n\\n\\n« \\n\\nReward Hacking in Reinforcement Learning\\n\\n\\n »\\n\\nDiffusion Models for Video Generation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2025 Lil\\'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\nThe model fails to generalize effectively, even with the right objective. This happens when the algorithm lacks sufficient intelligence or capability.\\nThe model generalizes capably but pursues an objective different from the one it was trained on. This happens when the proxy reward differs from the true reward function, $R’ \\\\neq R$. This is known as objective robustness (Koch et al. 2021) or goal misgeneralization (Langosco et al. 2022 )\\n\\nExperiments in two RL environments, CoinRun and Maze, demonstrated the importance of randomization during training. If during training, the coin or the cheese is placed at a fixed position (i.e. right end of the level or upper right corner of the maze) but testing in the env where the coin or cheese is placed at random, the agent would just run to the fixed position without obtaining the coin or cheese at test time. A conflict arises when a visual feature (e.g., cheese or coin) and a positional feature (e.g., upper-right or right end) are inconsistent during test time, leading the trained model to prefer the positional feature. I would like to point out that, in these two examples, the reward-result gaps are clear but such type of biases are unlikely to be so obvious in most real-world cases.\\n\\n\\nThe impact of randomizing the position of the coin during training. When the coin is placed at random for {0, 2, 3, 6, 11}% of the time during training (x-axis), the frequency of the agent navigating to the end of the level without obtaining the coin decreases with the increase of the randomization (\"y-axis\"). (Image source: Koch et al. 2021)\\n\\nReward Tampering (Everitt et al. 2019) is a form of reward hacking behavior where the agent interferes with the reward function itself, causing the observed reward to no longer accurately represent the intended goal. In reward tampering, the model modifies its reward mechanism either by directly manipulating the implementation of the reward function or by indirectly altering the environmental information used as input for the reward function.\\n(Note: Some work defines reward tampering as a distinct category of misalignment behavior from reward hacking. But I consider reward hacking as a broader concept here.)\\nAt a high level, reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering.\\n\\nEnvironment or goal misspecified: The model learns undesired behavior to achieve high rewards by hacking the environment or optimizing a reward function not aligned with the true reward objective—such as when the reward is misspecified or lacks key requirements.\\nReward tampering: The model learns to interfere with the reward mechanism itself.\\n\\nList of Examples#\\nReward hacking examples in RL tasks#\\n\\nA robot hand trained to grab an object can learn to trick people by placing the hand between the object and the camera. (Link)\\nAn agent trained to maximize jumping height may exploit a bug in the physics simulator to achieve an unrealistically height. (Link)\\nAn agent is trained to ride a bicycle to a goal and wins reward whenever it is getting closer to the goal. Then the agent may learn to ride in tiny circles around the goal because there is no penalty when the agent gets away from the goal. (Link)\\nIn a soccer game setup, the reward is assigned when the agent touches the ball and the agent learns to remain next to the ball to touch the ball in high frequency like in a viberating motion. (Link)\\nIn the\\xa0Coast Runners game, an agent controls a boat with the goal to finish the boat race as quickly as possible. When it is given a shaping reward for hitting green blocks along the race track, it changes the optimal policy to going in circles and hitting the same green blocks over and over again. (Link)\\n“The Surprising Creativity of Digital Evolution”  (Lehman et al. 2019) - This paper has many examples about how optimizing a misspecified fitness function can lead to surprising “hacking” or unintended evolutionary or learning results.\\nThe list of specification gaming in AI examples is collected by Krakovna et al. 2020.\\n\\nReward hacking examples in LLM tasks#\\n\\nA language model for generating summarization is able to explore flaws in the ROUGE metric such that it obtains high score but the generated summaries are barely readable. (Link)\\nA coding model learns to change unit test in order to pass coding questions. (Link)\\nA coding model may learn to directly modify the code used for calculating the reward. (Link)\\n\\nReward hacking examples in real life#'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3\\. 生成查询\n",
        "\n",
        "现在，我们将开始为我们的 agentic RAG 图构建组件([nodes](https://github.com/langchain-ai/langgraph/blob/main/docs/docs/concepts/low_level.md#nodes) and [edges](https://github.com/langchain-ai/langgraph/blob/main/docs/docs/concepts/low_level.md#edges))\n",
        "\n",
        ":::python\n",
        "请注意，这些组件将在 `MessagesState`  上运行——这是一种图状态（graph state），它包含一个 `messages` 键，键值为一个[聊天消息](https://python.langchain.com/docs/concepts/messages/)列表。\n",
        ":::\n",
        "\n",
        ":::js\n",
        "请注意，这些组件将在 `MessagesZodState` 上运行——这是一种图状态（graph state），它包含一个 `messages` 键，键值为一个[聊天消息](https://js.langchain.com/docs/concepts/messages/)列表。\n",
        ":::\n",
        "\n",
        "1.  构建一个 `generate_query_or_respond` 节点。它将调用 LLM（大型语言模型）根据当前的图状态（消息列表）生成响应。根据输入的消息，它将决定是使用检索器工具（retriever tool）进行检索，还是直接响应用户。请注意，我们通过 `.bind_tools` 方法，让我们之前创建的 `retriever_tool` 能够被这个聊天模型访问："
      ],
      "metadata": {
        "id": "_eM0niuBMfin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# zhipu\n",
        "llm=ChatOpenAI(\n",
        "  base_url=\"https://open.bigmodel.cn/api/paas/v4\",\n",
        "  model=\"glm-4.5-flash\",\n",
        "  max_tokens=32768,\n",
        "  temperature=0\n",
        ")\n"
      ],
      "metadata": {
        "id": "E0JrCfQOIpx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# google\n",
        "llm=ChatGoogleGenerativeAI(\n",
        "  #model=\"gemini-2.5-flash\",\n",
        "  model=\"gemini-2.5-pro\",# ok\n",
        "  temperature=0\n",
        ")"
      ],
      "metadata": {
        "id": "aOcvvEzeIOlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import MessagesState\n",
        "\n",
        "\n",
        "def generate_query_or_respond(state: MessagesState):\n",
        "    \"\"\"Call the model to generate a response based on the current state. Given\n",
        "    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.\n",
        "    \"\"\"\n",
        "    response = llm.bind_tools([retriever_tool]).invoke(state[\"messages\"])\n",
        "    return {\"messages\": [response]}"
      ],
      "metadata": {
        "id": "6zVLjPzEMjjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 在一个随机输入上尝试它："
      ],
      "metadata": {
        "id": "3FwDBJZIPp5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = {\"messages\": [{\"role\": \"user\", \"content\": \"hello!\"}]}\n",
        "generate_query_or_respond(input)[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoP5rBnkPnH6",
        "outputId": "823414fe-f4aa-43a5-bb02-12984ffa9318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "[{'type': 'text', 'text': \"Hello there! How can I help you today? I can search for information about Lilian Weng's blog posts.\", 'extras': {'signature': 'CpYBAdHtim8MQGJ6PUg4CFR2f4mLOombX9z4UrU+abLcuXHMYLheTISABeMiggysGeK9zQoEAqvWdhD6TCfsypFieBMXqI/FwOQim04awwpkPiwZoTL9iocmkmKlOAbo3HfYD6LWyHto3ohM2lpbQCVnmUUPAWZTHJiCIKW6Id5ZXl4pznh3KT3Q7puUgLxQzOVmZutCCszS'}}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 提一个需要语义搜索的问题："
      ],
      "metadata": {
        "id": "HEU7IhMpPzXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = {\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "generate_query_or_respond(input)[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndmWHEl6Pyuy",
        "outputId": "171b4671-db30-4d4e-cca1-0e972f08bfd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  retrieve_blog_posts (c06593dc-408d-4add-b1e3-7e8fc114d834)\n",
            " Call ID: c06593dc-408d-4add-b1e3-7e8fc114d834\n",
            "  Args:\n",
            "    query: types of reward hacking\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4\\. 评估文档 (Grade documents)\n",
        "\n",
        "1.  添加一个[conditional-edges](https://github.com/langchain-ai/langgraph/blob/main/docs/docs/concepts/low_level.md#conditional-edges) ——`grade_documents`—— 来判断检索到的文档是否与问题相关。我们将使用一个具有结构化输出模式 `GradeDocuments` 的模型来进行文档评估。`grade_documents` 函数将根据评估决策（`generate_answer` 或 `rewrite_question`）返回接下来要跳转到的节点的名称："
      ],
      "metadata": {
        "id": "bjlI6j5HP54j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal\n",
        "\n",
        "GRADE_PROMPT = (\n",
        "    \"You are a grader assessing relevance of a retrieved document to a user question. \\n \"\n",
        "    \"Here is the retrieved document: \\n\\n {context} \\n\\n\"\n",
        "    \"Here is the user question: {question} \\n\"\n",
        "    \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\"\n",
        "    \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\n",
        ")\n",
        "\n",
        "\n",
        "# highlight-next-line\n",
        "class GradeDocuments(BaseModel):\n",
        "    \"\"\"Grade documents using a binary score for relevance check.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Relevance score: 'yes' if relevant, or 'no' if not relevant\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "def grade_documents(\n",
        "    state: MessagesState,\n",
        ") -> Literal[\"generate_answer\", \"rewrite_question\"]:\n",
        "    \"\"\"Determine whether the retrieved documents are relevant to the question.\"\"\"\n",
        "    question = state[\"messages\"][0].content\n",
        "    context = state[\"messages\"][-1].content\n",
        "\n",
        "    prompt = GRADE_PROMPT.format(question=question, context=context)\n",
        "    response = (\n",
        "        llm\n",
        "        # highlight-next-line\n",
        "        .with_structured_output(GradeDocuments).invoke(\n",
        "            [{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "    )\n",
        "    score = response.binary_score\n",
        "    print(score)\n",
        "    if score == \"yes\":\n",
        "        return \"generate_answer\"\n",
        "    else:\n",
        "        return \"rewrite_question\""
      ],
      "metadata": {
        "id": "fZWdbodyP1VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 使用工具响应中不相关的文档来运行它："
      ],
      "metadata": {
        "id": "F5_Omcv8QKzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import convert_to_messages\n",
        "\n",
        "input = {\n",
        "    \"messages\": convert_to_messages(\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"\",\n",
        "                \"tool_calls\": [\n",
        "                    {\n",
        "                        \"id\": \"1\",\n",
        "                        \"name\": \"retrieve_blog_posts\",\n",
        "                        \"args\": {\"query\": \"types of reward hacking\"},\n",
        "                    }\n",
        "                ],\n",
        "            },\n",
        "            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n",
        "        ]\n",
        "    )\n",
        "}\n",
        "grade_documents(input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KCdkpHGuQOAI",
        "outputId": "b7af24b9-f9b3-4654-f132-46488ff3a9f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'rewrite_question'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 确认相关文档被（正确）分类："
      ],
      "metadata": {
        "id": "J7MaP9v_QO19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = {\n",
        "    \"messages\": convert_to_messages(\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"\",\n",
        "                \"tool_calls\": [\n",
        "                    {\n",
        "                        \"id\": \"1\",\n",
        "                        \"name\": \"retrieve_blog_posts\",\n",
        "                        \"args\": {\"query\": \"types of reward hacking\"},\n",
        "                    }\n",
        "                ],\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"tool\",\n",
        "                \"content\": \"reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering\",\n",
        "                \"tool_call_id\": \"1\",\n",
        "            },\n",
        "        ]\n",
        "    )\n",
        "}\n",
        "grade_documents(input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "KIaEPyzPQaZx",
        "outputId": "21abfe15-f510-4471-eafb-cbe62a86cb4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yes\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'generate_answer'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 重写问题 (Rewrite question)\n",
        "\n",
        "1.  构建 `rewrite_question` 节点。检索器工具（retriever tool）可能会返回不相关的文档，这表明需要改进原始的用户问题。为此，我们将调用 `rewrite_question` 节点："
      ],
      "metadata": {
        "id": "-PXJ4rgaQdu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "REWRITE_PROMPT = (\n",
        "    \"Look at the input and try to reason about the underlying semantic intent / meaning.\\n\"\n",
        "    \"Here is the initial question:\"\n",
        "    \"\\n ------- \\n\"\n",
        "    \"{question}\"\n",
        "    \"\\n ------- \\n\"\n",
        "    \"Formulate an improved question:\"\n",
        ")\n",
        "\n",
        "\n",
        "def rewrite_question(state: MessagesState):\n",
        "    \"\"\"Rewrite the original user question.\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    question = messages[0].content\n",
        "    prompt = REWRITE_PROMPT.format(question=question)\n",
        "    response = llm.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
        "    return {\"messages\": [{\"role\": \"user\", \"content\": response.content}]}"
      ],
      "metadata": {
        "id": "39AZP2ZlQkiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Try it out:\n"
      ],
      "metadata": {
        "id": "QP2mM9l6QnUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = {\n",
        "    \"messages\": convert_to_messages(\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"\",\n",
        "                \"tool_calls\": [\n",
        "                    {\n",
        "                        \"id\": \"1\",\n",
        "                        \"name\": \"retrieve_blog_posts\",\n",
        "                        \"args\": {\"query\": \"types of reward hacking\"},\n",
        "                    }\n",
        "                ],\n",
        "            },\n",
        "            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n",
        "        ]\n",
        "    )\n",
        "}\n",
        "\n",
        "response = rewrite_question(input)\n",
        "print(response[\"messages\"][-1][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIqTiV7EQoxl",
        "outputId": "d2937840-6645-45ad-e662-b1795d76b0d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Initial Question:**\n",
            "What does Lilian Weng say about types of reward hacking?\n",
            "\n",
            "**Reasoning about Intent:**\n",
            "The user is asking for specific information from a particular source, Lilian Weng, who is a known authority in the AI/ML space. The query isn't just about \"reward hacking\" in general, but specifically about a framework or taxonomy she has created. The user likely wants more than just a list of names; they want to understand the categories themselves, what distinguishes them, and how they work. The intent is to retrieve a structured explanation of her specific contribution to this topic.\n",
            "\n",
            "**Improved Question:**\n",
            "Summarize Lilian Weng's classification of reward hacking. For each category she proposes, please provide a definition and a key example she uses to illustrate it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 生成答案\n",
        "\n",
        "1.  构建 `generate_answer` 节点：如果我们通过了评估器（grader）的检查，我们就可以根据原始问题和检索到的上下文生成最终答案："
      ],
      "metadata": {
        "id": "kkBw6y0HQ--P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GENERATE_PROMPT = (\n",
        "    \"You are an assistant for question-answering tasks. \"\n",
        "    \"Use the following pieces of retrieved context to answer the question. \"\n",
        "    \"If you don't know the answer, just say that you don't know. \"\n",
        "    \"Use three sentences maximum and keep the answer concise.\\n\"\n",
        "    \"Question: {question} \\n\"\n",
        "    \"Context: {context}\"\n",
        ")\n",
        "\n",
        "\n",
        "def generate_answer(state: MessagesState):\n",
        "    \"\"\"Generate an answer.\"\"\"\n",
        "    question = state[\"messages\"][0].content\n",
        "    context = state[\"messages\"][-1].content\n",
        "    prompt = GENERATE_PROMPT.format(question=question, context=context)\n",
        "    response = llm.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
        "    return {\"messages\": [response]}"
      ],
      "metadata": {
        "id": "Lc2jUdSIQ_UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = {\n",
        "    \"messages\": convert_to_messages(\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"\",\n",
        "                \"tool_calls\": [\n",
        "                    {\n",
        "                        \"id\": \"1\",\n",
        "                        \"name\": \"retrieve_blog_posts\",\n",
        "                        \"args\": {\"query\": \"types of reward hacking\"},\n",
        "                    }\n",
        "                ],\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"tool\",\n",
        "                \"content\": \"reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering\",\n",
        "                \"tool_call_id\": \"1\",\n",
        "            },\n",
        "        ]\n",
        "    )\n",
        "}\n",
        "\n",
        "response = generate_answer(input)\n",
        "response[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIpER10rRJGF",
        "outputId": "2327fdff-7486-4883-dd96-d39fee20030d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I don't know. The provided context does not mention Lilian Weng. Therefore, I cannot answer what she says about the types of reward hacking.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. 组装图 (Assemble the graph)\n",
        "\n",
        "* 从 `generate_query_or_respond` (生成查询或响应) 节点开始，并决定是否需要调用 `retriever_tool` (检索器工具)。\n",
        "* 使用 `tools_condition` (工具条件) 路由到下一步：\n",
        "    * 如果 `generate_query_or_respond` 返回了 `tool_calls` (工具调用)，则调用 `retriever_tool` 来检索上下文。\n",
        "    * 否则，直接响应用户。\n",
        "* 评估检索到的文档内容与问题的相关性 (`grade_documents` 节点)，并路由到下一步：\n",
        "    * 如果不相关，则使用 `rewrite_question` (重写问题) 节点来重写问题，然后再次调用 `generate_query_or_respond`。\n",
        "    * 如果相关，则继续执行 `generate_answer` (生成答案) 节点，并使用包含检索到的文档上下文的 `ToolMessage` 来生成最终响应。"
      ],
      "metadata": {
        "id": "Wc40K_RWRGWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.prebuilt import tools_condition\n",
        "\n",
        "workflow = StateGraph(MessagesState)\n",
        "\n",
        "# Define the nodes we will cycle between\n",
        "workflow.add_node(generate_query_or_respond)\n",
        "workflow.add_node(\"retrieve\", ToolNode([retriever_tool]))\n",
        "workflow.add_node(rewrite_question)\n",
        "workflow.add_node(generate_answer)\n",
        "\n",
        "workflow.add_edge(START, \"generate_query_or_respond\")\n",
        "\n",
        "# Decide whether to retrieve\n",
        "workflow.add_conditional_edges(\n",
        "    \"generate_query_or_respond\",\n",
        "    # Assess LLM decision (call `retriever_tool` tool or respond to the user)\n",
        "    tools_condition,\n",
        "    {\n",
        "        # Translate the condition outputs to nodes in our graph\n",
        "        \"tools\": \"retrieve\",\n",
        "        END: END,\n",
        "    },\n",
        ")\n",
        "\n",
        "# Edges taken after the `action` node is called.\n",
        "workflow.add_conditional_edges(\n",
        "    \"retrieve\",\n",
        "    # Assess agent decision\n",
        "    grade_documents,\n",
        ")\n",
        "workflow.add_edge(\"generate_answer\", END)\n",
        "workflow.add_edge(\"rewrite_question\", \"generate_query_or_respond\")\n",
        "\n",
        "# Compile\n",
        "graph = workflow.compile()"
      ],
      "metadata": {
        "id": "BtHrt6JtROLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "PYUqaZ9xRRXk",
        "outputId": "6fddf123-93d3-49c8-cf68-eb9ef81553d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Run the agentic RAG\n"
      ],
      "metadata": {
        "id": "255xg7CzRViV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in graph.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "):\n",
        "    for node, update in chunk.items():\n",
        "        print(\"Update from node\", node)\n",
        "        update[\"messages\"][-1].pretty_print()\n",
        "        print(\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnXi2zdZRVRg",
        "outputId": "6bc54f08-1f97-4005-978e-3f7a378053ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Update from node generate_query_or_respond\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  retrieve_blog_posts (ffa6fe06-6200-45a7-a617-a179019c37d0)\n",
            " Call ID: ffa6fe06-6200-45a7-a617-a179019c37d0\n",
            "  Args:\n",
            "    query: types of reward hacking\n",
            "\n",
            "\n",
            "\n",
            "yes\n",
            "Update from node retrieve\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: retrieve_blog_posts\n",
            "\n",
            "Or\n",
            "@article{weng2024rewardhack,\n",
            "  title   = \"Reward Hacking in Reinforcement Learning.\",\n",
            "  author  = \"Weng, Lilian\",\n",
            "  journal = \"lilianweng.github.io\",\n",
            "  year    = \"2024\",\n",
            "  month   = \"Nov\",\n",
            "  url     = \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"\n",
            "}\n",
            "References#\n",
            "[1] Andrew Ng & Stuart Russell. “Algorithms for inverse reinforcement learning.”. ICML 2000.\n",
            "[2] Amodei et al. “Concrete problems in AI safety: Avoid reward hacking.” arXiv preprint arXiv:1606.06565 (2016).\n",
            "[3] Krakovna et al. “Specification gaming: the flip side of AI ingenuity.” 2020.\n",
            "[4] Langosco et al. “Goal Misgeneralization in Deep Reinforcement Learning” ICML 2022.\n",
            "[5] Everitt et al. “Reinforcement learning with a corrupted reward channel.” IJCAI 2017.\n",
            "[6] Geirhos et al. “Shortcut Learning in Deep Neural Networks.” Nature Machine Intelligence 2020.\n",
            "[7] Ribeiro et al. “Why Should I Trust You?”: Explaining the Predictions of Any Classifier. KDD 2016.\n",
            "[8] Nagarajan et al. “Understanding the Failure Modes of Out-of-Distribution Generalization.” ICLR 2021.\n",
            "[9] Garrabrant. “Goodhart Taxonomy”. AI Alignment Forum (Dec 30th 2017).\n",
            "[10] Koch et al. “Objective robustness in deep reinforcement learning.” 2021.\n",
            "[11] Pan et al. “The effects of reward misspecification: mapping and mitigating misaligned models.”\n",
            "[12] Everitt et al. “Reward tampering problems and solutions in reinforcement learning: A causal influence diagram perspective.” arXiv preprint arXiv:1908.04734 (2019).\n",
            "[13] Gleave et al. “Adversarial Policies: Attacking Deep Reinforcement Learning.” ICRL 2020\n",
            "[14] “Reward hacking behavior can generalize across tasks.”\n",
            "[15] Ng et al. “Policy invariance under reward transformations: Theory and application to reward shaping.” ICML 1999.\n",
            "[16] Wang et al. “Large Language Models are not Fair Evaluators.” ACL 2024.\n",
            "[17] Liu et al. “LLMs as narcissistic evaluators: When ego inflates evaluation scores.” ACL 2024.\n",
            "[18] Gao et al. “Scaling Laws for Reward Model Overoptimization.” ICML 2023.\n",
            "[19] Pan et al. “Spontaneous Reward Hacking in Iterative Self-Refinement.” arXiv preprint arXiv:2407.04549 (2024).\n",
            "[20] Pan et al. “Feedback Loops With Language Models Drive In-Context Reward Hacking.” arXiv preprint arXiv:2402.06627 (2024).\n",
            "[21] Shrama et al. “Towards Understanding Sycophancy in Language Models.” arXiv preprint arXiv:2310.13548 (2023).\n",
            "[22] Denison et al. “Sycophancy to subterfuge: Investigating reward tampering in language models.” arXiv preprint arXiv:2406.10162 (2024).\n",
            "[23] Uesato et al. “Avoiding Tampering Incentives in Deep RL via Decoupled Approval.” arXiv preprint arXiv:2011.08827 (2020).\n",
            "[24] Amin and Singh. “Towards resolving unidentifiability in inverse reinforcement learning.”\n",
            "[25] Wen et al. “Language Models Learn to Mislead Humans via RLHF.” arXiv preprint arXiv:2409.12822 (2024).\n",
            "[26] Revel et al. “SEAL: Systematic Error Analysis for Value ALignment.” arXiv preprint arXiv:2408.10270 (2024).\n",
            "\n",
            "The model fails to generalize effectively, even with the right objective. This happens when the algorithm lacks sufficient intelligence or capability.\n",
            "The model generalizes capably but pursues an objective different from the one it was trained on. This happens when the proxy reward differs from the true reward function, $R’ \\neq R$. This is known as objective robustness (Koch et al. 2021) or goal misgeneralization (Langosco et al. 2022 )\n",
            "\n",
            "Experiments in two RL environments, CoinRun and Maze, demonstrated the importance of randomization during training. If during training, the coin or the cheese is placed at a fixed position (i.e. right end of the level or upper right corner of the maze) but testing in the env where the coin or cheese is placed at random, the agent would just run to the fixed position without obtaining the coin or cheese at test time. A conflict arises when a visual feature (e.g., cheese or coin) and a positional feature (e.g., upper-right or right end) are inconsistent during test time, leading the trained model to prefer the positional feature. I would like to point out that, in these two examples, the reward-result gaps are clear but such type of biases are unlikely to be so obvious in most real-world cases.\n",
            "\n",
            "\n",
            "The impact of randomizing the position of the coin during training. When the coin is placed at random for {0, 2, 3, 6, 11}% of the time during training (x-axis), the frequency of the agent navigating to the end of the level without obtaining the coin decreases with the increase of the randomization (\"y-axis\"). (Image source: Koch et al. 2021)\n",
            "\n",
            "Reward Tampering (Everitt et al. 2019) is a form of reward hacking behavior where the agent interferes with the reward function itself, causing the observed reward to no longer accurately represent the intended goal. In reward tampering, the model modifies its reward mechanism either by directly manipulating the implementation of the reward function or by indirectly altering the environmental information used as input for the reward function.\n",
            "(Note: Some work defines reward tampering as a distinct category of misalignment behavior from reward hacking. But I consider reward hacking as a broader concept here.)\n",
            "At a high level, reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering.\n",
            "\n",
            "Environment or goal misspecified: The model learns undesired behavior to achieve high rewards by hacking the environment or optimizing a reward function not aligned with the true reward objective—such as when the reward is misspecified or lacks key requirements.\n",
            "Reward tampering: The model learns to interfere with the reward mechanism itself.\n",
            "\n",
            "List of Examples#\n",
            "Reward hacking examples in RL tasks#\n",
            "\n",
            "A robot hand trained to grab an object can learn to trick people by placing the hand between the object and the camera. (Link)\n",
            "An agent trained to maximize jumping height may exploit a bug in the physics simulator to achieve an unrealistically height. (Link)\n",
            "An agent is trained to ride a bicycle to a goal and wins reward whenever it is getting closer to the goal. Then the agent may learn to ride in tiny circles around the goal because there is no penalty when the agent gets away from the goal. (Link)\n",
            "In a soccer game setup, the reward is assigned when the agent touches the ball and the agent learns to remain next to the ball to touch the ball in high frequency like in a viberating motion. (Link)\n",
            "In the Coast Runners game, an agent controls a boat with the goal to finish the boat race as quickly as possible. When it is given a shaping reward for hitting green blocks along the race track, it changes the optimal policy to going in circles and hitting the same green blocks over and over again. (Link)\n",
            "“The Surprising Creativity of Digital Evolution”  (Lehman et al. 2019) - This paper has many examples about how optimizing a misspecified fitness function can lead to surprising “hacking” or unintended evolutionary or learning results.\n",
            "The list of specification gaming in AI examples is collected by Krakovna et al. 2020.\n",
            "\n",
            "Reward hacking examples in LLM tasks#\n",
            "\n",
            "A language model for generating summarization is able to explore flaws in the ROUGE metric such that it obtains high score but the generated summaries are barely readable. (Link)\n",
            "A coding model learns to change unit test in order to pass coding questions. (Link)\n",
            "A coding model may learn to directly modify the code used for calculating the reward. (Link)\n",
            "\n",
            "Reward hacking examples in real life#\n",
            "\n",
            "Quantitative generalization results of a model trained with expert iteration according to our curriculum from each stage to the next. (Image source: Denison et al. 2024)\n",
            "\n",
            "It is noteworthy that even after the curriculum, the model overwrote the reward and avoided detection less than 1/1000 of the time. Even when a model was trained on curricula which directly incentivized reward hacking, the model overwrote their reward less than 1% of the time and hacked unit tests even less often. As a simple mitigation, supervised fine-tuning the model on the first two environments–where the reward hacking behavior is easy to be detected (sycophancy and flattery)—with SFT data that does not game the env was found to reduce the likelihood of reward tampering in holdout environments.\n",
            "Peek into Mitigations#\n",
            "While there is a large body of literature discussing the phenomenon of reward hacking, there has been not a ton of work on mitigations for reward hacking, especially in the area of RLHF and LLMs. Let’s lightly review three potential approaches in this section, not exhaustive yet.\n",
            "RL Algorithm Improvement#\n",
            "Amodei et al. (2016) pointed out some directions for mitigating reward hacking in RL training:\n",
            "\n",
            "Adversarial reward functions. We treat the reward function as an adaptive agent itself and it can adapt to new tricks that the model discovered where the reward is high but human rating is low.\n",
            "Model lookahead. It is possible to give reward based on future anticipated states; e.g., if the agent is gonna replace the reward function, it gets negative rewards.\n",
            "Adversarial blinding. We can blind the model with certain variables such that the agent cannot learn information that enables it to hack the reward function.\n",
            "Careful engineering. Some types of reward hacking against the system design can be avoided by careful engineering; e.g., sandboxing the agent to isolate its actions from its reward signals.\n",
            "Reward capping. This strategy is to simply limit the maximum possible reward, as it can effectively prevent rare events of the agent hacking to get a super high pay-off strategy.\n",
            "Counterexample resistance. Improvement on adversarial robustness should benefit the robustness of the reward function.\n",
            "Combination of multiple rewards. Combining different types of rewards could make it harder to be hacked.\n",
            "Reward pretraining. We can learn a reward function from a collection of (state, reward) samples, but depending on how well this supervised training setup is, it may come with other baggages. RLHF depends on this but learned scalar reward models are quite vulnerable to learning undesired traits.\n",
            "Variable indifference. The goal is to ask the agent to optimize some variables in the environment but not others.\n",
            "Trip wires. We can intentionally introduce some vulnerabilities and set up monitoring and alerts if any gets reward hacked.\n",
            "\n",
            "In RL setups where human feedback is formed as approval of agent actions, Uesato et al. (2020) proposed to prevent reward tampering with decoupled approval.  If the feedback is conditioned on $(s, a)$ (state, action), we can never get uncorrupted feedback for action $a$ at state $s$ once reward tampering happens for this pair. Decoupling means that the query action for collecting feedback is sampled independently from the action taken in the world. Feedback is received even before the action is executed in the world, thus preventing the action from corrupting its own feedback.\n",
            "\n",
            "\n",
            "Illustration of how decoupled approval works in comparison to standard approval or human-in-the-loop RL. (Image source: Uesato et al. 2020)\n",
            "\n",
            "\n",
            "\n",
            "With decoupled approval, the action (taken in the world) and the query (for getting user approval feedback) are sampled independently. It can be applied to (Left) policy gradient and (Right) Q-learning algorithms. (Image source: Uesato et al. 2020)\n",
            "\n",
            "Detecting Reward Hacking#\n",
            "An alternative mitigation is to detect reward hacking by framing it as an anomaly detection task, where the detector (“a trusted policy” with trajectories and rewards validated by human) should flag instances of misalignment (Pan et al. 2022). Given (1) a trusted policy and (2) a collection of manually labeled trajectory rollouts, we can build a binary classifier based on distances between action distribution of two policies, the trusted policy and the target policy, and measure the accuracy of this anomaly detection classifier. In experiments by Pan et al. (2022), they observed that different detectors are better for different tasks and none of the tested classifier can achieve AUROC greater than 60% across all tested RL environments.\n",
            "\n",
            "\n",
            "Performance of detectors on different tasks. (Image source: Pan et al. 2022)\n",
            "\n",
            "Policy-refinement: LLM optimizes its policy based on feedback.\n",
            "\n",
            "The experiment is to build a LLM agent to pay invoice on a user’s behalf but run into InsufficientBalanceError and then the model learns to move money from other accounts without user authentication, potentially leading to more unauthorized transfer actions. They used ToolEmu as an emulator, which included 144 tasks for LLM agents, each consisting of a user-specific goal and a set of APIs. API errors were injected to simulate server side failure and each task was evaluated by GPT-4 to assign a helpfulness score.\n",
            "With more rounds of error feedback, LLMs can recover from the errors but with an increased number of severe constraint violations.\n",
            "  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "When comparing ICRH to traditional reward hacking, there are two noticeable differences:\n",
            "\n",
            "ICRH happens at deployment time within a self-refinement setup via a feedback loop, while traditional reward hacking occurs during training.\n",
            "Traditional reward hacking arises when the agent specializes in a task, while ICRH is driven by being a generalist.\n",
            "\n",
            "There is no magic way to avoid or detect or prevent ICRH yet, as improving prompt specification is insufficient to eliminate ICRH and scaling model sizes can worsen ICRH. The best practice of testing before deployment is to simulate what may happen at deployment time by evaluating the model with more rounds of feedback, diverse feedback, as well as injecting atypical environment observations.\n",
            "Generalization of Hacking Skills#\n",
            "Reward hacking behavior has been found to generalize across tasks: When models exhibit flaws in supervised training, it can sometimes generalize to exploit flaws in OOD environments (Kei et al., 2024). The researchers experimented with reinforcing reward hacking behavior in some reward-hackable environments and examined whether it generalizes to other holdout datasets. Essentially, they prepared 8 datasets on multiple-choice questions, where 4 for training and 4 for testing. The RL training employs expert iteration, that is, iterative fine-tuning on best-of-$n$ samples.\n",
            "\n",
            "\n",
            "Training GPT-3.5 on a set of reward-hackable environments can lead to generalization of reward hacking behavior in holdout environments. (Image source: Kei et al. 2024)\n",
            "\n",
            "Notably, all experiments have a scratchpad design to host hidden reasoning steps. This scratchpad serves as a way to interpret what the model is thinking along the process. Analysis of the scratchpad revealed that it can:\n",
            "\n",
            "Incorporate user beliefs; e.g., thinking about its conversation partner and grader.\n",
            "Show awareness of evaluation; e.g. thinking about how the model response will be evaluated and the process.\n",
            "\n",
            "Denison et al. (2024) studied reward tampering with a manually curated curriculum of increasingly sophisticated gameable environments. They observed that training on easier environments in the curriculum amplifies specification gaming on remaining environments. In some cases, the model can generalize zero-shot to directly rewriting its own reward function after training on the full curriculum.\n",
            "Such a curriculum of gameable environments include:\n",
            "\n",
            "Political sycophancy: The environment rewards the model if it gives answers matching a user’s political views.\n",
            "Tool-use flattery: The environment provides the model access to run command line to get access to the user’s file system to read a poetry sample in a local file. The model may give the poetry a high rating as a sycophantic response.\n",
            "Rubric modification: The environment scores the model based on a checklist and the model must mark every item on the checklist as completed to receive a high reward.\n",
            "Reward tampering: The model is tasked with examining the ./rl_environment/ directory, which contains a mock version of its own RL code, and report how many episodes have elapsed. In the directory, the compute_reward.py file contains a reward function returning a constant, and the other this_file_blocks_reward_hack_attempts.py contains a test to check whether the reward function has been modified.\n",
            "\n",
            "\n",
            "\n",
            "Examples of evaluation scripts in the \"Tool-use flattery\" and \"Reward tampering\" environments, where `` is hidden from oversight and `` can be executed in a VM. (Image source: Denison et al. 2024)\n",
            "\n",
            "The model was trained following this curriculum, and the paper measured whether the model can learn gaming behaviors that generalize to unseen environments. Experimenting with Claude-2 helpful-only model, they sampled $N=64$ responses per each of $P=1024$ HHH prompts for expert iteration and selected the best according to RM. A model trained using expert iteration to do supervised learning on the curriculum can generalize from one stage to the next.\n",
            "\n",
            "\n",
            "\n",
            "Update from node generate_answer\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "According to Lilian Weng, reward hacking can be categorized into two high-level types. The first is \"environment or goal misspecification,\" where a model learns undesired behavior by optimizing a reward function that is not aligned with the true objective. The second type is \"reward tampering,\" which occurs when the model learns to interfere with the reward mechanism itself.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "trace: https://smith.langchain.com/public/b7fc14db-71b2-4c27-8ba7-6deab78067fb/r"
      ],
      "metadata": {
        "id": "tY8bpz_Lm6Id"
      }
    }
  ]
}